\documentclass[a4paper, 12pt]{article}

%%%%%%%%%%%%%%%%%%%%%%Paquetes
\usepackage[spanish]{babel}  
\usepackage{indentfirst} %%%%%%%%%%%%%%%%Crear un indent al principio
\usepackage[latin1]{inputenc}%%%%%%%%%%%%ñ y acentos
\usepackage{amstext}%%%%%%%%
\usepackage{amsfonts}%%%%%%%
\usepackage{amssymb}%%%%%%%% AMSLaTeX
\usepackage{amscd}%%%%%%%%%%
\usepackage{amsmath}%%%%%%%%
\usepackage{enumerate}%%%%%%%%%%%%%%%%Mejoras del entorno enumerate
\usepackage[all]{xy}
\usepackage{latexsym}
\usepackage{color}
\usepackage[mathcal]{eucal}%%%%%%%Caligrafica matematica
\usepackage{graphicx}
\usepackage{url}
\usepackage{tcolorbox}
\usepackage{setspace}
\onehalfspacing
%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%Teoremas
\newtheorem{teo}{Teorema}[section]%%%%%%%%% Teorema
\newtheorem{defi}{Definición}[section]%%%%%%%% Definicion
\newtheorem{lema}[teo]{Lema}%%%%%%%%%%%%% Lema
\newtheorem{propo}[teo]{Proposición}%%%%%%%% Proposicion
\newtheorem{cor}[teo]{Corolario}%%%%%%%%%%%Corolario
\newtheorem{pro1}{}%[chapter]%%%%%%%%%Problema
\newenvironment{pro}{\begin{pro1} \sf \small} {\end{pro1}}
\newtheorem{*pro1}[pro1]{*}%%%%%%%%%%Problema complicado
\newenvironment{*pro}{\begin{*pro1} \sf \small} {\end{*pro1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%Comandos
\newcommand{\dem}{\noindent\textbf{Demostración. }\vspace{0.3 cm}}%%Demostracion
\newcommand{\sdem}{\noindent\textbf{Esquema de la demostración. }\vspace{0.3 cm}}%%Demostracion
\newcommand{\R}{\mathbb{R}}%%%%%%%%%%%%Numeros reales
\newcommand{\F}{\mathbb{F}}%%%%%%%%%%%%Cuerpo
\newcommand{\C}{\mathbb{C}}%%%%%%%%%%%%Numeros complejos
\newcommand{\Q}{\mathbb{Q}}%%%%%%%%%%%%Numeros racionales
\newcommand{\N}{\mathbb{N}}%%%%%%%%%%%%Numeros naturales
\newcommand{\Z}{\mathbb{Z}}%%%%%%%%%%%%Numeros enteros
\newcommand{\cua}{\mathbb{H}}%%%%%%%%%%%%Cuaterniones
\newcommand{\g}{\mathfrak{g}}%%%%%%%%%%%%Algebra de Lie del grupo G
\newcommand{\V}{\mathcal{V}}%%%%%%%%%%%%Variedad
\newcommand{\W}{\mathcal{W}}%%%%%%%%%%%%Variedad
\newcommand{\h}{\mathfrak{h}}%%%%%%%%%%%%Algebra de Lie del grupo H
\newcommand{\ind}{\textit{\bf{Indicación. }}}%%% Indicacion
\newcommand{\fin}{ $\Box $ \vspace{0.4 cm}}
\newcommand{\p}{\mathfrak{p}}%%%%%%%% Ideal primo
\newcommand{\m}{\mathfrak{m}}%%%%%%%% Ideal maximal
\newcommand{\limind}{\lim_{\longrightarrow} } 
\newcommand{\gp}{\mathcal{G'}}%%%%%%%%%%%Algebra del grupo G'
\newcommand{\lto}{\longrightarrow}%%%%%%Simplificacion de la flecha larga
\newcommand{\wa}{\omega_2} %%%%%%%%%%%forma simplectica
\newcommand{\Wa}{\Omega_2} %%%%%%%%%% forma simplectica lineal
\newcommand{\lag}{\lambda_g}%%%%%%%%%%%%Traslacion a la izquierda
\newcommand{\rg}{\rho_g}%%%%%%%%%%%%%%%%Traslacion a la derecha
\newcommand{\Gr}{\boldsymbol{G}}%%%%%%%%%%Recubridor universal
\newcommand{\norma}[1]{\: \parallel #1 \!\parallel\! }%%%Norma de un vector
\newcommand{\abs}[1]{\left|\, #1 \right|}  %%%Valor absoluto 
\newcommand{\Pro}{\mathbb{P}}%%%%%%Espacio proyectivo
\newcommand{\Problemas}{\newpage  \begin{center}{\Huge Problemas}\end{center}}
\newcommand{\Ejemplos}{\vspace{0.5 cm} {\bf Ejemplos}}
\newcommand{\escalar}[2]{\left\langle\, #1,#2\, \right\rangle}  %%%Producto escalar 
\newcommand{\campos}{\mathfrak{X} } %%%% Campos en una variedad
%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%Operadores
\DeclareMathOperator{\End}{End}%%%%%%%%%%Endomorfismo
\DeclareMathOperator{\Ad}{Ad}%%%%%%%%%%Adjunta
\DeclareMathOperator{\grad}{grad}%%%%%%%%%%Graciente
\DeclareMathOperator{\Dif}{Dif}%%%%%%%%%%Diferenciales
\DeclareMathOperator{\sop}{sop}%%%%%%%%%Soporte
\DeclareMathOperator{\distancia}{d}%%%%%%%%Distancia
\DeclareMathOperator{\sen}{sen}%%%%%%%%%%Seno español
\DeclareMathOperator{\Der}{Der}%%%%%%%%%%Derivaciones
\DeclareMathOperator{\rang}{rang}%%%%%%%%Rango
\DeclareMathOperator{\Hom}{Hom}%%%%%%Homomorfismos
\DeclareMathOperator{\Ann}{Ann}%%%%%%%Anulador
\DeclareMathOperator{\Img}{Im} %%%%Parte imaginaria
\DeclareMathOperator{\rad}{rad}%%%%%%%%Radical
\DeclareMathOperator{\Ker}{Ker}%%%%%%%Nucleo
\DeclareMathOperator{\Id}{Id}%%%%%%% Identidad
\DeclareMathOperator{\GL}{GL}%%%%%%%%%Grupo lineal
\DeclareMathOperator{\Apli}{Apli}%%%%%%Aplicaciones
\DeclareMathOperator{\Bil}{Bil}%%%%%Bilineales
\DeclareMathOperator{\Spec}{Spec}%%%%Espectro
\DeclareMathOperator{\Aut}{Aut}%%%%Espectro
\DeclareMathOperator{\Ob}{Ob}  %%% Objetos de una categoría
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%
\title{Grupos de Lie}
\author{José Luis Tábara}
\date{jltabara@gmail.com}
%%%%%%%%%%%%%%%%%

\begin{document}



\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black]

\vspace{-1cm}
\maketitle

\end{tcolorbox}

\thispagestyle{empty}

\tableofcontents

\newpage



\section{Introducción}


La teoría de grupos de Lie se crea en el último cuarto del siglo {\sc xix} gracias al genio del matemático noruego Sophus Lie.\index{Lie, Sophus} Veamos el caldo de cultivo donde floreció esta teoría que hoy en día esta evolucionadísima y tiene aplicaciones en muchos dominios, tanto de la matemática como de la física.

El concepto de grupo había nacido ya en los trabajos que realizó Galois \index{Galois, Evariste}sobre las ecuaciones algebraicas y la posibilidad de hallar fórmulas que permitieran resolverlas por radicales. Sin embargo el desconocimiento y la dejadez de algunos matemáticos hicieron que esta teoría no fuera conocida hasta mucho tiempo después de su construcción y por ello el concepto de grupo va a pasar desapercibido algunos años más de lo necesario. En aquella época, tanto en los trabajos de Galois como en los de Cauchy, \index{Cauchy, Agustin} los grupos  eran siempre considerados como subgrupos del grupo simétrico.

Hacia la mitad del siglo {\sc xix} Cayley \index{Cayley, Arthur} ya había dado una definición mo\-derna de lo que es un grupo abstracto, pero debemos esperar al trabajo de C.~Jordan \index{Jordan, Camille} (1870) (que trata sobre las ecuaciones algebraicas y los grupos asocia\-dos), para que la teoría de grupos pase a formar parte del conocimiento de la comunidad matemática internacional.

Los por entonces compañeros de trabajo Felix Klein \index{Klein, Felix} y Sophus Lie se entregan al estudio de este trabajo y de la teoría de grupos. El primer fruto de este trabajo es la exposición de Klein de su \guillemotleft Programa de Erlangen\guillemotright\  \index{programa de Erlangen} en el que basa sobre el concepto de grupo todas las geometrías que a lo largo del siglo {\sc xix} habían ido naciendo, tras los pioneros trabajos de Gauss, \index{Gauss, Carl} Lobacheski \index{Lobacheski, Nikolai} y Bolyai.\index{Bolyai, Janos}  Así la geometría se va a reducir en gran parte al estudio de los grupos y de sus invariantes.

Poco después inicia Lie sus trabajos sobre la teoría que hoy lleva su nombre. Poco o nada se parecen sus trabajos a lo que se expondrá en estas notas, debido fundamentalmente a dos hechos

\begin{itemize}
 \item En esta época el concepto de grupo estaba íntimamente unido al concepto de grupo de transformaciones. Ahora se estudia la teoría de grupos sin necesidad de considerar que estos actúan en un cierto espacio.

\item La teoría de grupos que hizo Lie era fundamentalmente de tipo local. Ello era ciertamente lógico, pues ni la teoría de variedades ni la topología estaban lo suficientemente desarrolladas para poder preguntarse por aspectos topológicos globales de los grupos de Lie, cosa que actualmente es fundamental.

\end{itemize}

La idea que llevó a Lie al estudio de los grupos continuos fue que casi todos los métodos clásicos de integración de ecuaciones diferenciales se basaban en la invariancia de la ecuación por un cierto grupo de transformaciones continuas. La integración de las ecuaciones diferenciales fue pues el motivo de la creación de esta teoría.


Los estudios de Lie no pasaron desapercibidos para Hilbert \index{Hilbert, David} en su tarea de axiomatizar la geometría. El método de estudio de la geometría adoptato por Lie seguía el camino trazado por Klein en el \guillemotleft programa de Erlangen\guillemotright. Sin embargo Lie suponía la diferenciabilidad de todas las funciones que aparecían en su estudio, cosa que a Hilbert no le parecía natural. Para Hilbert todo sería \mbox{correcto} si no hiciese falta más que suponer la continuidad y no la diferenciabilidad. Esto llevó a Hilbert a incluir este problema (en quinto lugar) entre los veintitrés que propuso en la conferencia de matemáticas de 1900. \index{quinto problema de Hilbert} Este problema fue resuelto en la década de los treinta por Von \mbox{Neumman} \index{Von Neumman, John} para el caso especial de los grupos compactos y por Gleason,\index{Gleason, Andrew} Montgomery \index{Montgomery, Deane} y Zippin \index{Zippin, Leo} en 1951 para el caso general.


La primera aplicación de los grupos de Lie se da en las ecuaciones diferenciales. Sabemos que la invariancia de una ecuación diferencial por un cierto grupo implica la existencia de cantidades conservadas y las cantidades conservadas son importantísimas en la integración de las ecuaciones. En esta misma línea se halla el teorema de Emmy Noether \index{Noether, Emmy} (1918) que es de gran importancia en física (teorías de campos). Estos problemas de integración, al menos en aquella época, eran de natu\-raleza estrictamente local y por tanto con la teoría local de grupos era suficiente para esta aplicación.

Sin embargo el nuevo siglo trajo consigo un desarrollo espectacular de la topología, tanto general como algebraica (Haussdorf \index{Haussdorf, Felix} 1911, Poincaré \index{Poincaré, Henri} 1895) y una formulación matemáticamente correcta de la hasta entonces idea intuitiva de variedad diferencial (Weyl \index{Weyl, Hermman} 1912, Whitney \index{Whitney, Hassler} 1936). La matemática había evolucionado lo suficiente para tratar la teoría de los grupos de Lie desde un punto de vista global, cosa que fue hecha en primer lugar y de manera sistemática por Claude Chevalley \index{Chevalley, Claude} (1946).

El primer descubrimiento de la teoría de Lie fue que a cada grupo se le puede asociar de un modo canónico una estructura algebraica, la de álgebra de Lie (también llamado grupo infinitesimal). En la versión moderna de la teoría, la estructura de álgebra de Lie asociada se deduce del estudio de los campos vectoriales invariantes por la acción del grupo. Sin embargo en los trabajos de Lie y de muchos de sus sucesores, el álgebra de Lie, o lo que es lo mismo las constantes de estructura del álgebra, se deducían de los términos de orden dos del desarrollo de Taylor de la operación del grupo.


Los problemas de la teoría de grupos se reducen en gran parte al estudio de problemas algebraicos en las álgebras de Lie, que no son sino problemas de la teoría de matrices. Los conceptos de álgebra y de grupo se hallan unidos a través del morfismo exponencial. El álgebra de Lie de un grupo nos informa de todas las características locales del grupo (teorema de Lie), sin embargo la estructura global no se refleja (al menos enteramente) en el álgebra de Lie debido a que a grupos no isomorfos (ni difeomorfos) le pueden corresponder álgebras que si lo sean.


El problema que surge después es la clasificación de las álgebras de Lie. A~este trabajo aparecen unidos los nombres de dos grandes matemáticos, W.~Killing \index{Killing, Walter} y sobre todo Eli Cartan,\index{Cartan, Eli} que es sin lugar a dudas el matemático que más contribuyó al desarrollo de los grupos de Lie y cuyo nombre aparecerá siempre ligado a importantes teoremas y conceptos de esta teoría. La teoría de estructura de las álgebras de Lie semisimples y complejas fue dada en primer lugar por Killing.  Cartan dio rigor y rellenó algunas lagunas que había dejado Killing en su trabajo. Posteriormente Cartan encontró que a toda álgebra de Lie semisimple y compleja le correspondía un álgebra real semisimple y compacta. Con este resultado se pudo clasificar localmente la estructura de los grupos compactos. Veinte años después, el mismo Cartan clasificó todas las álgebras de Lie semisimples y reales.

Una nueva aplicación surge en el dominio de la física. Los creadores de la mecánica cuántica pronto se dieron cuenta que el formalismo matemático de su teoría era un útil indispensable para su comprensión, pues la mecánica cuántica, a diferencia de la clásica, no es intuitiva, sino que solo es comprensible y asimilable desde un punto de vista matemático.
\mbox{H. Weyl} fue el primero en fijarse en la importancia que tenían los grupos en la teoría 
cuántica. En sus trabajos observó  que el estudio del momento angular de la mecánica cuántica era sensiblemente equivalente a la teoría de representaciones de un grupo de Lie compacto, el grupo $\mathrm{SO}(3)$ (y por tanto de su recubridor universal $\mathrm{SU}(2)$). Se vio que la existencia de momentos angulares semienteros estaba ligada no a las representaciones lineales de $\mathrm{SO}(3)$, sino a las de su recubridor universal. Weyl estudió con gran profundidad la estructura y representaciones de los grupos compactos.  Estos estudios llegarían a ser lo que actualmente se conoce como análisis armónico (aunque esta teoría también ha recibido influencia de otras partes de la matemática). 

No se detienen aquí las aplicaciones físicas y matemáticas de los grupos de Lie. En 1950 Charles Erhesman,\index{Erhesman, Charles} basándose en los trabajos de Eli Cartan, propone el concepto moderno de conexión infinitesimal: las conexiones en fibrados principales. Los fibrados principales son variedades donde actúa libremente un grupo de Lie y las conexiones en ellos son distribuciones de campos que satis\-facen ciertos criterios de invariancia por la ley de grupo, o visto desde una perspectiva dual es una 1-forma que satisface cierta ley de transformación bajo la acción del grupo.

Esta teoría matemática fue hábilmente utilizada por los físicos, pues es el sustrato matemático donde se apoyan las teorías clásicas de campos gauge. A~pesar de que la formalización matemática de la  teoría cuántica de campos gauge es una asignatura pendiente de los físicos actuales, todo parece indicar que la estructura de grupo de Lie y las representaciones lineales de estos tendrán una gran importancia en la teoría cuántica de campos.

La teoría de grupos de Lie ha influido enormemente sobre otras partes de la matemática y se ha dividido en muchas ramas. Podemos citar por ejemplo la Teoría de grupos de Lie en dimensión infinita o la Teoría de los grupos algebraicos. Las álgebras de Hoph, que aparecieron primeramente en el estudio de la cohomología de los grupos de Lie, también merecen hoy en día una especial atención.



\newpage

\section{Definición de grupo de Lie}

\begin{defi}

Un {\sf grupo de Lie} \index{grupo de Lie} de dimensión  $n$  es un conjunto $G$ dotado de una estructura de variedad diferenciable de dimensión  $n$  y de una estructura de grupo relacionadas entre sí mediante los axiomas:
\begin{itemize}

 \item  La aplicación multiplicación $( g, g' )  \rightarrow  g. g'$ de $G \times G$ en $G$ es diferenciable, estando $G \times G$ dotado de la estructura de variedad producto.

\item El paso al inverso $g \rightarrow g^{-1}$ de $G$ en $G$ es diferenciable.

\end{itemize}

\end{defi}

Si en la definición de grupo de Lie sustituimos diferenciable, por diferenciable hasta el orden  $k$, obtenemos lo que se llama un {\sf grupo de Lie de orden  $k$}.\index{grupo de Lie!de orden $k$} Es obvio que todo grupo de Lie es a la vez un grupo de Lie de orden $k$  para cualquier~$k$. Decimos que la estructura de orden  $k$ y la de orden infinito son {\sf compatibles} \index{estructuras compatibles} si la de orden $k$ se deduce del modo natural de la estructura de orden infinito. Un problema interesante de la teoría de grupos de Lie es saber si todo grupo de Lie de orden  $k$ tiene una estructura de orden infinito compatible y si ésta es única. Para  $k$ estrictamente positivo es un problema resuelto en los albores de la teoría. El caso  $k= 0$ es el denominado {\sf quinto problema de Hilbert} \index{quinto problema de Hilbert} y se resolvió de modo afirmativo en la década de los cincuenta.

\bigskip

\noindent{\bf Nota.}



Salvo indicación en contra, cuando expresemos que algún objeto es diferenciable, consideraremos que es de clase $C^\infty$.




\begin{defi}

Una aplicación $\varphi : G \rightarrow H$ es un {\sf morfismo de grupos de Lie}  \index{morfismo!de grupos de Lie} si cumple:

\begin{itemize}

\item $\varphi$ es un morfismo de grupos.
\item $\varphi$ es una aplicación diferenciable.

\end{itemize}

\end{defi}


La estructura de grupo de Lie es un caso particular de una estructura más general, que es la de grupo topológico.

\begin{defi}

Un {\sf grupo topológico} \index{grupo!topológico} es un espacio topológico  $G$ dotado de una estructura de grupo que satisface:

\begin{itemize}

 \item  La multiplicación es una operación continua.

\item El paso al inverso es una operación continua.

\end{itemize}

\end{defi}


Como hemos dicho todo grupo de Lie es un grupo topológico del modo natural y, si existe, sólo puede haber una estructura subyacente de grupo de Lie para cualquier grupo topológico.

\bigskip

Otro tipo de estructura relacionada con nuestra teoría es la de {\sf grupo de Lie de dimensión infinita}. \index{grupo de Lie!de dimensión infinita}Para definir esta estructura, el grupo de Lie debería tener como variedad subyacente una variedad de dimensión infinita, ya sea valorada en un Banach o en general en un espacio localmente convexo. La razón de que esta parte de la teoría requiera un estudio distinto es debido a que ciertos resultados de la topología y del cálculo diferencial no son válidos en dimensión infinita.

\bigskip

Supongamos ahora que $G$ es una variedad compleja de dimensión  $n$  y que la multiplicación y el paso al inverso son funciones holomorfas. Estaríamos hablando entonces de un {\sf grupo de Lie complejo} \index{grupo de Lie!complejo} u {\sf holomorfo}. \index{grupo de Lie!holomorfo} Naturalmente todo grupo de Lie holomorfo tiene una estructura subyacente de grupo de Lie real.

\bigskip

A pesar de que como hemos visto la noción de grupo de Lie tiene varias acepciones, nosotros consideraremos siempre grupos de Lie reales de orden infinito y de dimensión finita.


\bigskip

Algunas propiedades fundamentales de los grupos de Lie son puramente topológicas.  El marco natural para formula dichas propiedades es el de los grupos topológicos.

Dado un grupo topológico $G$, denotamos por $\lambda_g$ a la aplicación
$$
\begin{array}{cccc}
\lambda_g :& G & \rightarrow & G \\
        &a&\rightarrow& ga
 \end{array}
 $$
 que es una aplicación continua, por ser continua la multiplicación.  Esta aplicación admite una inversa, precisamente $\lambda_{g^{-1}}$, lo que prueba que $\lambda_g$ es un homeomorfismo, que llamamos \index{traslación!a la izquierda}{\sf traslación a la izquierda}.       Análogamente se define la \index{traslación!a la derecha} {\sf traslación a la derecha} $\rho_g(a)= ag$.
 
 Si $U$ es un conjunto, denotamos por $gU$  al conjunto $\lambda_g(U)$.  Si $U$ es abierto, $gU$ también, puesto que $\lambda_g$ es un homeomorfismo.  Si $U$ y $V$ son dos conjuntos denotamos $UV$ al subconjunto
 $$
 UV=\{gh \text{ con } g \in U \text{ y } h \in V\}
 $$
 Resulta que 
 $$
 UV= \bigcup_{g\in U} gV
 $$
  Si $U$ y $V$ son abiertos, entonces $UV$ es abierto, por ser unión de abiertos.
  
  Denotamos por $U^{-1}$ al subconjunto
  $$
  U^{-1}= \{g^{-1} \text{con } g \in U\}
  $$
que coincide con la imagen del conjunto $U$ por el homeomorfismo inversión.  $U^{-1}$ es abierto si $U$ lo es.  Si $U$ es un entorno de $e$, por la continuidad del producto, debe existir un entorno abierto $W$ de $e$ tal que $W^2 \subset U$.  Llamando $V= W \cap W^{-1}$ se tiene que $V$ es un entorno abierto  del neutro que cumple  $V^{-1}= V$.

\begin{propo}

Si $H  \subset G$ es un subgrupo que es un abierto de la topología, entonces también es un cerrado.

\end{propo}

\dem

Si $g \not \in H$ entonces $gH$ es un coset distinto y no puede tener intersección con~$H$.  Es evidente que 
$$
H^c= \cup_{g \in H^c} gH
$$
que es una unión de abiertos y por lo tanto $H$ es cerrado. \fin

\begin{propo}

Dado un grupo topológico $G$ denotamos por $G_e$ la componente conexa de la unidad. $G_e$ es un subgrupo normal de $G$.

\end{propo}

\dem

Como $G_e \times G_e$ es conexo, su imagen por la multiplicación está contenida en la componente conexa de $e$.  La operación es interna.

La aplicación $g \rightarrow g^{-1}$ es un homeomorfismo que transforma componentes conexas en componentes conexas.  Como $e^{-1}=e$, tenemos que ${G_e}^{-1}=G_e$ y ya sabemos que es subgrupo.

La aplicación $\lambda_g\rho_{g^{-1}}$ es un homeomorfismo y razonando de modo similar se prueba que el subgrupo es normal. \fin

\begin{propo}

Sea $G$ conexo y $U$ un entorno del neutro.  El subgrupo que genera $U$ es todo $G$.

\end{propo}

\dem

Sea $H \subset G$ el subgrupo generado por $U$.  Si $g \in H$ entonces $gU$ también debe estar contenido en $H$.    Todo punto de $g\in H$ tiene un entorno ($gU$)  contenido en~$H$, por lo que podemos afirmar que $H$ es abierto.  Por la proposición anterior también es cerrado y como $G$ es conexo, coinciden. \fin


Hemos visto que existen entornos de la unidad tales que $V^{-1}= V$.  En este caso es fácil ver que el subgrupo generado por $V$ coincide con 
$$
\bigcup_{n=1}^\infty V^n
$$
donde $V^n= \{g_1 \cdots g_n\} \text{ con } g_i \in V\}$.

\begin{cor}

Si $G$ es conexo y de Lie, entonces tiene una base de entornos numerable.

\end{cor}

Si $G$ es de Lie podemos suponer que el entorno $V$ anterior es coordenado y por lo tanto tiene una base numerable. Inductivamente  se prueba que $V^n$ tiene base numerable
(pues $V^n= V^{n-1}V$) y la unión de todos estos conjuntos tiene base numerable.  \fin


\begin{propo}

Si $G$ satisface el criterio de separación $T_1$, entonces es Haussdorf.

\end{propo}

\dem

Como $G$ es $T_1$, existe un entorno $U$ que contiene a $e$ y no contiene a $g$.  Hemos visto que existe un entorno $V$ tal que $V^{-1}V\subset U$.  Entonces $gV$ y $V$ no tienen intersección, pues sería contradictorio.

Si $g$ y $g'$ son dos puntos distintos, tenemos que $(g^{-1}g)$ y $e$ tienen entornos $W$ y $W'$ disjuntos.  Entonces $gW$ y $g'W'$ son entornos disjuntos de $g$ y $g'$. \fin

\begin{cor}

Todo grupo de Lie es Haussdorf.

\end{cor}


\subsection*{Problemas}

\begin{pro}

La composición de morfismos de grupos de Lie es de nuevo un morfismo de grupos de Lie y la identidad es claramente un morfismo de grupos de Lie.  Por lo tanto podemos hablar de la categoría de los grupos de Lie.  Si $\varphi:G \rightarrow H$ es un morfismo de grupos de Lie, que además es biunívoco, entonces su inverso es  también un morfismo de grupos de Lie.

\end{pro}


\begin{pro}

Dado un elemento $g \in G$ denotamos por $I_g$ a la aplicación $I_g(a)= gag^{-1}$.  Probar que $I_g$ es un morfismo de grupos de Lie de $G$ en $G$.  Los morfismos de este tipo se llaman \index{automorfismo interior} {\it automorfismos interiores}.

\end{pro}


\begin{pro}

        Los dos axiomas que definen un grupo de Lie se pueden reemplazar por uno: la aplicación $( g, g') \rightarrow g. ( g')^{-1}$ de $G  \times G$ en $G$ es diferenciable donde $G  \times G$ es el producto de variedades.


\end{pro}

\begin{pro}

Dado un grupo topológico $G$, cualquier subgrupo $H \subset G$ es también un grupo topológico con las estructuras inducidas.

\end{pro}



\begin{pro}
Probar que la composición de morfismos de grupos de Lie es de nuevo un morfismo.  Lo mismo con el inverso.
\end{pro}




\begin{*pro} \label{pro:equivalencia}
        Demostrar que  $ i) \Rightarrow  ii)$ por lo que es redundante la definición dada de grupo de Lie.



\end{*pro}

\newpage


\section{Ejemplos de grupos de Lie}

\begin{itemize}

\item $\R^n$  con la suma habitual y la estructura de variedad natural. Este grupo es abeliano y de dimensión $n$. En general es un grupo de Lie todo espacio vectorial real o complejo de dimensión finita.

\item $\mathrm{GL} (n,\R)$ con las estructuras naturales. La fórmula del producto de matrices  $ c_{ij}=\sum a_{ik} b_{kj}$ prueba que la multiplicación es diferenciable. Lo mismo es válido para el paso al inverso (es un polinomio en las coordenadas). Análogamente se procede con $\mathrm{GL} (n,\C)$ o con $\Aut (V)$, donde $V$ un espacio vectorial de dimensión finita.



\item Si $G$ y $H$ son dos grupos de Lie, entonces $G  \times H$ es un grupo de Lie con las estructuras producto. Es lo que se llama {\sf producto directo} de los grupos de Lie \index{producto!de grupos de Lie} $G$ y $H$.  $\R^n$ es el producto de $n$  copias de $\R$. 

\item El conjunto $\mathrm{GL} (n)  \times \R^n$, donde el producto es la composición como transformaciones afines de $\R^n$. Un cálculo en coordenadas prueba que efectivamente es un grupo de Lie, cuya dimensión es   $n^2 + n$.  No debemos confundir este grupo con el producto directo de $\mathrm{GL}(n)$ y $\R^n$, aunque las variedades de ambos grupos sean iguales.


\item Sea  $A$  una  $\R$-álgebra asociativa con unidad y dimensión finita. El conjunto  $S$  de {\sf elementos invertibles} \index{elementos invertibles} de  $A$  es un grupo de Lie de la misma dimensión que $ A$.  En efecto, dado  $a \in A$  denotemos por  $m(a)$  la transformación lineal \guillemotleft multiplicar por  a\guillemotright. La función  $m : A  \rightarrow  \End (A)$  es continua por ser continua la multiplicación de $A$ (y ésta es continua por ser bilineal). Resulta que un elemento  $a$  es invertible si y solo si 
 $\det ( m(a)) \neq 0$. Esto prueba  que $S$ es abierto de  $A$. La multiplicación es diferenciable puesto que es bilineal.   La diferenciabilidad de la inversión se deduce del problema~\ref{pro:equivalencia}. $\mathrm{GL}(V)$ es un caso particular de esta construcción aplicado a $\End (V)$.  Los cuaterniones no nulos es otro ejemplo importante.



\end{itemize}

\begin{propo}

Sea $G$ un grupo de Lie y $H$ un conjunto que sea a la vez subvariedad y sugrupo.  Con las estructuras inducidas $H$ es un grupo de Lie.

\end{propo}

\dem

Por restricción  tenemos que el producto
$$
H \times H \rightarrow G
$$
es diferenciable.  Esta función tiene su imagen contenida en $H$.  Como $H$ es subvariedad podemos afirmar que el producto
$$
H \times H \rightarrow H
$$
es diferenciable.  De modo análogo se procede con la inversión. \fin


\begin{itemize}

\item El conjunto de los números complejos no nulos es un grupo de Lie.  Los complejos de módulo 1 
$$
T^1=\{ z \in \C \text{ tales que } \abs{z}=1\}
$$
forman un subgrupo y su variedad soporte es la circunferencia.   Este grupo es conmutativo, compacto y de dimensión 1. También puede obtenerse como el cociente de $\R$ módulo  $\Z$ dotado de las estructuras cociente. Este grupo recibe el nombre de {\sf 1-toro}. \index{toro} También se suele denotar por $S^1$.




\item Sea $G$ un grupo de Lie arbitrario. Sea $G_e$ la {\sf componente conexa de la unidad}.\index{componente conexa de la unidad} $G_e$ es una subvariedad abierta de $G$.  

\end{itemize}

Admitamos el siguiente teorema, debido a Eli Cartan,   \index{teorema!Cartan}que se demostrará a lo largo de estas notas:

\begin{teo}[Cartan]

Si $G$ es un grupo de Lie, $H$ un subgrupo en el sentido algebraico y un cerrado        considerado desde el punto de vista topológico, entonces  $H$  es un grupo de Lie con la operación inducida.

\end{teo}

\begin{itemize}

\item Sea $V$ un espacio vectorial real o complejo.  El conjunto
$$
\mathrm{Sl}(V)= \{A \in \mathrm{GL}(V) \text{ tales que } \det(A)=1\}
$$
es un subgrupo cerrado del grupo lineal.  Se denomina  \index{grupo!espacial lineal}{\sf grupo lineal especial}.  De modo intrínseco se puede definir como el conjunto de automorfismos de $V$ que conservan una forma de volumen prefijada.


\item Sea $V$ un espacio euclídeo real. Una aplicación lineal $A$ es una \index{isometría}{\sf isometría} si
$$
\escalar{A(x)}{A(y)}= \escalar{x}{y} \text{ para todo } x,y \in V
$$
El conjunto de isometrías
$$
\mathrm{O}(V)=\{A \in \mathrm{GL}(V) \text{ tal que } A \text{ es isometría}\}
$$
es un subgrupo cerrado.  Si tomamos una base ortonormal, las matrices que representan las isometrías son las matrices ortogonales.  Este subgrupo es isomorfo al grupo matricial
$$
\mathrm{O}(n,\R)= \{A \in \mathrm{GL}(n,\R) \text{ tales que } A A^t= \mathrm{Id}\}
$$
El conjunto $\mathrm{SO}(V)$ de las isometrías de determinante 1 es también un grupo de Lie. El grupo $\mathrm{O}(V)$ se llama \index{grupo!ortogonal}{\sf grupo ortogonal} y $\mathrm{SO}(V)$ es el \index{grupo!especial ortogonal}{\sf grupo especial ortogonal}.  La misma construcción es válida para espacios complejos.

\item Si $V$ posee una métrica de signatura $(p,q)$ del mismo modo se puede construir los grupos $\mathrm{O}(p,q,\R)$ y   $\mathrm{SO}(p,q,\R)$.  A nivel matricial tenemos
$$
\mathrm{SO}(p,q,\R)=\{A \in \mathrm{GL}(n,\R) \text{ tales que } AMA^t= M\}
$$
donde 
$$
M= \begin{pmatrix}
\mathrm{Id}_p&0\\
0 &-\mathrm{Id}_q
\end{pmatrix}
$$  En el caso complejo todas las métricas se reducen a la euclídea y no existen los correspondientes grupos complejos.


\item Si en $V$ tenemos una forma simpléctica, sus automorfismo forman el \index{grupo!simplectico} {\sf grupo simpléctico}.  Aqui no hay grupo especial, puesto que todo automorfismo simpléctico tiene determinante 1 (teorema de Liouville).  En bases adecuadas
$$
\mathrm{Sp}(n,\R)= \{ A \in \mathrm{GL}(2n,\R) \text{ tales que } A M A^t= M\}
$$
donde 
$$
M= \begin{pmatrix}
0 & \mathrm{Id}_n\\
-\mathrm{Id}_n& 0
\end{pmatrix}
$$
 A veces también se denota $\mathrm{Sp}(2n,\R)$ para indicar la dimensión del espacio vectorial.  En complejos se construye del mismo modo.




\item Si $V$ es un espacio complejo dotado de una métrica hermítica, el conjunto de isometrías de dicha métrica se denota $\mathrm{U}(V)$ y se llama \index{grupo!unitario}{\sf grupo unitario}.  A nivel matricial, tomando una base ortonormal, tenemos
$$
\mathrm{U}(n,\C)= \{ A \in \mathrm{GL}(n,\C) \text{ tales que } AA^*=\mathrm{Id}\}
$$
donde $A^*$ denota la matriz traspuesta conjugada. Las isometrías de determinante unidad forman el {\sf grupo especial unitario} $\mathrm{SU}(V)$.

\end{itemize}







\subsection*{Problemas}

\begin{pro}

Demostrar rigurosamente que $\C^*$ junto con la multiplicación es un grupo de Lie.

\end{pro}

\begin{pro}

Demostrar que los grupos $T^1$ y $\mathrm{U}(1)$ son isomorfos.

\end{pro}


\begin{pro}

        Sean $ x = (x_1, x_2)$  $ y = (y_1, y_2)$   puntos de $\R^2$. Sea
 $$
  x \cdot y = (x_1 + y_1e^{x^2} , x_2 + y_2)
  $$
 Probar que $\R^2$ es un grupo de Lie con este producto.


\end{pro}


\begin{pro}

Dados puntos de $\R^3$  consideraremos el producto
 $$(a_1 , b_1 , c_1) \cdot (a_2  , b_2 , c_2) = (a_1+ a_2 , b_1 +b_2 , c_1+ c_2 + \frac{1}{2}(b_1 c_2 - a_1b_2))$$
Probar que es un grupo de Lie, llamado {\it grupo de Heisenberg} \index{grupo!de Heisenberg}.

\end{pro}

\begin{pro}

Dado un espacio vectorial (sobre $\R$) de dimensión finita, demostrar que el grupo proyectivo asociado es un grupo de Lie de dimensión $n^2-1$.  Generalizar al caso complejo.

\end{pro}


\begin{pro}

Probar que los cuaterniones de norma unidad forman un grupo de Lie. ¿Cuál es la variedad del grupo?

\end{pro}


\begin{pro}


Probar que  $G_e$  es un subgrupo normal de  $G$. Probar que  $G/G_e$ es un espacio topológico discreto y por tanto un grupo de Lie de dimensión cero. Debido a este resultado la teoría de grupos de Lie se reduce (en gran parte) al estudio de los grupos conexos y al de los grupos discretos.

\end{pro}


\begin{pro}

Consideremos la aplicación
$$
\begin{array}{cccc}
\mu: & \mathrm{Gl}(n) & \rightarrow & \mathrm{Gl}(n) \\
 & A & \rightarrow& A^tA
 \end{array}
 $$
Demostrar que $\mu$ es diferenciable y que $\mu^{-1}(\mathrm{Id})$ es una subvariedad, que es justamente el grupo ortogonal $\mathrm{O}(n)$.  Concluir de esta manera que $\mathrm{O}(n)$ es un grupo de Lie.  Cambiar la aplicación $\mu$ para obtener por el mismo método el resto de los grupos clásicos.

\end{pro}




\begin{pro}

Aceptemos el teorema de Cartan del subgrupo cerrado. Sea   $\varphi : G  \rightarrow   H $ un morfismo de grupos continuo. Pruébese que $\Ker(\varphi)$   es un grupo de Lie.

\end{pro}

\begin{pro}


Probar que las intersecciones de grupos clásicos son grupos de Lie.

\end{pro}



\begin{pro}

Probar que $\mathrm{SO}(n,\R)$, $\mathrm{O}(n,\R)$, $\mathrm{U}(n)$, $\mathrm{SU}(n)$ tienen como varie\-dad soporte un espacio compacto. Sea $T_2$ una métrica de signatura $(p , q)$. Probar que el grupo\footnote{El grupo $\mathrm{O}(3,1)$ se denomina grupo de Lorentz} de la métrica $\mathrm{O}(p, q)$ no es compacto.

\end{pro}

\begin{pro}

$\mathrm{GL}(n)$ es un subgrupo cerrado de $\mathrm{GL}(n+1)$ vía el morfismo
$$
A \rightarrow \begin{pmatrix}
A & 0 \\
0 & 1\end{pmatrix}
$$
De modo análogo se procede con otros grupos clásicos.  El grupo de las afinidades de $\R^n$ se puede considerar incluido en $\mathrm{GL}(n+1)$ mediante
$$
(A,t) \rightarrow \begin{pmatrix}
A & t \\
0 & 1\end{pmatrix}
$$

\end{pro}

\begin{pro}

El conjunto $\mathrm{UT}_n(\R)$ (upper triangular) de las matrices triangulares superiores es un subgrupo cerrado del grupo lineal. El conjunto $\mathrm{SUT}_n(\R)$ de las matrices triangulares en cuya diagonal solamente hay unos, es otro subgrupo cerrado.

\end{pro}

\begin{pro}

La función
$$
t \rightarrow \begin{pmatrix}
1 & t \\
0 & 1\end{pmatrix}
$$
es un isomorfismo de grupos de $\R$ y $\mathrm{SUT}_2(\R)$.

\end{pro}

\begin{pro}

Sea $G$ un grupo cerrado de $\mathrm{GL}(V)$. El conjunto
$$
G^+= \{ g \in G \text{ tales que  } \mathrm{det}(g)>0\}
$$ es un grupo cerrado de $G$.

\end{pro}


\newpage


\section{Álgebras de Lie}

Expondremos en este capítulo algunas nociones fundamentales de la teoría de álgebras de Lie, con el fin de familiarizarnos algo más con esta estructura algebraica, importantísima en el estudio de los grupos de Lie. Para ciertos ejemplos será necesario tener conocimientos un poco avanzados de geometría diferencial, puesto que la estructura de álgebra de Lie es también básica en geometría diferencial. Todos los cuerpos  $k$  que nos aparezcan serán supuestos de característica nula.

\begin{defi}

Sea  $k$  un cuerpo. Un {\sf álgebra de Lie} \index{algebra@álgebra!de Lie} sobre  $k$  o simplemente una  $k$-álgebra de Lie es un  $k$-espacio vectorial $L$ dotado de una aplicación, notada $ [-,- ]$, de $L \times L$ en $L$ que satisface:
\begin{itemize}

 \item  Es bilineal sobre  $k$.

\item  Es antisimétrica, $ [x, y ] = - [y,x ]$

\item  {\sf Identidad de Jacobi}, $[x,[y,z ]  ] =[y,[z,x ]  ] + [z,[x,y ]  ] $ \label{identidad de Jacobi}

\end{itemize}

\end{defi}

\noindent{\bf Notas. }

\begin{itemize}

\item $[x,y]$ se lee: paréntesis de  $x$ e $y$, conmutador de $x$ e $y$  o también corchete de  $x$ e $y$.

\item Llamamos {\sf dimensión} \index{dimensión de un álgebra} de la  $k$-álgebra a la dimensión del  $k$-espacio $L$. A~veces puede ocurrir que $L$ sea un álgebra de Lie sobre varios cuerpos. En este caso debemos tener cuidado y especificar siempre el cuerpo en cuestión. Las álgebras de dimensión no finita se dirá que tienen dimensión infinita.

\item Si $[x,y]=0$  decimos que los elementos  $x$  e  $y$  conmutan. Un álgebra es {\sf abeliana}\index{algebra@álgebra!abeliana} si conmutan todo par de elementos.

\end{itemize}

\noindent{\bf Ejemplos.}

\begin{itemize}

 \item  Sea  $\mathcal{V}$  una variedad diferenciable de dimensión finita. El conjunto de todos los campos vectoriales diferenciables sobre $\mathcal{V}$, junto con el paréntesis de Lie constituyen un álgebra de Lie, cuya dimensión es infinita. La denotamos $\campos(\V)$.

\item Si  $G$  es un grupo de Lie, el conjunto $\g$ de los campos vectoriales invariantes a izquierdas junto con el conmutador de Lie es un álgebra de Lie. Su dimensión coincide con la dimensión del grupo (véase la sección \ref{sec:algebragrupo}).

\item Si  A  es una  $k$-álgebra asociativa, podemos introducir en A una nueva operación definiendo $[x,y] = xy - yx$. Este nuevo producto dota a A  de estructura de  $k$-álgebra de Lie. Un caso  importante lo tenemos cuando A  es el álgebra de endomorfismos de un espacio vectorial.

\item Si $\mathcal{V}$ es una variedad {\sf simpléctica}, \index{variedad simpléctica} entonces $C^ \infty (\mathcal{V})$ es un álgebra de Lie infinitodimensional si llamamos corchete de dos elementos a su {\sf paréntesis de Poison} \index{paréntesis de Poison}.

\item $\R^3$ con el {\sf producto vectorial}.\index{producto!vectorial}

\item Sea  A  un álgebra no necesariamente asociativa. En  A  denotamos el producto por un punto.       Decimos que un endomorfismo  $D$  de  A  es una {\sf derivación}~si \index{derivación!de un álgebra} 
$$D(a\cdot b) = D(a)\cdot b + a\cdot D(b)$$

El conjunto de derivaciones de A, $\Der(A)$, es un espacio vectorial. Si  $D$  y  $D'$ son derivaciones es fácil comprobar que $[D,D'] = D \circ D' - D'\circ D$ es una derivación. Tenemos entonces una estructura de álgebra de Lie en el conjunto de derivaciones. Los campos vectoriales sobre una variedad son un caso particular, siendo $A= C^\infty (\V)$.

\end{itemize}

        Sean ahora  $A$  y  $B$  dos subespacios de una  $k$-álgebra de Lie. Denotamos $[A,B]$ al subespacio de  generado por los vectores de la forma $[x,y]$ con $ x\in A$,
$  y \in  B$.

\begin{defi}

Un subespacio   $L' \subset  L$  es una {\sf subálgebra de Lie} \index{subálgebra de Lie} de $V$ si   \linebreak $[L',L'] \subset  L'$.  Con las estructuras inducidas es un álgebra de Lie. Un subes\-pacio  $I$  es un {\sf ideal}  de $L$  si $[I,L] \subset   I$.

\end{defi}

 Todo ideal \index{ideal de una álgebra de Lie} es una subálgebra, pero el recíproco en general no es cierto.
 
 \bigskip

\noindent{\bf Ejemplos.}

\begin{itemize}

\item Sea  $A = \mathrm{End} (V)$. El conjunto  $sl(V)$ de las matrices de traza nula es un ideal de  $A$.

\item Sea  $A = M_n (k)$ el conjunto de matrices cuadradas de orden  $n$  sobre un cuerpo  $k$.  Sea M una matriz de orden $n$. El conjunto de matrices  $B$ que cumplen  $BM + MB^t = 0$ es una subálgebra. Si $M$ es la identidad el álgebra se denota $so(n)$.

\item Sea  $\mathcal{V}$  una variedad con un elemento de volumen $\tau$.  El conjunto de campos de {\sf divergencia} \index{divergencia} nula (campos tales que $D^L \tau  = 0$) es una subálgebra.

\item Sea  $\mathcal{V}$  una variedad riemaniana o semirriemaniana con métrica  $T_2$. El conjunto de {\sf campos Killing} \index{campo!Killing} ($D$ es Killing si $D^L T_2 = 0$) es una subálgebra.

\item Sea  $\mathcal{V}$  simpléctica. Los {\sf campos localmente hamiltonianos} \index{campo!localmente hamiltoniano} ($D^L \omega_2 = 0$) forman una subálgebra.

\item Supongamos dada en $\mathcal{V}$ una distribución de campos  p-dimensional y regu\-lar. Decimos que un campo vectorial pertenece a la distribución si en todos los puntos el vector correspondiente pertenece al subespacio de la distribución. Según el teorema de integrabilidad de Frobenius \index{teorema!Frobenius} dicha distribución es integrable sí y sólo si los campos de dicha distribución forman una subálgebra de Lie del álgebra de campos de la variedad.

\item $\Der  (A)$ es una subálgebra de $\mathrm{End} (A)$.

\end{itemize}

\begin{defi}
Sean $L$ y $M$   dos álgebras de Lie. Una aplicación  $\varphi : L \rightarrow  M$   es un {\sf morfismo de álgebras de Lie} \index{morfismo!de álgebras de Lie} si:
\begin{itemize}
\item $\varphi$ es lineal.
\item $\varphi ([x,y]) = [\varphi(x),\varphi(y)] \text{ para todo } x,y$.
\end{itemize}
\end{defi}

La identidad es un morfismo de álgebras de Lie y la composición de estos morfismos es de nuevo un morfismo de álgebras. Tenemos pues el concepto de categoría de  $k$-álgebras de Lie. \index{categoría de $k$-álgebras de Lie}

Algunas propiedades inmediatas de los morfismos de álgebras de Lie son las que siguen:
\begin{itemize}

\item La inyección canónica de una subálgebra es un morfismo.

\item Si $\varphi : L \rightarrow   M$   es un morfismo, la antiimagen de un ideal es un ideal y la imagen de una subálgebra es una subálgebra.

\item Si $\varphi$  y $\phi$  son morfismos $\lambda\varphi$, $\varphi + \phi$ también.

\item Los isomorfismos de esta categoría coinciden con los morfismos biunívocos. Las nociones de inyectividad y epiyectividad categóricas y conjuntistas coinciden.

\end{itemize}

Sea ahora  $I$  un ideal de $L$. Sobre el conjunto $L/I$ existe una única estructura de  $k$-álgebra de Lie que hace que la proyección canónica   sea un morfismo de álgebras.
Naturalmente dicha estructura viene definida por la fórmula:
                        $$[\pi(x), \pi(y)] = \pi([x,y])$$
que es independiente de los representantes. Decimos que $L/I$ es el {\sf cociente} \index{algebra@álgebra!cociente} de $L$ módulo $I$.


\bigskip

Sean    $L$ y $M$     dos álgebras. Sobre el conjunto $L \times M$   introducimos el corchete:
                        $$[(x,x'),(y,y')]  = ([x,y],[x',y'])$$
Con este producto componente a componente hemos construido lo que se denomina {\sf producto directo de las álgebras} \index{producto!de álgebras de Lie} $L$ y $M$. Las proyecciones canónicas son morfismos.


\bigskip

Sea ahora $L$ una  $k$-álgebra  y  $k \hookrightarrow   K$  una extensión del cuerpo base. Sobre $ L \otimes_k K$ existe una estructura de espacio vectorial sobre  $K$. Por las propiedades  del producto tensorial existe una única aplicación  $K$-bilineal en  $ L \otimes_k K$  que cumple.
$$
[x \otimes \lambda , y \otimes\mu]=[x,y]\otimes \lambda \mu
$$
Esta aplicación dota a  $ L \otimes_k K$ de estructura de $K$-álgebra de Lie. Decimos que  $ L \otimes_k K$  se ha obtenido de $L$ por {\sf extensión de escalares}. \index{extensión de escalares}  En adelante denotaremos por $L_K$ a esta $K$-álgebra.  Extendiendo por $K$-linealidad los morfismos, puede verse que la asociación $L \rightarrow  L_K$ es un functor covariante de la categoría de  $k$-álgebras en la de \mbox{$K$-álgebras.}

\bigskip

Sean de nuevo $L$ y $M$  $k$-álgebras. Consideremos el  $k$-espacio $L \otimes M$. Sobre este  $k$-espacio existe una estructura de  $k$-álgebra de Lie que cumple:
$$
[x \otimes y', x' \otimes y'] = [x,y] \otimes [x',y']
$$
Esto es consecuencia  de la propiedad universal del producto tensorial. Decimos que $L\otimes  M$ es el {\sf producto tensorial de las álgebras} \index{producto!tensorial de álgebras} $L$ y $M$.

\bigskip

Supongamos ahora que $L$  es de dimensión finita y sea  $X_1,\cdots, X_n$  una base de $L$.
Las constantes $C_{ij}^k$ que satisfacen:
$$
[X_i,X_j] =  C_{ij}^k X_k
$$
se denominan {\sf constantes de estructura} \index{constantes de estructura} del álgebra en la base dada y coinciden con las componentes del tensor $[-,-]$  en dicha base. Por lo tanto las constantes se comportan tensorialmente frente a cambios de base. Una gran parte de la teoría de álgebras de Lie consiste en encontrar bases del álgebra donde las constantes de estructura sean lo más simples posibles.  Dichas constantes no son arbitrarias sino que satisfacen ciertas condiciones.


\begin{lema}

Las constantes de estructura en una base dada satisfacen:
\begin{itemize}
\item $C_{ij}^k = -C_{ji}^k$

\item $(C_{ij}^l C_{lk}^s + C_{ki}^l C_{lj}^s + C_{jk}^l C_{li}^s  ) = 0$

\end{itemize}
\end{lema}

\dem


Estas condiciones son equivalentes a que los elementos de la base satisfagan la anticonmutatividad y la identidad de Jacobi. \fin



	Si  $E$  es un espacio vectorial de dimensión finita sobre $\R$ y $[-,-] : E \times E \rightarrow   E$   una aplicación bilineal, tal que en una base dada $\{e_i \}$ se cumpla:
\begin{itemize}

\item $\left[e_i, e_j \right] = - \left[e_j, e_i \right]$

\item $\left[ \left[ e_i, e_j\right], e_k \right] + \left[ \left[e_j, e_k \right], e_i\right] + \left[ \left[e_k, e_i \right], e_j\right] = 0$
\end{itemize}
para todo  $i$, $ j$, $k$,  entonces  $E$  dotado de ese producto es un álgebra de Lie. Dicho de otra forma: para que un espacio vectorial con una aplicación bilineal sea un álgebra de Lie basta con que se satisfagan las condiciones que la definen para los elementos de una base arbitraria. Entonces, dado un tensor de tipo (1,2) que cumpla las igualdades del lema anterior, tenemos un álgebra de Lie.


\bigskip

Sea   $L$ un álgebra de Lie. Para cada $x\in  L$  denotemos por
$\mathrm{Ad}_x (y) = [x,y]$. 

\begin{defi}
Llamamos {\sf métrica de Cartan-Killing} \index{métrica de Cartan-Killing} de $L$  y denotamos  $B$  a la aplicación bilineal definida como:
$$
                        B (x,y) = \mathrm{Tr} (Ad_x \circ Ad_y)
$$
\end{defi}

Esta es una métrica simétrica sobre el espacio vectorial  $L$.

\bigskip

Sea de nuevo $L$ un álgebra de Lie. Sea $\tau (L)$ el álgebra  tensorial covariante del  $k$-espacio vectorial  $L$. Consideremos en $\tau (L)$  el ideal  $I$ generado por los elementos de la forma:
$$
X Y - Y X - [X,Y]
$$

El cociente de  $L$ módulo dicho ideal es una  $k$-álgebra asociativa que denotaremos $ \mathrm{U} (L)$. 
 $\mathrm{U} (L)$  se denomina {\sf álgebra envolvente} \index{algebra@álgebra!envolvente} del álgebra de Lie. Este álgebra contiene de modo natural a  $L$  y goza de la siguiente {\sf propiedad universal}:\index{propiedad universal!álgebra envolvente}

\begin{quote}
 Si  $A$  es un álgebra asociativa y $ \varphi : L \rightarrow  A$  es una aplicación lineal que cumple:
$$
        \varphi([x,y]) = \varphi(x). \varphi(y) - \varphi(y) . \varphi(x)
$$
entonces existe una única aplicación de  $k$-álgebras $ \varphi_*: \mathrm{U} (L) \rightarrow A $ que sobre  $L$  coincide con  $\varphi$.
\end{quote}

Para más información nos remitimos a la sección \ref{sec:envolvente}.




\subsection*{Problemas}

\begin{pro}
        Probar el teorema de factorización canónica para  $k$-álgebras de Lie, asi como los teoremas clásicos de isomorfismo conocidos de otras estructuras.
\end{pro}

\begin{pro}
        Sea $K$ un cuerpo y  $k$  un subcuerpo. ¿Qué relación existe entre las dimensiones sobre $K$ y sobre $k$  de una misma $K$-álgebra de Lie?
\end{pro}

\begin{pro}

Demostrar que $\g$ es un ideal de $\g\times \h$. Además el cociente módulo dicho ideal es precisamente $\h$.

\end{pro}


\begin{pro}
        Probar que el álgebra de campos y el de funciones diferenciables en una variedad simpléctica son de dimensión infinita. ¿Es cierto esto para campos de Killing?
\end{pro}

\begin{pro}
Decimos que un álgebra de Lie es simple si sus únicos ideales son el cero y el total. Probar, mediante argumentos geométricos, que $\R^3$ con el producto vectorial es simple.
\end{pro}

\begin{pro}
Calcular en coordenadas la matriz de $\mathrm{Ad}_x$  y de la forma bilineal  $B$. Los antiguos decían que  $B$  se obtenía contrayendo índices. Explicar la razón.
\end{pro}

\begin{pro}
Calcular las constantes de estructura de un álgebra obtenida por extensión de escalares y del producto tensorial de álgebras, en función de las cons\-tantes de estructura \guillemotleft antiguas\guillemotright.
\end{pro}

\begin{pro}
Sea $L$  un álgebra de Lie unidimensional. ¿Cuál es el álgebra envolvente de $L$? Hacer lo mismo con un álgebra abeliana de dimensión  $n$.
\end{pro}


\begin{pro}
Probar que si $\varphi : L \rightarrow M$  es epiyectivo, existe una correspondencia biunívoca entre ideales de $M$  e ideales de  $L$ que contienen a $\Ker(\varphi)$.
\end{pro}

\begin{pro}

Sea  $k(x_1,\cdots, x_n)$  el anillo de polinomios en $n$ indeterminadas. Calcular las derivaciones de este álgebra. ¿Cómo actúa el conmutador en este álgebra?

\end{pro}


\begin{pro}

En $\R^3$ denotemos por $l_x = x, l_y =  y , l_z = z$.
Consideremos el elemento\linebreak  $L^2 = l_x^2 + l_y^2 + l_z^2$ del álgebra envolvente. Probar que  $L^2$ conmuta con todos los elementos del álgebra de Lie.

\end{pro}

\begin{*pro}
Llamamos {\it álgebra derivada} \index{algebra@álgebra!derivada} de un álgebra de Lie $L$ y denotemos
$D(L)$ al ideal $[L,L]$. Inductivamente definimos 
$$D^n
(L) = D (D^{n-1} (L))$$
Decimos que un álgebra de Lie es {\it resoluble} \index{algebra@álgebra!resoluble} si existe $n$ tal que
$D^n (L) = 0$

\smallskip


Sea $L$  resoluble. Entonces las subálgebras y los cocientes de  $L$  son reso\-lubles.
Sea $ 0 \rightarrow  L'\rightarrow L\rightarrow  M \rightarrow  0$  exacta. Entonces  $L$ es resoluble sí y sólo si son resolubles $L'$ y~$M$.
Utilizando que $\mathcal{AB/B = A/AB}$   probar que la suma de los ideales resolubles de $L$ es resoluble. Concluir que en toda  $k$-álgebra de Lie existe un ideal resoluble, máximal entre el conjunto de ideales resolubles. Dicho ideal se denomina {\it radical} \index{radical de un álgebra} de $L$ y se denota   $r$.
Probar que $L/r$ tiene radical nulo.
\end{*pro}

\begin{*pro}
Una  $k$-álgebra $L$ es {\it semisimple} \index{algebra@álgebra!semisimple}  si  $B$  es no degenerado.  Probar que si $I$ es un ideal  su ortogonal también es un ideal.
Probar que si $I$ es un ideal la métrica de Cartan-Killing en $I$ coincide con la restricción de la métrica de $L$. En general esto es falso para subálgebras.
Probar que todo álgebra semisimple es isomorfa al producto directo de álgebras simples.
\end{*pro}

\begin{*pro}
En $\mathrm{End} (V)$ definimos la siguiente métrica:
$$
                        t (A, B) = \mathrm{Tr} (A \circ B)
$$
Calcular la forma de Cartan-Killing en $\mathrm{End} (V)$ en función de $t$. Hacer lo mismo con $sl (V)$ y con $so (n)$.
\end{*pro}










\newpage


\section{Álgebra de Lie de un grupo}\label{sec:algebragrupo}



	
Para cada  $g \in G$  sean $\lag$  y $\rg$  las funciones.
$$		\lag :  G \rightarrow  G	\qquad \qquad 	\lag (g') =  g .\:  g'$$
$$		\rg :  G \rightarrow  G	\qquad \qquad 	\rg (g') =  g' .\: g$$
$\lag$ se llama {\sf traslación a la izquierda} \index{traslación!a la izquierda} por  $g$  y  $\rg$   se llama {\sf traslación a la derecha} \index{traslación!a la derecha} por $g$. Tanto $\lag$  como   $\rg$  son difeomorfismos de  $G$. La diferenciabilidad de  $\lag$ es consecuencia del siguiente diagrama:


\begin{center}
  \begin{tabular}{ccccl}
       $G$   & $\lto$ & $G \times G$ &   $\rightarrow $  & $G$      \\ 
    $ g'$     & $\lto$  & $(g,g')$      & $\lto$      & $g.g' =\lambda_g(g')$      \\ 
  \end{tabular} 

\end{center}

\noindent De la propiedad $(\lag)^{-1} = \lambda_{g^{-1}}$  se deduce la biunivocidad. Estas aplicaciones verifican las siguientes propiedades:
\begin{itemize}

\item	$\lag \lambda_{g'} =  \lambda_{gg'}$
\item	$\rg \rho_{g'}  =  \rho_{g'g}$
\item	$\lag  \rho_{g'} =   \rho_{g'} \lag$

\end{itemize}

La aplicación $\lambda : G\rightarrow \Dif (G)$  que asocia a cada elemento  $g$  el difeomorfismo  $\lag$  es un morfismo de grupos.  $\rho : G \rightarrow \Dif (G)$  es un antimorfismo de grupos.

Una propiedad fundamental de los grupos de Lie es la que sigue:
dados dos puntos  $g$, $g'$  de $G$, existe entonces una única traslación a la izquierda que transforma el elemento  $g$   en el elemento  $g'$. Decimos por esto que el grupo de Lie es un {\sf espacio homogéneo}. 
\index{espacio!homogéneo} Esta condición de homogeneidad es importantísima, puesto que nos permitirá construir objetos globales en el grupo partiendo de objetos definidos en un punto (que normalmente será el elemento neutro). 

\bigskip

Si $\sigma :\V \rightarrow \V$ es un difeomorfismo de una variedad arbitraria,  podemos construir una aplicación que transforma campos vectoriales en campos vecto\-riales. Esta transformación se denotará  $\sigma'$  y se define de tal manera que cumpla la siguiente fórmula:
$$\left(\sigma'(X)\right)_a ={(\sigma_*)} (X_{\sigma^{-1}(a)})$$
donde $\sigma_*$ es la aplicación tangente.  
Si entendemos los campos vectoriales como derivaciones del anillo de funciones esta aplicación viene definida mediante la fórmula:
$$
			\sigma'(X) = {(\sigma^*)}^{ -1} \circ X \circ \sigma^*
$$



\begin{defi}

El campo $\sigma' (X)$  se denomina {\sf imagen directa} del campo \index{imagen directa de un campo} $X$  por el difeomorfismo  $\sigma$.

\end{defi}

 Se cumple la fórmula $(\sigma \varphi )' =\sigma' \varphi'$  y  $\sigma'$ conserva el paréntesis de Lie.

\begin{defi}

Un campo diferenciable  $X$  sobre un grupo de Lie es {\sf inva\-riante por la izquierda} \index{campo!invariante por la izquierda} si   
$\lag ' (X) = X$     para todos los elementos  $g \in G$. 
\end{defi}

\begin{lema}

Si  $X$   y $X'$ son invariantes  y  $\lambda \in \R $  entonces  $X + X'$, $[X, X']$  y    $\lambda X$  son invariantes.
\end{lema}


\begin{defi}

Se llama {\sf álgebra de Lie} del grupo \index{algebra@álgebra!de un grupo de Lie}  $G$  al álgebra de todos los campos invariantes dotada de la operación de conmutación de campos. En general denotaremos a este álgebra por $\g$.

\end{defi}



\begin{lema}

Si dos campos invariantes coinciden en el elemento neutro, entonces coinciden globalmente.

\end{lema}

\dem


Por ser  $X$  invariante  $\lag' X = X$  para todo $g$, en especial   $X_g = (\lag)'_e X_e$.  Luego el campo vectorial queda caracterizado por su valor en el neutro (o en general por su valor sobre cualquier punto). \fin

\begin{lema}

Sea  $v$  un vector tangente a  $G$  en el elemento neutro. El campo vectorial  $X$  que cumple:
$$
(X)_g = (\lag)'_e  v
$$
es invariante, diferenciable y cumple que   $X_e = v$.
\end{lema}

\dem


La invariancia de este campo se demuestra por simple comprobación. Más dificultosa es la demostración de su diferenciabilidad. Debido a la tediosidad de esta demostración, remitimos al lector a la bibliografía. \fin

\begin{cor}

El morfismo de  $\g$  en $T_e (G)$  que asocia a cada campo inva\-riante su valor en el neutro es un isomorfismo lineal.
\end{cor}

Vía este isomorfismo se puede dotar a  $T_e (G)$  de una estructura de álgebra  de Lie. Ambas álgebras son canónicamente isomorfas.

	
En la sección \ref{sec:morfismos} probaremos que a cada morfismo de grupos se le puede asociar un morfismo entre las correspondientes álgebras.

	


\subsection*{Problemas}
\begin{pro}

Si $G$ es un grupo abeliano su álgebra también lo es.

\end{pro}

\begin{pro}


Sea  $G$  un grupo de Lie. Sea  $H$  un subgrupo en el sentido algebraico y además un abierto de la topología. Probar que entonces  $H$  es también cerrado y por el teorema de Cartan es un subgrupo de Lie. Aplíquese esto a la componente conexa de la unidad.
\end{pro}

\begin{pro}

Sea $G$  un grupo de Lie. Un campo  $X$   es {\it invariante por la derecha} \index{campo!invariante por la derecha} si ${(\rho_g)}^{'}  X = X$
para todo $g \in G$. 
\begin{itemize}
\item El conjunto de campos invariantes a la derecha forman un álgebra de Lie.

\item Cada campo invariante a la derecha queda caracterizado por su valor en el neutro.

\item Sea   $ i  : G \rightarrow  G $ la aplicación tomar inverso. Probar que  $i$  establece un isomorfismo entre las álgebras de campos invariantes a izquierda y  a derecha.

\item ¿Qué relación existe entre las estructuras de álgebra de Lie inducidas en  $T_e (G)$ por los campos invariantes a izquierda y a derecha ?

\item Supongamos que  $G$  no es conmutativo  ¿Es posible que un campo   $X$   sobre   $G$  sea a la vez invariante a la izquierda y a la derecha?

\end{itemize}
\end{pro}


\begin{pro}

Sea   $G$   un grupo de Lie y $G_e$ la componente conexa del neutro. Probar que las álgebras de Lie de   $G$    y de    $G_e$   son canónicamente isomorfas.
\end{pro}




\newpage

\section{Exponencial de matrices}\label{sec:exponencial}

Sea  $V$  un espacio vectorial real o complejo, que por simplicidad supondremos de dimensión finita. Consideremos en   $V$   una norma cualquiera. Los resultados serán independientes de la norma elegida, pues en dimensión finita todas son equivalentes. Como sabemos  $V$   es completo  (Banach).

Consideremos en   $\End (V)$  la siguiente {\sf norma}, \index{norma de un endomorfismo} que denotaremos igual que la norma de   $V$   para no recargar la notación.  Si   $T \in \End (V)$
 $$
 \parallel T \parallel  =\sup _{\parallel e\parallel =1}\parallel T(e)\parallel
$$
$\End(V)$ dotado de esta norma es un espacio de Banach. Esta norma también se puede definir como el menor número que satisface
$$
\parallel T (e) \parallel  \leq \parallel T \parallel \cdot   \parallel e \parallel  \text{ para todo }  e \in V
$$
La norma que hemos introducido en $\End (V)$  posee la {\sf propiedad submultiplicativa}. \index{propiedad submultiplicativa}
$$
\parallel T \circ T' \parallel \leq   \parallel T \parallel \cdot \parallel T '\parallel \text{ para todo } T, T'
$$

\begin{lema}


La composición es una aplicación continua de 
$$\End(V)
\times \End(V) \rightarrow \End(V)$$

\end{lema}

Esta proposición es consecuencia inmediata de la propiedad submultiplicativa.

\begin{defi}

Sea $T$  un endomorfismo. Llamamos {\sf exponencial} \index{exponencial!de un endomorfismo} de   $T$  y denotamos  $\exp(T)$  a la suma de la siguiente serie:
$$
 \sum _{i=0}^\infty \frac{T^i}{i!}=\mathrm{Id} + T  + \frac{T^2}{2!}+ \cdots
$$
\end{defi}

\begin{lema}


La serie anterior converge para todo endomorfismo $T$.

\end{lema}


\dem

Por la propiedad submultiplicativa
$$\norma{T^i}\,\,\, \leq {\norma{T}}^{\,\,i}$$
Luego tenemos
$$
\norma{\sum_{i=n}^m{\frac{T^i}{i!}}}\,\,\, \leq \sum_{i=n}^m\frac{1}{i!}\norma{T^i} \,\,\,\leq
\sum_{i=n}^m\frac{1}{i!}{\norma{T}}^{\,\,i}
$$
Lo que prueba que esta serie es de Cauchy y por tanto converge. \fin

Si ahora tomamos una base en   $V$, a cada endomorfismo le podemos asociar una matriz y obtenemos el concepto de {\sf exponencial matricial}.\index{exponencial!de una matriz} Recordemos que todo lo que hemos hecho es independiente de la norma elegida en  $V$.


\begin{lema}
$\exp ( AT A^{-1} ) =  A  \exp (T)  A^{-1}$
\end{lema}

\dem



Sea $S_n$ la suma parcial n-ésima de $\exp(T)$.Tenemos que $\lim_{n
\rightarrow\infty } S_n= \exp(T)$
$$
        \exp (AT A^{-1} ) = \lim_{n \rightarrow\infty } (1 + AT A^{-1} +\frac{{(ATA^{-1})}^2}{2}+ \cdots)
$$
Sabemos que ${(ATA^{-1})}^n   = A T^n  A^{-1}$. Podemos entonces sacar factor co\-mún a izquierda y derecha en la suma parcial de   $\exp (ATA^{-1})$.
$$
\exp (ATA^{-1} )  = \lim_{n \rightarrow\infty } A (S_n)  A^{-1}
$$
Ahora aplicamos que la multiplicación es continua y obtenemos.
$$
\exp (ATA^{-1} ) = A  \lim_{n \rightarrow\infty }  S_n A^{-1} = A  \exp (T) A^{-1}
$$
Esto concluye el lema. \fin



\begin{lema}

Si   $T$   y   $T'$   conmutan
$$
 \exp ( T + T' ) = \exp (T) \exp (T')
$$
\end{lema}

\dem


La demostración de esta propiedad es análoga a la demostración que se hace para los números reales. Si   $T$   y   $T'$   no conmutan esta fórmula es falsa. \fin


\begin{cor}

$\exp (T)$  es invertible para todo   $T$.
\end{cor}

\dem

$\exp(0)= \mathrm{Id}$, $\exp{(T-T)} =\mathrm{Id}$ por lo tanto
$$
\exp{(T)} \exp{(-T)} = \mathrm{Id}
$$
puesto que $T$ y $-T$ conmutan.  Así $\exp{(T)}$ es invertible.  Es más, hemos probado que  $\exp (-T ) = {(\exp (T) )}^{-1}$. \fin

\begin{cor}


La exponencial es una función de $End (V) $ en $\mathrm{GL} (V)$.

\end{cor}

Demos finalmente la fórmula del determinante de una exponencial. Podemos hacer los cálculos matricialmente y suponer que el cuerpo es  $\C$, extendiendo escalares si es necesario. Por tanto sea   $M$    una matriz    $n \times n$     compleja. Toda matriz compleja 
$M$   es triangulable, luego existe otra matriz   $B$ (de cambio de base) tal que
$B  M  B^{-1} = M'$  es triangular superior. Podemos suponer por tanto que $M$ es triangular superior. Pero en este caso unos ligeros cálculos nos dicen que $\exp(M)$ es triangular superior y su diagonal esta formada por las exponenciales de los elementos de la diagonal de $M$. Por lo tanto si $t_1, \cdots , t_n$ son los elementos de la diagonal de $M$ tenemos
$$
\det(\exp(M))=e^{t_1} e^{t_2} \cdots e^{t_n}= e^{t_1+ \cdots +t_n}= e^{\mathrm{Tr}(M)}
$$
Luego obtenemos finalmente la fórmula \index{determinante de la exponencial}
$$
\det (\exp (T) ) = \exp ( \mathrm{Tr}(  T ))
$$
donde $T$ es cualquier endomorfismo.

\bigskip

La siguiente definición de álgebra de Lie de un grupo matricial puede parecer un poco chocante, pero será evidente dentro de algunas páginas.

\begin{defi}

Sea   $G $   un subgrupo de Lie cerrado de   $\mathrm{GL} (\R^n)$. El {\sf álgebra de Lie} \index{algebra@álgebra!de un grupo matricial}   de   $G$   es el conjunto de matrices cuadradas    $M$  de tamaño $ n \times n$  tales que   $\exp (M) \in  G$. Naturalmente el paréntesis de Lie es el conmutador.
\end{defi}


\begin{propo}

El álgebra de Lie de $\mathrm{GL} (n)$ es  $M_{n \times n}$.  Dicha álgebra también se nota $gl(n)$.

\end{propo}

\begin{propo}

El álgebra de Lie de $\mathrm{SL} (n)$ son las matrices cuadradas   $n \times   n $ de traza nula. Dicha álgebra se denota $sl(n)$

\end{propo}

\dem

$\exp(T) \in   \mathrm{SL} (n)  \Leftrightarrow  \det (\exp  (T)) = 1 \Leftrightarrow
  \exp (\mathrm{Tr}(T) ) = 1 \Leftrightarrow  \mathrm{Tr} (T) = 0$ \fin

\begin{propo}

El álgebra de Lie de $\mathrm{O}(n)$ esta formado por las matrices   $M$   tales que
$M^t + M = 0$. Dicha álgebra se denota $o(n)$
\end{propo}

\dem
$\exp  M  \in \mathrm{O}(n)  \Leftrightarrow \exp (M)  (\exp (M) )^t =  \mathrm{Id} \Leftrightarrow $ \\
 $ \exp  (M)  \exp  ( M^t)= \mathrm{Id}\Leftrightarrow   \exp (M + M^t ) = \mathrm{Id}
\Leftrightarrow M + M^t = 0$ \fin



\subsection*{Problemas}

\begin{pro}

Probar que en efecto $\norma{-}$ es una norma.  Demostrar que $\norma{A}\,\,\leq \sum_{ij} \abs{a_{ij}}$.

\end{pro}

\begin{pro}
Probar que la exponencial es una función continua.
\end{pro}


\begin{pro}

Dada una matriz $M$ (posiblemente singular) la aplicación
$$
\begin{array}{cccc}
\gamma_M:& \R & \rightarrow & \mathrm{Gl}(n)\\
     &     t & \rightarrow& \exp(tM)
     \end{array}
     $$
     es un morfismo de grupos de Lie.  El vector tangente a dicha curva en el neutro es precisamente $M$.
     
     \end{pro}
     
\begin{pro}

Sea $A$ una matriz real.  La matriz $A^tA$ es simétrica y por lo tanto diagonaliza.  Sea $\lambda$ el mayor de los autovalores de esta matriz.  Entonces $\norma{A}\,\,=\sqrt{\lambda}$.  Si la matriz es compleja se realiza el mismo procedimiento pero con $A^*A$, donde $A^*$ es la matriz traspuesta y conjugada.


\end{pro}     
     
     
\begin{pro}

Si $M$ es una matriz de traza nula, entonces podemos considerar que $\gamma_M$ es una función de $\R$ en $\mathrm{Sl}(n)$.

\end{pro}     

\begin{pro}
Sea $M$ una matriz cuadrada.  Probar que
$$
\exp(M^t)= (\exp(M))^t
$$

Lo mismo con la matriz conjugada y con la hermítica conjugada.
\end{pro}

\begin{pro}

Probar que efectivamente $sl(n)$ y $o(n)$ son subálgebras de Lie

\end{pro}


\begin{pro}

Calcular el álgebra de Lie de  $\mathrm{U}(n)$  y  del grupo de Lorentz.

\end{pro}


\begin{pro}

Demostrar que  $o(3)$ es isomorfa a $\R^3$ dotada del producto vectorial.

\end{pro}

\newpage

\section{Grupos uniparamétricos de traslaciones a la derecha}\label{sec:grupouniparametrico}

Si  $G$   no es compacto, en principio no puede asegurarse que un campo genere un grupo uniparamétrico global. Veremos sin embargo que los campos invariantes siempre generan grupos globales y analizaremos como son los difeomorfismos que componen ese grupo uniparamétrico.

\begin{lema}

Un automorfismo    de la variedad  $G$  es una traslación a la derecha si y sólo si   conmuta con todas las traslaciones a la izquierda.
\end{lema}

\dem

Si   $\sigma = \rg$  para algún $g \in  G$, entonces es claro que    conmuta con todas las traslaciones a la izquierda.

Recíprocamente
$$
\sigma (g) = \sigma (ge) = \sigma ((\lambda _g)(e)) = (\sigma \lambda _g)(e) = (\lambda _g\sigma)(e) = g\sigma (e)
$$
y por lo tanto $\sigma$ consiste en multiplicar por la derecha por  $\sigma(e)$. \fin


\begin{lema}

Sea $\{\tau_t\}$ un grupo uniparamétrico de difeomorfismos de la varie\-dad  $G$  y  $X$  su generador infinitesimal. Cada elemento $\tau_t$  es una traslación a la derecha si y sólo si  $X$  es invariante por la izquierda.

\end{lema}



\dem

Si todos los  $\tau_t$  son traslaciones a la derecha cada traslación a la izquierda conmuta con ellos. Se sabe entonces por ecuaciones diferenciales que el campo  $X$  es invariante por la izquierda.

Recíprocamente, si todas las traslaciones a la derecha dejan invariante a  $X$, cada automorfismo   $\tau_t$  debe conmutar con todas las traslaciones a la izquierda y por el lema anterior cada   $\tau_t$  es una traslación a la derecha. \fin


\begin{defi}

Llamaremos {\sf grupo uniparamétrico} \index{grupo!uniparamétrico} de traslaciones a la dere\-cha a todo grupo uniparamétrico de automorfismos, tal que cada automorfismo sea una traslación a la derecha.

\end{defi}

\begin {lema}

Dar un grupo uniparamétrico de traslaciones a la derecha equivale a dar un morfismo de grupos de  $\R$  en  $G$.

\end{lema}


\dem

Sea  $\varphi : \R \rightarrow  G $ un morfismo de grupos. Definimos entonces un grupo uniparamétrico de automorfismos por la fórmula
$$
 \tau_t   =  \rho_{\varphi (t)}
$$
El recíproco es igualmente claro. Debe notarse que el morfismo  $\varphi$  es una aplicación diferenciable si y solo si el grupo uniparamétrico de automorfismos también lo es. \fin

\begin{teo}

$X$  es un elemento del álgebra de Lie si y solo si genera un grupo uniparamétrico de traslaciones a la derecha.
\end{teo}

\dem

Si demostramos que $ X$  genera un grupo uniparamétrico global se concluye por los lemas anteriores. Veamos que en efecto  $X$  genera un grupo.        En primer lugar tengamos en cuenta lo siguiente

Si  $c : (a,b) \rightarrow   G$  es una curva solución del campo invariante  $X$, entonces  $ \lag  . c $ es también una curva integral.  Por ello nos basta con encontrar la curva integral que para $ t = 0$ pasa por $e$. Por el teorema de existencia y unicidad de ecuaciones diferenciales existe esa curva local $c$, al menos en un intervalo  $ (-\varepsilon ,\varepsilon) $. La aplicación
$$
\tau:(-\varepsilon ,\varepsilon) \times G \rightarrow G
$$
definida por $\tau(t,g)=g.c(t)$ es un grupo uniparamétrico local que soluciona la ecuación diferencial. Esta aplicación se prolonga fácilmente a todo  $\R$  por el método expuesto en los libros de ecuaciones diferenciales, que reza más o menos así: conocemos  $\tau_t$  para  $t$  muy pequeño. Si $ r$  es un número cualquiera debe existir un número natural tal que  $r/n$  pertenezca al intervalo  $(-\varepsilon ,\varepsilon )$. Definimos entonces
$$
\tau_r=\tau_{n(r/n)}=(\tau_{(r/n)})^n
$$
lo que prolonga el subgrupo. \fin

\begin{defi}

Llamaremos {\sf subgrupo uniparamétrico} \index{subgrupo!uniparamétrico} de  $G$  a todo morfismo de grupos de Lie  $\varphi$  de $\R$  en  $G$.

\end{defi}

Resumiendo, hemos probado el siguiente resultado.

\begin{teo}

A cada elemento del álgebra de Lie de  $G$  le corresponde un subgrupo uniparamétrico de  $G$, siendo el recíproco también cierto. Esta asociación nos permite introducir una estructura de álgebra de Lie en el conjunto de todos los subgrupos uniparamétricos de  $G$.

\end{teo}


Admitimos el siguiente resultado (veáse el problema \ref{pro:subgrupoder}):     todo subgrupo de  $\R$  que sea cerrado y discreto para la topología usual de  $\R$  es de la forma  $r\Z$, con $r$ un número real. De este resultado se obtiene la

\begin{propo}
La imagen de todo subgrupo uniparamétrico es homeomorfa o bien a  $\R$  o bien a la circunferencia.
\end{propo}



\subsection*{Problemas}

\begin{pro}\label{pro:subgrupoder}
Sea $H$ un subgrupo cerrado y discreto de $\R $.  Probar que exis\-te un elemento positivo mínimo.  Deducir de esto que $H$ debe estar formado por los múltiplos de dicho elemento mínimo
\end{pro}


\newpage

\section{Álgebra del producto}


Sean  $G$  y  $H$  grupos de Lie, $\pi_1$ y $\pi_2 $   designarán las proyecciones canónicas en el primer y segundo factor. $G \times H$  tiene estructura de grupo de Lie y su dimensión es la suma de las dimensiones de los factores.

Sea  $\varphi : K \rightarrow G \times H $ un morfismo de grupos. Esto ocurre si y sólo si \linebreak $ \pi_i \circ \varphi $~es un morfismo de grupos para  $i = 1,  2$.  Lo mismo se pueden decir de la diferenciabilidad (propiedad universal de los productos). Por tanto
$$
\varphi : \R \rightarrow G \times H$$
  es un subgrupo uniparamétrico si y sólo si    $\pi_i \circ \varphi$  son subgrupos uniparamé\-tricos. Este resultado se puede expresar de un modo más breve como
$$
\mathrm{Mor} (\R, G \times  H) \approx \mathrm{Mor} (\R , G) \times \mathrm{Mor} (\R,  H)
$$
Como simples conjuntos, el álgebra del producto de grupos es el producto de las álgebras. Esto también se puede deducir  isomorfismo
$$
 T_e (G \times H) \approx T_e(G)  \times T_e(H)
$$
  entendiendo los grupos como simples variedades. Los elementos del álgebra de Lie de
$ G \times H$  puede escribirse como un par $(X,Y)$, siendo $ X$  un campo invariante de  $G$  e  $Y$  un campo de  $H$. Es claro que todo campo de la forma $(X,0)$  conmuta con todo campo de la forma $(0,Y)$  puesto que el primero genera traslaciones a la derecha dadas por elementos de  $G$  y el segundo por elementos de  $H$  y  $\rho_a$  y  $\rho_b$  conmutan si $a$ es un elemento de  $G$  y  $b$  un elemento de  $H$. Podemos entonces afirmar que la multiplicación en el álgebra del producto de grupos se realiza \guillemotleft componente a componente\guillemotright.
$$
[(X,Y),(X',Y)] = ([X,X'], [Y,Y'])
$$

\begin{teo}

El álgebra de un producto directo de grupos es el producto directo de sus álgebras.

\end{teo}

\newpage

\section{Formas invariantes}

Denotemos por  $\omega$ es una forma diferenciable definida sobre un grupo de Lie~$G$. 

\begin{defi}
 Diremos que  $\omega$ es {\sf invariante por la izquierda} \index{forma!invariante} (o que es una {\sf forma de Maurer-Cartan}) \index{forma!de Maurer-Cartan} si  $(\lag)^*\omega= \omega$  para todo elemento $g$ del grupo, donde el asterisco denota  la imagen inversa o pull-back. 
\end{defi}

Sabiendo que la diferencial exterior conmuta con la imagen inversa es fácil probar el siguiente resultado.

\begin{propo}

Si $\omega$ y $\alpha$ son dos formas invariantes por la izquierda, entonces también son invariantes  $\omega + \alpha$, $ \omega \wedge \alpha $, $\mu \alpha$ ($\mu$ un número real) y $d\omega$. Además una 0-forma (una función) es invariante sí y sólo sí es una cons\-tante.

\end{propo}


Si  $\omega$ es una 1-forma de Maurer-Cartan  y  $X$  un campo invariante, entonces la función  $\omega (X)$ debe ser invariante y como tal una constante. Las \mbox{1-formas} invariantes se identifican de  modo canónico con el {\sf dual del álgebra de Lie} de $G$, que denotamos $\g^*$.
	
	\bigskip

Si $\omega$  es invariante, entonces   queda definida por su valor en el elemento neutro del grupo. Propagando una forma $\omega$  definida en el elemento neutro mediante la fórmula 
$$
\omega_g= (\lambda_{g^{-1}})^*\omega
$$  
se obtiene una forma definida globalmente, invariante por la izquierda y que además en el elemento neutro coincide con $\omega$. La diferenciabilidad de esta forma se demuestra siguiendo el mismo esquema que en la demostración análoga para campos. Hemos demostrado la 

\begin{propo}

La función que a cada forma invariante sobre el grupo le asocia su valor en el elemento neutro es un isomorfismo de álgebras entre el álgebra de formas de Maurer-Cartan y el álgebra exterior del espacio tangente en el elemento neutro.

\end{propo}

Supongamos ahora un grupo de Lie orientado. Todo grupo de Lie es orien\-table, pues tomando una forma de grado máximo no nula en el espacio tangente a la identidad y propagándola, obtenemos una forma de grado máximo en todo el grupo y que no se anula en ningún punto. En el caso en que  $G$ sea compacto todas las formas de grado $n$ tienen integral finita. En especial debe existir  una   que cumpla que su integral es la unidad. Esta   es lo que se llama {\sf elemento de volumen canónico} \index{elemento de volumen canónico} del grupo de Lie compacto  $G$. Se utiliza para dotar a  $G$  de una integración invariante ({\sf medida de Haar}).\index{medida de Haar} (Véanse los problemas).

\bigskip

Sea ahora  $T_2$  una métrica euclídea definida en el álgebra de Lie. Propagando  $T_2$ obtenemos una estructura riemanniana sobre  $G$. La métrica así definida es claramente invariante por la izquierda.  Del mismo modo podemos transportar cualquier métrica de tipo (p,q)  desde el elemento neutro hasta cualquier punto.


\bigskip

Ya hemos visto que si una forma es invariante, lo mismo le ocurre a su dife\-rencial exterior. El álgebra exterior asociada al espacio tangente a  $G$  en la identidad puede dotarse de un operador  $d$ de cuadrado nulo y de grado  +1. Este operador no es más que el operador diferencial exterior trasladado a este álgebra por el isomorfismo canónico entre dicha álgebra y el álgebra de formas de Maurer-Cartan. Veamos que lo podemos definir sin utilizar para nada la estructura diferenciable de la variedad, sino sólo la estructura de álgebra de Lie. Consideremos la {\sf fórmula de Cartan}\index{fórmula!de Cartan}
$$
d\omega (X,Y)= X(\omega(Y))- Y(\omega(X)) - \omega([X,Y])
$$
válida para cualquier variedad. En el caso en que las formas y los campos implicados sean invariantes se reduce a:
$$
d\omega(X,Y)= -\omega([X,Y])
$$

Mediante esta fórmula conocemos  $d$ siempre que  $\omega$ sea de grado 1. Pero como las formas de grado uno generan toda el álgebra exterior y  $d$ debe ser una antiderivación,  podemos conocer  $d\omega$ para una $\omega$  invariante arbitraria. 




\subsection*{Problemas}

\begin{pro}
Sea $\{\omega_k\}$ una base de 1-formas invariantes.  Probar que deben existir ciertas cons\-tantes $a_{ij}^k$ que cumplan
$$
d\omega_k = a_{ij}^k \: \: \omega_i \wedge \omega_j
$$
Ver que relación existe entre las constantes $a_{ij}^k$ y las constantes de estructura del álgebra en la base dual de  $\{\omega_k\}$.
Demostrar que la condición que verifican las constantes de estructura debida a la identidad de Jacobi puede deducirse de la condición  $d^2=0$.
\end{pro}


\begin{pro}
Encontrar la fórmula general de  $d$ sobre el álgebra exterior de un álgebra de Lie.
\end{pro}





\begin{*pro}

Sea  $G$  un grupo topológico localmente compacto. Llamamos medida de Haar a toda medida    $\mu \neq 0$     definida sobre la  $\sigma$-álgebra de los borelianos del espacio topológico  $G$  y que cumple:
$$
\mu (B) =  \mu (\lag B) \quad(\text{ para todo boreliano y todo }  g  \text{ de }G.)
$$

\begin{itemize}
\item Aplicando el teorema de Riez, probar que un elemento de volumen invariante a izquierdas  $\omega_n$ sobre un grupo de Lie induce una medida de Haar.

\item Probar que en un grupo compacto  $G$   es también invariante por la derecha.
\end{itemize}
{\bf Indicación. }Considérese la medida   $\mu_g$ definida por $ \mu_g(B) =  \mu  (B . g )$  y el teorema que nos asegura que la medida de Haar es única salvo constantes.Téngase en cuenta también que    $\mu(G)$ es finito por ser   $G$   compacto.

\end{*pro}


\begin{*pro}

Sea  $G$  compacto y $\mu$     la medida de Haar que hace que  $G$  tenga masa  1. Considérense los espacios  $L_p (G)$ asociados a dicha medida.

Sea  $\lag$ y $\rg$  los siguientes operadores:
$$
			(\lag  f) (x) = f (g^{ - 1}  x)
$$
$$
			( \rg f ) (x)  = f (x . g )
$$
Probar que  $\lag$ y $\rg$  son isometrías.	Considérese en especial el caso  $L_2(G)$.

\end{*pro}

\begin{*pro}

Sean  $f, h \in L_1 (G)$. 
 Su producto de convolución es 
$$
 f * h= \int\limits_{G} f(x) h(g^{-1}x) d\mu
$$
\begin{itemize}

\item Probar que   $*$  es bilineal.

\item $\norma{f*g} \,\,\,\leq \norma{f}\cdot \norma{g}$ (utilícese el teorema de Fubini) lo que implica que $L_1(G)$ es un álgebra de Banach.

\item Ver si   $L_1 (G) $ es conmutativa en general. ¿Qué funciones pertenecen al centro de  $L_1 (G$)?

\item Encontrar, si existe, la unidad del álgebra.


\end{itemize}
\end{*pro}




\newpage

\section{Diferencial de un morfismo}\label{sec:morfismos}

Sea  $\varphi: G \rightarrow H $ un morfismo. En especial tenemos que  $\varphi$  manda el elemento neutro de un grupo al elemento neutro del otro grupo.   La aplicación  $\varphi'_e : \g  \rightarrow\h$  la denotaremos en lo sucesivo por  $d\varphi$  y la llamaremos {\sf diferencial del morfismo} \index{diferencial de un morfismo}  $\varphi$. Resulta que $d\varphi$ es un morfismo de álgebras de Lie. Para probar este resultado necesitamos unos preliminares.

\begin{defi}
Sean  $\mathcal{V}$  y  $\mathcal{W}$  variedades y  $f$  una aplicación diferenciable de  $\mathcal{V}$  en  $\mathcal{W}$. Dos campos  $X$  en  $\mathcal{V}$  y  $X'$ en  $\mathcal{W}$  se dice que están {\sf relacionados} \index{campos relacionados} por la aplicación  $f$  si para todo elemento  $x \in \mathcal{V}$ tenemos:
$$
X'_{f(x)} = f'_x (X_x)
$$
Esto es equivalente a decir que para toda función  $g$  diferenciable sobre  $\mathcal{W}$  se tiene	
$$
X(f \circ g)= (X'f)\circ g
$$
\end{defi}
Un campo y su imagen directa por un difeomorfismo están relacionados por dicho difeomorfismo. Esta definición que hemos dado es lo más que se puede conseguir cuando $ f$  no es difeomorfismo.


\begin{lema}

Sea  $f$  un morfismo de variedades. $X_1$  y  $X_2$  campos en la primera variedad, $Y_1$, $Y_2$  campos en la segunda variedad. Si  $X_i$  está relacionado con  $Y_i$ mediante  $f$  entonces $[X_1, X_2]$  está relacionado con  $[Y_1, Y_2]$.

\end{lema}

\dem
\begin{equation*}
\begin{split}
[X_1, X_2] (gf) = X_1 X_2 (gf) - X_2 X_1 (gf) =\\
 X_1 (Y_2(g) f) - X_2(Y_1(g) f) =\\
 (Y_1Y_2 g) f - (Y_2Y_1 g )  f  = ([Y_1,Y_2]g)f
\end{split}
\end{equation*}
y queda demostrado. \fin


\begin{lema}

Si  $\varphi$  es un morfismo entre dos grupos de Lie y sea  $X$  un elemento del álgebra del primer grupo. Entonces $ X$  y  $d\varphi(X) $ están relacionados por la aplicación  $\varphi$.

\end{lema}

\dem

La puede realizar fácilmente el lector si tiene en cuenta la siguiente propiedad que caracteriza a los morfismos de grupos  
$$
\varphi \circ \lag =\lambda_{\varphi(g)} \circ \varphi \quad \text{ para todo } g 
$$
Derivando se deduce el enunciado. \fin

\begin{teo}

$d\varphi : \g \rightarrow \h$  es un morfismo de álgebras de Lie.
\end{teo}

\dem

El campo invariante relacionado con un elemento de  $\g$  es único y se concluye por el lema anterior. \fin

\begin{cor}
	
Si  $\varphi$  y $\phi$   son morfismos  $d (\varphi \circ \phi) = d\varphi \circ d\phi$.
Asociando a cada grupo su álgebra y cada morfismo su dife\-rencial tenemos un functor covariante de la categoría de grupos de Lie en la categoría de  $\R$-álgebras de Lie de dimensión finita.
\end{cor}

Este functor permite resolver muchos problemas planteados para grupos de Lie estudiando \guillemotleft simplemente\guillemotright\ su álgebra. El estudio del álgebra es más simple pues esta es la \guillemotleft parte lineal\guillemotright\ del grupo. De especial interés es el problema inverso del que acabamos de resolver: ¿si  $f : \g \rightarrow \h$   es un morfismo de álgebras de Lie, exis\-te entonces un morfismo entre los grupos de tal manera que  $f$  sea la diferencial de ese morfismo?  Veremos que el problema así planteado no tiene en general solución más que localmente, o  bien tiene solución global, siempre que el grupo $G$ sea conexo y simplemente conexo. En la solución de este problema que hemos planteado aparecerán frecuentemente lo que se llama morfismos locales de grupos. Los definiremos seguidamente.


\begin{defi}

Sean  $G$  y  $H$  dos grupos de Lie, $U$  un entorno del elemento neutro del grupo  $G$. Una función  $\varphi : U\rightarrow H $ se dice que es un {\sf morfismo local} \index{morfismo!local} de grupos de Lie si  $\varphi(ab) = \varphi(a) \varphi(b)$  siempre que el producto de  $a$  y  $b$  siga perteneciendo al entorno  $U$.  Se dice que es un {\sf isomorfismo local}\index{isomorfismo local} cuando  $d\varphi$  sea invertible y por lo tanto debe existir un morfismo local de grupos que sea el \guillemotleft inverso\guillemotright\  de  $\varphi$.
\end{defi}

Por el mismo método seguido para los morfismos globales se puede ver que un morfismo local induce un morfismo de álgebras de Lie.

	
	\newpage

\section{Morfismo exponencial}


Se sabe que dada cualquier matriz  $M$  de orden  $n$, se puede construir otra matriz del mismo orden, llamada exponencial de  $M$  mediante la  serie
$$
\exp (M) = \sum_{i=0}^{\infty}\frac{M^i}{i!}
$$

 Hemos visto en la sección \ref{sec:exponencial} que $\exp (M)$  es siempre una matriz invertible y su inversa es precisamente $\exp (-A)$. 
De este modo la exponencial puede considerarse como una función de dominio el álgebra de Lie del grupo $\mathrm{GL} (n)$ y con valores en dicho grupo. Natu\-ralmente en un grupo de Lie que no sea matricial esta definición carece de sentido. Veamos el argumento de tipo geométrico que nos permitirá cons\-truir una función exponencial para un grupo de Lie abstracto.

Si al número real $ t$  le hacemos corresponder la matriz  $\exp (tM)$ tenemos definido un grupo uniparamétrico en $\mathrm{GL} (n)$.   Calculemos el vector tangente a la curva $\exp (tM)$ en el elemento neutro del grupo. Derivando término a término la serie exponencial (cosa lícita) se obtiene la familiar expresión:
$$
(\exp (tM) )' = M\exp (tM)
$$

Tomando el valor para $t = 0$ vemos que el vector tangente en  $e$  es justamente~$M$. Tenemos que $\exp (M)$ coincide con  $\varphi_M (1)$, si designamos por $\varphi_M$  al subgrupo uniparamétrico generado por el elemento  $M$ del álgebra de Lie. Esta definición si es generalizable y ya no es necesario utilizar la serie.



\begin{defi}

Sea  $G$ un grupo de Lie y  $\g$  su álgebra. La {\sf función exponencial} \index{función!exponencial}$ \exp : \g \rightarrow G $ se define como
$$
\exp (X) = \varphi_X (1)
$$
donde $\varphi_X$ es el subgrupo uniparamétrico generado por $X$.

\end{defi}

\begin{propo}

Para cada $X \in \g$, $\exp(tX)$ es el grupo uniparamétrico cuyo vector tangente en el neutro es $X$.

\end{propo}

\dem


Hemos visto en la sección \ref{sec:grupouniparametrico} que para todo $X \in \g$ existe un único grupo uniparamétrico, que denotamos por $\varphi_X$, cuyo vector tangente en el neutro es~$X$.

Dado $\lambda \in \R$ tenemos que $\varphi_X(\lambda t)$ es también un grupo uniparamétrico cuyo vector tangente es $\lambda X$. Por la unicidad, $\varphi_X(\lambda t)= \varphi_{\lambda X}(t)$.  Entonces
$$
\exp(tX)= \varphi_{tX}(1) = \varphi_X(t \cdot 1) =\varphi_X(t)
$$
lo que demuestra el enunciado. \fin

\begin{teo}

 La exponencial es una aplicación diferenciable donde  $\g$ se considera con la estructura de variedad dada por un isomorfismo lineal con $\R^n$. La exponencial establece un difeomorfismo local entre un entorno del cero en  $\g$ y un entorno del elemento neutro en el grupo.

\end{teo}

\dem

Sea la variedad producto $G \times \g$. Definamos en esta variedad el siguiente campo:
$$
D_{(g,X)}=\left(X_g,0\right)
$$


En coordenadas vemos que este campo es diferenciable.  La curva integral que pasa por $(g, X )$ es $(g \exp ( tX ) , X )$. Por ello este campo genera un grupo global  de difeomorfismos que denotamos por $\tau_t$. En particular  $\tau_1$ es una aplicación diferenciable. Podemos expresar la exponencial como
 $\exp (X) =\pi \circ \tau_1 (e, X )$ donde $\pi$  denota la proyección en el primer factor del producto $G \times \g$, lo que demuestra la diferenciabilidad.

\smallskip

Para ver que es isomorfismo, para cada vector tangente $Y$ en el origen de~$\g$, tomamos la curva $t \rightarrow tY$.  Consideramos $\exp(tY)$, derivamos igualamos el parámetro a cero.  Por la proposición anterior resulta que ese vector es justamente $Y$. Entonces la diferencial  es la identidad (hemos identificado el espacio tangente a $\g$ en el origen con el espacio vectorial).  \fin



Escojamos un  isomorfismo lineal  $\gamma$  de $\g$ en $\R^n$ (o lo que es lo mismo una base del álgebra de Lie). Entonces  $\gamma \circ \exp^{- 1}$  es un difeomorfismo de un entorno del neutro de $G$  con un entorno del cero en $\R^n$. Hemos dado una carta local en un entorno del elemento neutro. Las coordenadas asociadas a dicha carta se denominan {\sf coordenadas canónicas}. \index{coordenadas canónicas} Esta definición tiene algo de confusa pues las coordenadas dependen de la base escogida en el álgebra de Lie. Componiendo con una traslación a la izquierda podemos introducir coordenadas en un entorno de cualquier punto.




\begin{teo}

Si $\varphi : G \rightarrow   H $ es un morfismo continuo, entonces es diferenciable.

\end{teo}

\dem

 Basta ver que es diferenciable en un entorno del neutro. Con las coordenadas canónicas se reduce al caso de los morfismos entre espacios vectoriales del tipo  $\R^n$  y $\R^m$. Si conocemos la imagen de $\{e_i\}$ (base canónica de $\R^n$) por el morfismo entonces, por ser morfismo de grupos, conocemos la imagen de cualquier punto cuyas coordenadas sean todas racionales. Dar este morfismo entre $\Q^n$  y $\R^m$  equivale a dar una aplicación lineal entre esos espacios. Como exigimos que el morfismo sea continuo debe ser también una aplicación lineal entre $\R^n$  y  $\R^m$. Como localmente el morfismo se puede expresar como una aplicación lineal, y como las aplicaciones lineales son diferenciables, concluimos que el morfismo es diferenciable en un entorno del neutro y por tanto globalmente diferenciable. \fin

\begin{cor}

Si dos grupos de Lie son isomorfos como grupos topológicos
entonces son isomorfos como grupos de Lie.

\end{cor}

Entonces sobre un grupo topológico que sea una variedad topológica existe a lo más una estructura de grupo de Lie. La cuestión de la existencia es  el quinto problema de Hilbert.




\bigskip

 Cuando  $A$  y  $B$  son dos matrices que conmutan (o dos elementos del álgebra de Lie de paréntesis nulo) sabemos que $\exp (A) \exp (B) = \exp (A + B)$. En el caso general tenemos que $\exp (A) \exp (B) = \exp ( f (A,B))$ donde
$f (A,B)$ es una serie donde aparecen conmutadores de $A$  y  $B$.  Es lo que se conoce como {\sf fórmula  de Campbell-Haussdorf}. \index{fórmula!de Campbell-Haussdorf}  Esta fórmula es bastante complicada y raramente se usa. Lo importante de esta fórmula es que $f (A,B)$ solo depende de la estructura del álgebra de Lie y así podemos conocer el producto de dos elementos próximos al neutro de  $G$  en función solo del álgebra. Dicho de otra forma: el álgebra de Lie determina el producto de dos elementos localmente.






\begin{propo}

Si  $\varphi  : G \rightarrow H$  es un morfismo de grupos de Lie, tenemos el siguiente diagrama conmutativo
$$
\xymatrix{
\g\ar[d]_\exp   \ar[r]^\varphi&\ar[d]^\exp \h\\
G \ar[r]^{d\varphi}& H}
$$



\end{propo}

\dem

Tenemos que   $\varphi(\exp ( tX ) )$  es un grupo uniparamétrico en  $H$. Derivando obte\-nemos que su vector tangente en el neutro de  $H$  es precisamente  $d\varphi (X)$.
Pero si a $ t$   le asociamos  $\exp (t (d\varphi(X) ) )$, tenemos un grupo uniparamétrico con la misma propiedad. Por la unicidad del grupo asociado  tenemos la siguiente igualdad, válida para todo  $t$
$$
\varphi(\exp ( tX ) ) = \exp (t  d\varphi (x) )
$$
Tomando  $t = 1$ concluimos. \fin






\begin{propo}\label{propo:conexo}

Sea  $G$  un grupo de Lie conexo. $U$  un entorno abierto del elemento neutro. Entonces $G$  está generado por el subconjunto  $U$.


\end{propo}


\dem

Restringiendo $U$  si es preciso podemos suponer que   $U$  es simétrico ($U^{-1}$ =  $U$). En este caso el grupo que genera   $U$  es  $ \cup _n  U^n$ pues fácilmente se ve que es grupo y claramente es el mínimo que contiene a   $U$. Cada   $U^n$  es un conjunto abierto pues la aplicación multiplicación es continua. Como la unión de abiertos es abierta concluimos que el grupo generado por   $U$  forma un conjunto abierto.  El complementario de   $U^n$  está formado por la unión de las clases a la derecha, distintas del subgrupo. Estos son abiertos (son homeomorfos al subgrupo) y el complementario es entonces abierto. \fin




\begin{cor}

Sea $\varphi : G \rightarrow   H$  un morfismo de grupos de Lie siendo  $G$  un grupo conexo.
 Si     $d\varphi = 0$  entonces  $\varphi(g) = e$   para todo elemento de $G$.


\end{cor}

\dem

Por el diagrama conmutativo anterior tenemos que $\varphi$  sobre un entorno de   $e$   verifica el enunciado. \fin

\begin{cor}

Sea  $G$ conexo. Sea  $k :\g \rightarrow \h$     un morfismo de álgebras de Lie. A lo más puede existir un morfismo de grupos   $\varphi: G  \rightarrow H$ que cumpla que  $d\varphi = k$.

\end{cor}


\dem

Basándonos en el diagrama conmutativo anterior, en un entorno del neutro   debe venir definido como
$$
\varphi  = \exp \circ k \circ \exp^{-1}
$$

Como  $G$  es conexo, esta función sólo puede admitir a lo más una extensión  a  todo el grupo de tal modo que siga siendo morfismo de grupos. En general esta extensión no es posible. \fin

\newpage

\section{Grupos abelianos conexos}


Como una aplicación un poco más especializada de los conceptos y resultados relacionados con la función exponencial vamos a clasificar los grupos de Lie abelianos y conexos. Los primeros grupos abelianos que se nos vienen a la cabeza son $\R^n$ y $T^m$. Por tanto el producto directo de estos dos grupos, $\R^n  \times T^m$, es otro grupo de Lie abeliano y conexo. Resulta que esos son todos los grupos abelianos y conexos. \index{grupo!abeliano conexo} La clave para demostrar este resultado es el siguiente lema.

\begin{lema}

Sea $K$  un subgrupo de $\R^n$ que forma un conjunto cerrado y discreto del espacio topológico $\R^n$.  Existen entonces vectores linealmente independientes $(e_1, \dots   , e_p)$  que cumplen que
$$
K = \{\sum z_i  e_i  \text{ con } z_i \in \Z \}
$$

\end{lema}

\dem

Por inducción sobre la dimensión de $n$.

\smallskip

Caso $n=1$. Debe existir un elemento  $x \in  K$  tal que $\mid x \mid$ es menor que el módulo de cualquier otro punto. Entonces $K = \Z . x$,  pues si no ocurriera esto encontraríamos uno de módulo menor.

\smallskip

Sea $n> 1$. Dotemos a $\R^n$ de un producto escalar. Elijamos nuevamente $e_1 \in K$  tal que  $\mid e_1 \mid \leq \mid x \mid$  cualquiera que sea $ x$  de $ K$. Entonces
 $\R e_1 \cap  K$ consiste en los múltiplos de $e_1$.

Consideremos ahora la proyección canónica
$$
\pi : \R^n \rightarrow \R^n /\R e_1 \approx \R^{n - 1}
$$

Tenemos que $\pi (K)$ es un subgrupo cerrado y discreto de $\R^{n - 1}$. Por hipótesis de inducción   $\pi(K)$ está generado por $(\pi (e_2 ),\dots   , \pi(e_p ))$. Escogiendo  $e_i$  como el representante de menor módulo, tenemos que los vectores $(e_1 ,  \dots , e_p)$  cumplen el enunciado. \fin

Completando los vectores linealmente independientes $e_1 , \dots , e_p$   hasta una base vemos que
$$
\R^n/K \approx \R^{n-p} \times T^p
$$


\begin{lema}

Sea  $G$  conexo y abeliano.  $\exp :\g \rightarrow G$ es un morfismo de grupos de Lie epiyectivo y su núcleo,  $\mathrm{Ker}(\exp)= \exp^{ - 1} (e)$,  es un subgrupo discreto cerrado.

\end{lema}

\dem


Como  $G$  es abeliano, su álgebra también. Es válida en este álgebra la relación $ \exp (X + Y) = \exp X \cdot \exp Y$,   lo que prueba que es morfismo de grupos. Su núcleo es discreto y cerrado por establecer la exponencial un difeomorfismo local. La imagen de   $G$   por la exponencial es un subgrupo de   $G$ que es un entorno del neutro. Por ser este conexo debe generar al grupo  $G$. Entonces   es epiyectiva.  \fin

\begin{teo}

Todo grupo abeliano y conexo es de la forma $\R^n \times T^m$, donde  $m$  es el rango del  $\Z$-módulo $\exp^{- 1} (e)$.

\end{teo}

\begin{cor}

Todo grupo abeliano, conexo y compacto es un toro. Todo grupo abeliano, conexo y simplemente conexo es un $\R^n$.

\end{cor}


\newpage

\section{Conexiones invariantes y exponencial}


\begin{defi}

Si $\V$ es una variedad, una {\sf conexión} \index{conexión} sobre $\V$  es un operador
$$
\nabla : \campos (\V) \times  \campos (\V) \rightarrow \campos (\V)
$$
que satisface (notando  $\nabla(X,Y  ) = \nabla_X Y  ) $

\begin{itemize}

\item $\nabla_X (Y + Y') = \nabla_X Y +\nabla_{X}Y'$
\item $\nabla_X(fY) = X(f) Y + f \nabla_X(Y)$
\item $\nabla_{fX+X'}(Y) = f \nabla_X Y + \nabla_{X'} Y$

\end{itemize}

\end{defi}


	
\begin{defi}


Un difeomorfismo $\phi$  de $\V$ es una {\sf transformación afín} \index{transformación!afín} si 
$$\nabla_{\phi' (X)} (\phi' (Y)) = \phi'(\nabla_X Y)$$
 para todos los campos $X, Y$ ($\phi'$ denota la imagen directa).

\end{defi}


Sea  $\{X_1 ,  \cdots , X_n \}$ una base de campos global sobre la variedad $\V$ (en general estas bases no existen globalmente, sino sólo localmente ). Las funciones   $\Gamma_{ij}^k$ que satisfacen
$$
\nabla_{X_i} X_j = \Gamma_{ij}^k X_k
$$
se denominan {\sf símbolos de Cristoffel}  \index{símbolos de Cristoffel} de la conexión en la base dada. Conocidos los símbolos de Cristoffel se puede reconstruir totalmente la conexión aplicando la definición.


\begin{defi}


Sea  $G$  un grupo de Lie. Una conexión  $\nabla$   sobre  $G$  es {\sf invariante} \index{conexión!invariante} (por la izquierda) si todas las traslaciones a izquierda $\lambda_g$  son transformaciones afines de $\nabla$.

\end{defi}


\begin{lema}

Sea $\nabla$   invariante. Sean  $X$ e $Y$  dos campos invariantes. Entonces  $\nabla_X Y$ es invariante. 

\end{lema}

\dem

Veamos que $\nabla_X Y$ es invariante si lo son $X, Y$. 
$$
\lambda_{g} ^{'} (\nabla_X Y)= \nabla_{\lambda_{g} ^{'}(X)} \lambda_{g}^{'} (Y) = \nabla_X Y
$$
puesto que $X$, $Y$ son invariantes.  \fin

Con ayuda de la conexión podemos construir una aplicación bilineal $\alpha: \g \times \g \rightarrow \g$ definiendo
$$
\alpha(X, Y)= \nabla_X Y
$$

\begin{lema}

Sea $\alpha: \g\times \g \rightarrow \g$ una aplicación bilineal y $\{X_1, \cdots ,X_n\}$ una base del álgebra de Lie.  Sea $\nabla$ la única conexión que cumple $\nabla_{X_i}X_j= \alpha(X_i,X_j)$. Entonces $\nabla$ es invariante.

\end{lema} 

\dem


Todo campo vectorial sobre $G$ como una combinación lineal (los coeficientes son funciones) de la base del álgebra de Lie. Si denotamos el campo por $Z$, tenemos que $Z= f_1 X_1+ \ldots + f_n X_n$.  Teniendo en cuenta esto y tras un ligero cálculo se puede demostrar que la conexión es invariante para toda transformación de la forma $\lambda_g$ y que por tanto la conexión es invariante por la izquierda. \fin

 Estos lemas nos dicen que para construir una conexión invariante sobre  $G$  basta conocer una aplicación bilineal  $\alpha$ de $ \g \times \g$ en $\g$. Los símbolos de Cristoffel, en una base de campos invariantes, son números en vez de funciones. Es obvio que en toda álgebra de Lie existen dos conexiones lineales canónicas. Una es la asociada al tensor nulo y la otra asociada a la operación paréntesis de Lie. El punto clave de éste tema es el siguiente teorema.

\begin{teo}


Sea  $\nabla$ una conexión invariante asociada a un tensor  antisimé\-trico ($\alpha(X, X ) = 0$).  Sea      $\gamma_X$ la geodésica que cumple    $\gamma_X (0) = e$,   $\dot {\gamma_X}(0) = X$, entendiendo $X$  como vector tangente en  $e$. Entonces   $\gamma_X : \R \rightarrow G$ es un subgrupo uniparamétrico.

\end{teo}

\dem

Sea   $\gamma_X$ la curva integral de  $X \in \g$   que para   $t = 0$  pasa por el neutro. Por definición
$$
\nabla_{\stackrel{.}{\gamma_X}} \stackrel{.}{\gamma_X} = \nabla_X X
$$
donde la última igualdad debe restringirse a la curva. Debido a la antisimetría del tensor tenemos que $\nabla_X X = 0$. Por lo tanto  $\gamma_X$  es la geodésica buscada y ya hemos demostrado en capítulos anteriores que es un grupo uniparamétrico. \fin


\begin{defi}

Llamamos {\sf función exponencial} \index{función!exponencial} de $\g$  en $G$, a la exponencial asociada a cualquier conexión invariante que satisfaga   $\nabla_X  X = 0$  para todo campo invariante.

\end{defi}

Ya hemos demostrado en el capítulo \ref{sec:exponencial} que este isomorfismo es una aplicación diferenciable y es un isomorfismo local entre los elementos neutros del álgebra y del grupo.

\subsection*{Problemas}

\begin{pro}

Demostrar, sin recurrir a temas anteriores, que en efecto $\gamma_X$ es un grupo uniparamétrico.  Para ello comprobar que las funciones $\phi(s) \phi(t)$ y $\phi(s+t)$ cumplen  la misma ecuación diferencial y concluir aplicando el teorema de unicidad para ecuaciones diferenciales.

\end{pro}

\begin{pro}

Demostrar que 
$$
\alpha(t) =\begin{pmatrix}
  \cos (t)        &    \sin (t) \\
   -\sin (y)       &     \cos (t)
\end{pmatrix}
$$
es un grupo uniparamétrico del grupo lineal de dos dimensión.  Calcular su {\rm gene\-rador infinitesimal}, \index{generador infinitesimal}  (el elemento del álgebra de Lie que es tangente a la curva para $t=0$).  Reconstruir el grupo uniparamétrico utilizando la exponencial.

Hacer lo mismo con 
$$
\beta(t)=\begin{pmatrix}
  \cosh (t) + \sinh (t)       & 0 \\
   \sinh (t)       &    \cosh (t) - \sinh (t)
\end{pmatrix}
$$
$$
\gamma(t)=\begin{pmatrix}
  1 +t      &  t \\
-t       &    1-t
\end{pmatrix}
$$
\end{pro}

\newpage

\section{Subgrupos cerrados}


El teorema de Cartan del subgrupo cerrado, al igual que teorema de los morfismos continuos o el
teorema que resuelve el quinto problema de Hilbert, es uno de los
teoremas que se denominan de \guillemotleft diferenciabilidad automática\guillemotright.
Basta  dar la continuidad de un cierto objeto para obtener  automáticamente su diferenciabilidad. La primera
demostración de este teorema en su forma general se debe a Eli
Cartan y data de finales de la década de los  20. Sin embargo
Von Neumman había probado ya este resultado en el caso de los
subgrupos del grupo lineal. Este  teorema  es sólo válido para grupos de Lie reales. En los grupos complejos puede ocurrir que el subgrupo sea cerrado, pero que tenga dimensión impar, siendo por tanto imposible introducir coordenadas complejas.

\begin{defi}

Sea  $H$  un subgrupo de  $G$  en el sentido algebraico y que a la vez sea un cerrado para la topología de  $G$. Llamamos álgebra de Lie del grupo  $H$, y lo denotamos por $\h$,     al  subconjunto de  $\g$:
$$
\h    = \{X \in \g \text{  tales que } \exp ( tX) \in H \text{ para todo }  t\}
$$

\end{defi}

En principio $\h$    es sólo un subconjunto de $\g$,  a pesar de que lo hayamos llamado álgebra.


\begin{propo}

$\h $   es un subespacio vectorial de $\g$.

\end{propo}



\dem

Consideremos una norma en $\g$ .  El razo\-namiento que emplearemos es independiente por completo de la norma escogida.

\smallskip

Sea $\{X_n\}$  una sucesión de elementos de $\h$, ninguno de los cuales puede ser nulo y que satisfagan:

\begin{itemize}
\item   $\lim  X_n = 0$         en la norma dada.
\item   Existe   $\displaystyle \lim_{n\rightarrow {\infty}} \frac{X_n}{\norma{X_n}}  = X$
\end{itemize}

Veamos que $\exp (tX)$ es siempre un elemento de  $H$. Dado  $t$, existe una sucesión de enteros $\{m_n\}$  tal que 
$$
 \lim_{n \rightarrow \infty}  m_n \norma{X_n}\,\, = t
 $$
        Según esto
$$
  \lim_{n\rightarrow \infty}  \exp (m_n X_n ) = \exp (tX )
$$
debido a que el límite conmuta con  la exponencial. Pero cada elemento de la sucesión está en  $H$  pues  $\exp (m_n X_n ) = \exp(X_n )^{m_ n}$   y  $H$  es un grupo.  Como  $H$  es cerrado,  $\exp (tX)$ también está en  $H$.

\smallskip

Dados $ X_1,X_2\in \h$, tenemos la curva $t \rightarrow\exp (tX_1 ) \exp (tX_2 )$. Derivando e igualando a cero el parámetro $t$,  observamos que el vector tangente a esa curva en el neutro  es justamente  $X_1 + X_2$. Como la exponencial es un difeomorfismo entre entornos de los elementos neutros, para  $t$  suficientemente pequeño se puede escribir
$$
\exp  (tX_1)  \exp  (tX_2) = \exp ( f (t) )
$$

Del resultado anterior deducimos que  $f(t)/t$ tiende a $ X_1 + X_2$  cuando  $t$ tiende a cero. Sea  $X_n = f ( 1/n) $  y entonces $ X =(X_1+X_2)/\norma{X_1+X_2}$.  Aplicando el razonamiento del principio de la proposición constatamos que   $\exp (tX)$  es un elemento de  $H$  para todo  $t$. Luego  $\h$   es un subgrupo. Pero es  evidente por la definición que si  $X$  pertenece a  $\h$  también pertenece $\lambda X$. Esto prueba que  $\h$  es un subespacio. \fin

Se puede probar directamente que  $\h$  es también una subálgebra sin, embargo nosotros demostraremos este resultado como corolario del teorema de Cartan, para cuya demostración solo se necesita que  $\h$  sea subespacio.

Tenemos que ver ahora que la exponencial establece un difeomorfismo de un entorno del origen de  $\h$   con un entorno del neutro en  $H$. Como hemos demostrado que  $\h$  es subespacio, este constituirá una carta local en un entorno del neutro, lo que va a dotar a  $H$  de estructura de variedad. Para otro punto cualquiera de  $H$  consideramos la composición de una traslación con la carta del elemento neutro que nos dará otra carta compatible.


\begin{propo}

Existen entornos  $U\subset \g$  y  $V\subset G$  tales que
$$
\exp (U \cap \h ) = V\cap H
$$


\end{propo}


\dem

La realizaremos por reducción al absurdo. Escribamos  $\g = \h \oplus E$     donde  $E$  es un subespacio suplementario de  $\h$  totalmente arbitrario.

\smallskip

Sea   $\varphi : \g \rightarrow G$  definido como   $\varphi (X , X' ) = \exp (X)  \exp (X') $    donde hemos utilizado la descomposición de $\g$   en suma directa. El teorema de la función inversa prueba que  $\varphi$   es un difeomorfismo local en un entorno del cero.

Si no existieran los entornos del enunciado, satisfaciendo las condiciones indicadas, podríamos construir una sucesión $\{X_n ,X'_n \}$  de elementos de $\g$  tales que
$\exp (X_n )  \exp (X'_n )$  pertenezca a  $H$, siendo  $X'_n$  distinto de cero para todo $ n$. Pero como  $\exp( X_n)$  pertenece a  $H$,  también debe pertenecer a  $H$  el elemento $\exp (X'_n )$.

Sea la sucesión  $X'_n/\norma{X'_n}\,\,$.  Por compacidad debe existir una subsucesión de ésta que converja. Llamaremos a un elemento de ese tipo $ \xi $.        Entonces
 $$\xi = \lim_{n\rightarrow \infty} \frac{X'_{n_k}}{\norma{X'_{n_k}}}$$

 Este $\xi$   sería un elemento no nulo, pues debe tener norma  1  y pertenecería tanto a  $\h$  como a  $E$. Pero aquí hay una contradicción pues la suma de  $\h$  y  $E$ es directa. \fin


Es claro que  $H$  es una subvariedad regular de  $G$  pues puede definirse localmente como ceros de funciones.



\begin{teo}[del subgrupo cerrado] \index{teorema!del subgrupo cerrado}


Sea $G$ un grupo de Lie. $H$ un subgrupo de la estructura
algebraica de $G$ que es cerrado para la topología. Entonces $H$ es
un subgrupo de Lie. $H$ es además un subgrupo de Lie regular.

\end{teo}

Veamos algunos corolarios que se deducen de un modo inmediato de este teorema.

\begin{cor}

$\h$  es el álgebra del subgrupo  $H$  y por tanto además de sub\-espacio vectorial es  subálgebra.

\end{cor}

\begin{cor}

Los grupos clásicos $\mathrm{SU} (n)$, $ \mathrm{SO} (n)$, $\mathrm{SL} (n)$,$\dots$ son grupos de Lie.

\end{cor}

\dem

La definición matricial de estos grupos prueba que el espacio que
ellos comprenden es un cerrado, pues se definen igualando funciones
continuas a constantes. \fin

\begin{cor}

Todo morfismo continuo entre grupos de Lie es diferenciable.

\end{cor}

\dem

La gráfica de un morfismo continuo de $G$ en $G'$ es un subgrupo
cerrado del producto directo $G \times G'$. Como es diferenciable
el subgrupo, también es diferenciable el morfismo. \fin


\begin{cor}

El núcleo de un morfismo de grupos de Lie es un subgrupo de Lie cuyo álgebra de Lie es el núcleo de la diferencial del morfismo.

\end{cor}

\dem


Teniendo en cuenta el diagrama conmutativo de la exponencial, se concluye que la exponencial de un elemento está en el núcleo del morfismo de grupos si y solo si el elemento está en el núcleo de la diferencial del mor\-fismo. \fin

\begin{cor}

Cualquier subgrupo compacto de  $G$  es de Lie.

\end{cor}

\dem

En  una variedad todo compacto es cerrado. \fin



\begin{cor}

La intersección de dos subgrupos cerrados es un subgrupo de Lie, cuya álgebra es la intersección de las respectivas álgebras.

\end{cor}



\begin{defi}

Sea  $G$  un grupo de Lie. Llamamos centro de  $G$  \index{centro de un grupo} y denotamos $C$ (o $C_G$  si hiciese falta) al subgrupo de  $G$  formado por los elementos que conmutan con todo elemento de  $G$.
$$
C = \{g \in G \text{ tales que } gb = bg  \text{ para todo } b \in G \}
$$

\end{defi}

\begin{propo}

$C$ es un subgrupo cerrado.
\end{propo}



\dem

Sea $\{g_i\}$ una sucesión de elementos de $C$ que converge hacia $g$. Como
 $g_ib = bg_i$, pasando al límite, y por ser la multiplicación una aplicación continua, tenemos que $gb = bg$.

\subsection*{Problemas}

\begin{pro}
Calcular el álgebra de Lie del centro de un grupo $G$. Ver la relación entre el centro de un grupo y el centro de su álgebra de Lie.
\end{pro}

\newpage

\section{Grupos asociados a aplicaciones bilineales}



Los grupos  que vamos a estudiar en esta sección constituyen los ejemplos más importantes de grupos de Lie. 

	
\begin{defi}


Sea  $E$  un espacio vectorial real de dimensión finita y  $T$ un tensor homogéneo ($p$  veces contravariante y $q$  veces covariante). Llamamos \index{grupo!de un tensor} {\sf grupo  del tensor $T$} a los automorfismos  $\varphi$  de  $E$  que satisfacen
$$
\varphi_*(T) = T
$$
\end{defi}

De las propiedades functoriales de la imagen directa se deduce que  forman un subgrupo de $\mathrm{GL} (E)$. Lo denotaremos en general   $\mathrm{Aut}(T)$, aunque muchas veces emplearemos otra notación para adecuarnos a la empleada habitualmente en matemáticas.

\bigskip


\noindent	{\bf Notación}

\smallskip

En general a una métrica la denotaremos por  $T_2$. El producto escalar con esa métrica se denotará $\escalar{u}{v}$. Una aplicación bilineal de  $E \times E \rightarrow E$  se denotará por una letra griega ($\phi$ por lo general). Esta operación dota a  $E$  de una estructura de álgebra (en general no será ni asociativa ni conmutativa). Emplearemos la notación $\phi ( u,v ) = u . v$. Llamamos {\sf forma matricial}\index{grupo!forma matricial} del grupo $\mathrm{Aut}(T)$  al conjunto de matrices que dejan invariante $T$ 

\begin{propo}

$\mathrm{Aut}(T)$  es un subgrupo de Lie cerrado de $\mathrm{GL} (E)$.

\end{propo}

\dem

Consideremos una representación matricial de $\mathrm{Aut}(T)$. En ese caso vemos claramente que las ecuaciones que definen el conjunto $\mathrm{Aut}(T)$ se obtienen igualando ciertos polinomios en las variables $a_{ij}$  a constantes. Debido a eso $\mathrm{Aut}(T)$ es un conjunto cerrado de $\mathrm{GL} (E)$. \fin



\begin{propo}

Sea  $E$  un  espacio vectorial y  $T_2$  una métrica. El conjunto de automorfismos de  $T_2$  consiste en los endomorfismos  $\varphi$  que satisfacen
$$
\escalar{u}{v}= \escalar{\varphi(u)}{\varphi(v)}
$$
\end{propo}

Dicho de otro modo, son las {\sf isometrías} \index{isometría}de $T_2$.

\begin{propo}

Sea $\phi$  una estructura de álgebra. Los automorfismos de $\phi$  son los   que cumplen
$$
\varphi ( u \cdot v ) = \varphi (u) \cdot \varphi (v)
$$
\end{propo}

O sea, no son más que los isomorfismos del álgebra $(E, \phi )$.

\bigskip



\noindent{\bf Ejemplos.}

\begin{itemize}

\item	Sea  $T_2$  una métrica euclidea en  $E$. Tomando una base ortonormal,  los automorfismos de la métrica son las matrices que cumplen que 
$$
M. M^t    =\mathrm{Id}
$$
Forman el grupo $\mathrm(O)(n)$, llamado \index{grupo!ortogonal} {\sf grupo ortogonal}.

\item En general si denotamos también por  $T_2$ a la matriz de la métrica en una base, los automorfismos de  $T_2$  son las matrices que cumplen
$$
M \cdot T_2 \cdot M^t = T_2
$$
El grupo de automorfismo de  $T_2$  se denota  por  $\mathrm{O} (p, q, r )$. Si no hay ceros se denota  
$\mathrm{O} (p,q)$ y si no aparecen menos unos, ni ceros, $\mathrm{O} (p)$, que es el ejemplo anterior.


\item Sea  $L$  un álgebra de Lie.El grupo de sus automorfismos es un grupo de Lie.
\end{itemize}



\begin {defi}

Sea  $T_2$  una métrica en  $E$. Llamamos {\sf derivación} \index{derivación!de una métrica} de  $T_2$  a  todo endomorfismo  $D$  de  $E$  que cumpla
$$
\escalar{D(u)}{v}+ \escalar{u}{D(v)}=0
$$
\end{defi}
Matricialmente un endomorfismo que sea derivación satisface
$$
D. g + g. D^t  = 0
$$
El conjunto de derivaciones de  $g$  se denota  $\mathrm{Der} (T_2)$


\begin{defi}

Sea $\phi$  una operación bilineal en  $E$. Llamamos {\sf derivación} \index{derivación!de un álgebra}de  $\phi$ a todo endomorfismo  $D$  de  $E$  que cumpla
$$
 D (u \cdot v) = D (u) \cdot v + u \cdot D (v) 
$$

\end{defi}
El conjunto de derivaciones de  $\phi$ se nota  $\mathrm{Der} ( \phi )$.

\begin{teo}

El álgebra de Lie de  $\mathrm{Aut}(g)$  es  $\mathrm{Der} (g)$.
El álgebra de Lie de $\mathrm{Aut}(\phi  )$ es  $\mathrm{Der} (\phi )$.

\end{teo}

\dem

No la haremos completa.  Comencemos con el caso de una métrica. Un elemento  $X$  pertenece al álgebra del grupo de la métrica sí y sólo sí  $\exp (tX)$ pertenece a dicho grupo para todo  $t$,  como se deduce del teorema de Cartan. Si $ X$  está en el álgebra se cumple
$$
\escalar{ \exp (tX) (u)}{ \exp (tX) (v)} = \escalar{u}{v}
$$

Derivamos e igualamos  $t$  a cero. Para ello es necesario recordar que una aplicación bilineal se deriva aplicando la fórmula de Leibniz.
$$
\escalar{ X (u)}{ v} + \escalar{u}{ X (v)} = 0
$$
Veamos ahora la demostración en forma matricial.
$$
\exp (tX) \cdot T_2  \cdot(\exp (tX))^t = T_2
$$
Derivando e igualando  $t$  a cero tenemos que
$$
X T_2+ T_2X^t = 0
$$
que es la fórmula matricial de una derivación. Entonces si  $X$  está en el álgebra, es una derivación.

Supongamos ahora que tenemos una operación bilineal  $\phi$. Siguiendo los mismos pasos derivamos la expresión
$$
(\exp tX )(u \cdot v) = (\exp tX) (u) \cdot (\exp tX) (v)
$$
lo que nos da, tras tomar  t = 0 
$$
X (u \cdot v) = X(u) \cdot  v + u \cdot X (v)
$$


\bigskip

Para probar los inversos se procede de la siguiente manera. Conociendo que  $X$  es una derivación se obtiene por recurrencia una fórmula que nos da como actúa  $X^n$  sobre el producto  de dos elementos. Con esto, y teniendo en cuenta la serie de la exponencial, se prueba entonces que la exponencial de una derivación es un elemento del respectivo grupo. \fin



Sea  $\V$  una variedad arbitraria de dimensión finita. Denotamos por  $A$  el álgebra de funciones diferenciables, que es de dimensión infinita.   A cada automorfismo de la variedad  $\V$  le corresponde un automorfismo del álgebra~$A$, mediante la imagen inversa.
 
 El conjunto de derivaciones del álgebra  $A$, \guillemotleft podría\guillemotright\  entenderse como el álgebra de Lie del \guillemotleft grupo de automorfismos de  $\V$\guillemotright. Podemos pensar en los campos como elementos del  álgebra de ese grupo.   El teorema de existencia y unicidad de ecuaciones diferenciales ordinarias, prueba que a todo campo le corresponde un grupo uniparamétrico de difeomorfismos de  $\V$ (en general solo localmente). Vemos la gran analogía, sobre todo si la variedad es compacta, entre el problema de integrar ecuaciones y el de hallar los subgrupos uniparamétricos del grupo de los automorfismos de  $\V$. Razonamientos parecidos se pueden hacer por ejemplo con el grupo de simplectomorfismos de una variedad simpléctica, o las isometrías de una varie\-dad riemanniana. En estos casos el \guillemotleft álgebra\guillemotright\  del grupo puede entenderse como el conjunto de campos localmente hamiltonianos o el de campos Killing.  La formulación correcta de estas intuiciones lleva al estudio de los grupos de Lie de dimensión infinita.


\newpage

\section{Espacios homogéneos y grupos cociente}


	
Dado un grupo  $G$  y un subgrupo  $H$  podemos construir el espacio cociente,  $G / H$, que es el espacio de clases (por ejemplo a la izquierda) módulo  $H$. Si el subgrupo  $H$  es normal, en  $G / H$ existe una única estructura de grupo que hace de la proyección canónica un morfismo de grupos. 

\begin{defi}


Sea $G$ un grupo de Lie y $H$ un subgrupo cerrado.  Llamamos {\sf espacio homogeneo} \index{espacio!homogéneo} al conjunto $G/H$ dotado de la topología cociente.

\end{defi}

\noindent{\bf Notas}

\begin{itemize}

\item La condición de que  $H$  sea cerrado es fundamental, pues si esto no ocurriera la topología no sería Hausdorf  en el espacio cociente.

\item  $\pi$ es continua por definición.

\item Dado   $g \in G$  existe una aplicación biunívoca de  $G / H$  en si mismo. La denotamos  $\lag$ y viene definida por la fórmula:
$$
\lag(\pi(b))= \pi (g.b)
$$
Si  $g \in   H$ entonces $\lag =\mathrm{Id}$.
\end{itemize}

El siguiente teorema es el resultado fundamental de este capítulo que expondremos sin demostración.

\begin{teo}

Sea  $G$  un grupo de Lie y  $H$  un subgrupo cerrado. Sobre el espacio topológico  $G / H$ existe una única estructura de variedad diferenciable que hace que la proyección canónica $\pi$  sea una submersión (esto es, que sea diferenciable y que la aplicación tangente sea epiyectiva en todo punto).


Existen {\sf secciones locales} \index{sección!local} diferenciables de la proyección. Esto significa que para cada punto  $x$  de  $G / H$ existe un entorno  $W_x$  abierto y una aplicación diferenciable  
$S_x : W_x \rightarrow  G$  que cumple   $\pi \circ S_x =\mathrm{Id}$ sobre $W_x$.


\end{teo}



Siempre consideraremos en  $G / H$  esa estructura de va\-riedad.
Por ser  $\pi$  una submersión, una función  $f : G / H \rightarrow\mathcal{V}$ es dife\-renciable sí y sólo sí 
$f \circ \pi : G \rightarrow\mathcal{V}$  es diferenciable, donde  $\mathcal{V}$  es una variedad arbitraria.



\begin{teo}[del grupo cociente]

Sea  $G$  un grupo de Lie y  $H$  un subgrupo cerrado normal. Sobre el espacio homogéneo $G / H$  existe una estructura de grupo cociente. Con estas estructura $G / H$ es un grupo de Lie.

\end{teo}

\dem

Debemos ver que la aplicación   de   $\overline{\varphi}:G / H \times G / H  \rightarrow G / H$  dada por
$$  
\overline{\varphi}(\pi (a), \pi (b) ) = \pi (a. b^{ - 1} )
$$
 es diferenciable. Para ello basta con comprobarlo localmente.

Sean  $S_a$  y  $S_b$  secciones locales en entornos abiertos  $W_{\pi(a)}$  de  $\pi(a)$  y  $W_{\pi(b)}$   de $\pi(b)$.	En un entorno, la aplicación que estamos considerando se expresa como
$$
\pi \circ \varphi \circ (S_a \times S_b)=\overline{\varphi}
$$
donde $ \varphi : G \times G \rightarrow G$  es la función  $\varphi(a,b) = ab^{ - 1}$. Como localmente es composición de funciones diferenciables, $\overline{\varphi}$   es diferenciable. \fin





El álgebra de Lie del grupo  $G / H$ es isomorfa al cociente del álgebra  $\g$ por el ideal $\h$, pues  $\h$  es precisamente el núcleo de la aplicación tangente a la proyección canónica en el neutro.



\begin{propo}

Sea  $G$  un grupo de Lie,  $H$  cerrado y  $G / H$ el espacio homogéneo.
Si  $G$  es compacto, también lo son   $ H$  y   $G / H$. Si  $H$  y  $G / H$  son compactos también lo es  $G$.

\end{propo}

\dem

Si $G$ es compacto, entonces al ser $H$ cerrado es también compacto. Como $G/H$ es la imagen epiyectiva de un compacto, debe ser compacto.

\smallskip

Si $H$ y $G/H$ son compactos podemos recubrir  $G / H$ mediante un número finito de abiertos donde están definidas secciones locales. La existencia de secciones locales sobre  $U$, prueba que  
$\pi^{-1}(U)$  es homeomorfo a  $U \times H$  y ya se concluye facilmente que  $G$  es compacto. \fin



\newpage


\section{Acciones de grupos de Lie}


Las acciones de grupos de Lie fue lo que en realidad estudio Lie (en su aspecto local). Una acción de un grupo de Lie en una variedad es lo que antiguamente se denominaba  un grupo continuo de transformaciones. Lie descubrió que al dar una acción del grupo construíamos una subálgebra de Lie de campos sobre la variedad en la que actúa el grupo. Es lo que se llama el álgebra de los  \index{generador infinitesimal}{\sf generadores infinitesimales}\index{campo!fundamental} o el álgebra de {\sf campos fundamentales}.


Este paso  es una demostración de tipo diferencial. Lo que también descubrió Lie es que al dar una subálgebra de campos en la variedad podíamos reconstruir, en general sólo localmente, una acción de un cierto grupo sobre la variedad. En el caso en que el álgebra que damos es unidimensional (un campo~$X$ sobre la variedad) el problema de Lie equivale a encontrar un grupo uniparamétrico que genere $X$. La solución local a este problema no es más que una nueva formulación del teorema de existencia y unicidad de ecuaciones diferenciales ordinarias.
Con este ejemplo vemos que el problema de Lie desde un punto de vista global no siempre tiene solución. Además la existencia  de un grupo uniparamétrico global, no imponía condiciones de tipo topológico a la variedad. Sin embargo para el caso de grupo de dimensión mayor que uno, si ocurre esto.

 Un espacio homogéneo puede considerarse como una variedad donde actúa de modo transitivo un grupo. Este enfoque de la teoría de espacios homogéneos es mucho más geométrico y visual. 


\begin{defi}

Una {\sf acción por la izquierda}\index{acción!por la izquierda} (que nosotros llamaremos simplemente acción) de un grupo de Lie  $G$ sobre una variedad  $\mathcal{V}$  es una aplicación diferenciable  $\varphi : G \times  \mathcal{V} \rightarrow \mathcal{V}$ que, denotando  $\varphi (g,x) = g.x $,  satisface las siguientes propiedades:


\begin{itemize}
\item  $e \cdot x = x$      para todo  $x$     ($e$   elemento neutro).
\item  $g\cdot (h\cdot x) = (gh)\cdot x$
\end{itemize}
También se dice que $\V$  es una {\sf $G$-variedad}.

\end{defi}



La aplicación 
$$
\begin{array}{cccc}
\lambda_g: & \V & \rightarrow &\V \\
         &   x & \rightarrow & g x
         \end{array}
         $$
se llama {\sf traslación a la  izquierda}\index{traslación!a la izquierda} por~$g$ y es un difeomorfismo  cuyo  inverso es  $\lambda_{g^{-1}}$. 
La función
$$\begin{array}{cccc}
\lambda:& G & \rightarrow & \mathrm{Dif}(\V)\\
          & g& \rightarrow& \lambda_g
          \end{array}
          $$
 es un morfismo de grupos. El núcleo de este morfismo se denomina\index{núcleo de una acción} {\sf núcleo de la acción}.  Sin embargo, dar un morfismo de grupos de  $G$  en $\mathrm{Dif}(\mathcal{V}) $ no es dar una acción, pues aunque se satisfagan las propiedades,    en general no se va a cumplir que  $\varphi$  sea diferenciable como función de   $G \times \mathcal{V}$  en $\mathcal{V}$.



\bigskip

\noindent{\bf Ejemplos.}

\begin{itemize}


\item Si $G$ actua sobre $\V$, cualquier subgrupo $H \subset G$ actua, por restricción, sobre $\V$.

\item La multiplicación  $m : G \times G \rightarrow G$  es una acción por la izquierda del grupo   $G$ sobre si mismo.

\item El grupo $ G$ actúa también sobre si mismo por conjugación   mediante la fórmula
 $$
\lambda_g ( g') = g . g'. g ^{- 1}
$$

\item $\mathrm{GL}(V)$ actua sobre $V$ mediante la regla $\phi \cdot v= \phi(v)$.

\item Si  $\phi : G \rightarrow\mathrm{GL} (V)$  es un morfismo de grupos de Lie, entonces  $G$  actúa en  $V$  mediante la fórmula.
$$
g . v = \phi (g) ( v)
$$
De modo similar se construye una acción del grupo de las afinidades de $V$ sobre el propio espacio $V$.

\item En general, si $\phi:G_1 \rightarrow G_2$ es un morfismo de grupos de Lie, se tiene la siguiente acción de $G_1$ sobre $G_2$.
$$
\begin{array}{ccc}
G_1 \times G_2 & \rightarrow & G_2 \\
(g,h) & \rightarrow & \phi(g) \cdot h
\end{array}
$$

\item $G$ actúa sobre el espacio homogéneo  $G / H$    ($g . \pi (a) = \pi(g . a )$).  En particular  $G$  actúa sobre todos sus grupos cociente.

\item Lo que en  ecuaciones diferenciales se denomina grupo uniparamétrico de automorfismos en  $\mathcal{V}$, no es más que una acción  de  $\R$  en  $\mathcal{V}$.

\item Si  $G$  actúa sobre  $\mathcal{V}$  y sobre  $\mathcal{W}$  entonces actúa sobre  $\mathcal{V} \times \mathcal{W}$.

\item El grupo lineal actua sobre el conjunto de todas las matrices cuadradas
$$
\begin{array}{ccc}
\mathrm{GL}_n(\R) \times M_n(\R) & \rightarrow & M_n(\R) \\
(A,M)& \rightarrow& A\cdot M \cdot A^{-1}
\end{array}
$$



\end{itemize}


\begin{defi}

Llamamos \index{acción!por la derecha} {\sf acción por la derecha} de un grupo de Lie $G$ sobre una variedad $\V$ a una aplicación de $\V \times G \rightarrow \V$ que cumple:

\begin{itemize}

\item $x\cdot e= x$

\item $(x\cdot g)\cdot h= x\cdot (g  h)$

\end{itemize}

\end{defi}


\begin{defi}

Sea $ \mathcal{V}$  y  $\mathcal{W}$  dos $G$-variedades. Una aplicación diferenciable
$\alpha : \mathcal{V} \rightarrow\mathcal{W}$ es {\sf equivariante} \index{equivariante} si $\alpha(g\cdot x)=g\cdot\alpha(x)$.

\end{defi}


Las aplicaciones equivariantes conservan tanto la estructura diferenciable como la acción del grupo. Son por ello los principales candidatos para formar los morfismos de la categoría de variedades donde actúa  $G$. Es claro que la composición de dos funciones equivariante es de nuevo equivariante y que la identidad lo es.

\bigskip



Sea  $\varphi : G \times \mathcal{V} \rightarrow\mathcal{V}$  una acción. A cada punto  $x$ de variedad  $\mathcal{V}$  se le puede asociar la función $ A_x : G \rightarrow\mathcal{V}$  
$$
A_x (g) = g . x
$$
$A_x$  es una función diferenciable y es  equivariante, actuando  $G$  sobre si mismo por medio de la multiplicación. 



\begin{defi}

Una acción es {\sf fiel}\index{acción!fiel} o {\sf efectiva} \index{acción!efectiva} si el morfismo $\lambda:g \rightarrow \mathrm{Dif}(\V)$ es inyectivo.

\end{defi}


\begin{defi}

Decimos que una acción de  $G$  en  $\mathcal{V}$  es {\sf transitiva} \index{acción!transitiva} si para cada par de puntos  $x$, $x'$  de $\mathcal{V}$, existe al menos un  $g$  que cumple que $x = g x'$.  Naturalmente ese $g$ en general no será único. Una variedad donde actúa de modo transitivo  $G$  se denomina \index{espacio!homogéneo}  {\sf $G$-espacio homogéneo}.

\end{defi}



Decimos que  $G$  actúa de modo {\sf casi efectivo} \index{acción!casi efectiva} si el núcleo del morfismo antes mencionado es un subgrupo discreto. Dada una acción no efectiva   $\varphi : G \times \mathcal{V} \rightarrow\mathcal{V}$ tenemos una acción efectiva de
$G/\mathrm{Ker}(\lambda) \times \mathcal{V} \rightarrow\mathcal{V}$ sin más que aplicar el teorema de factorización canónica al morfismo $\lambda$.

\begin{defi}

Llamamos {\sf grupo de isotropía}\ \index{grupo!de isotropía} de un punto  $x$  de  $\mathcal{V}$  al conjunto
$$
G_x = \{g  \in  G  \text{ tales que }  g \cdot x = x \}
$$
\end{defi}

Tenemos que $G_x= {A_x}^{-1}(x)$ y  $G_x$  es un subgrupo cerrado. Entonces o bien es discreto o bien es un subgrupo de Lie de dimensión mayor o igual a $1$. 

\begin{defi}

Un grupo de Lie actúa {\sf libremente}  \index{acción!libre} si el grupo de isotropía de todo punto se reduce a la identidad. Actúa de modo {\sf casi libre} \index{acción!casilibre} si el grupo de isotropía es discreto en todo punto.

\end{defi}


\begin{defi}


Sea   $\varphi : G \times  \mathcal{V} \rightarrow\mathcal{V}$  una acción. Llamamos {\sf órbita}  \index{orbita@órbita} del punto  $x$  al conjunto
$$
\mathrm{O}_x= \{(g . x ) \text{ con }  g\in G\}= A_x(G)
$$ 

\end{defi}
La relación  \guillemotleft estar en órbita\guillemotright\   es una relación de equivalencia. Las órbitas son pues las clases de equivalencia de la relación $ x\sim x'$ si existe $g$ tal que  $x = gx'$. El espacio cociente de $\mathcal{V}$ por la acción de $G$ se denomina {\sf espacio de órbitas} \index{espacio!de órbitas} y se denota $\mathcal{V}/ G$.  Siempre se puede dotar a este espacio de la topología cociente, pero el problema es que en general dicha topología no es \guillemotleft buena\guillemotright. En general  tampoco podemos dotar de estructura diferenciable al espacio de órbitas de tal forma que la proyección canónica sea diferenciable. Decir que una acción es transitiva es lo mismo que decir que existe una sola órbita. Es obvio, por definición, que una órbita es un conjunto homogéneo de la acción. 




\begin{propo}

Sea $\varphi : G \times \V \rightarrow\V$ una acción.  Entonces $A_x$ factoriza por el espacio homogéneo $G/G_x$ dando el diagrama conmutativo
$$
\xymatrix{
G \ar[r]^{A_x}\ar[d]_\pi & V \\
G/G_x \ar[ur]_{\overline{A}_x} 
}
$$
\end{propo}







Por estar dotado de estructura cociente $\overline{A}_x$ es
diferenciable.  Ademas $\overline{A}_x$ es inyectiva y es una inmersión.
La imagen de $G/G_x$ por $\overline{A}_x$ es la órbita de~$x$. La órbita de cualquier punto es una subvariedad isomorfa al espacio homogéneo $G/G_x$.

\bigskip


La característica esencial de los espacios homogéneos es la transitividad. Re\-cordemos que en el grupo de Lie hacíamos uso de la transitividad para extender los campos vectoriales a partir del valor en un punto. En el caso de los espacios homogéneos esto también se puede hacer siempre que el resultado sea independiente de la traslación a la izquierda que hayamos escogido y que transforme un punto  $x$  en  $x'$. Notemos que en la definición   de espacio homogéneo dada en la sección anterior, existía en este un
punto particular, la clase del elemento neutro. Sin embargo con la definición de homogeneidad de este capítulo esto no pasa.

\bigskip

\noindent{\bf Ejemplos.}

\begin{itemize}

\item Sea  $\R \times \mathcal{V} \rightarrow\mathcal{V}$  un grupo uniparamétrico. La órbita del punto  $x$  es justamente la curva integral del generador infinitesimal que pasa por  $x$. Entendiendo cada punto  $x$  de  $\mathcal{V}$  como una partícula puntual, la órbita de  $x$  es la trayectoria que sigue la partícula bajo la acción del sistema dinámico dado por el generador infinitesimal del grupo. Esto puede justificar el nombre de órbita que le damos a ese conjunto.

\item $\mathrm{GL} (V)$ actúa de modo natural sobre  $V$  por la fórmula   $ T \cdot e = T (e)$. Si le quitamos a  $V$   el cero es fácil  ver que   $\mathrm{GL} (V)$  actúa de modo transitivo.

\item El grupo de las afinidades    actúa transitivamente en $\R^n$.


\item El grupo de Poincaré  (producto semidirecto del grupo de Lorentz y $\R^n$)  actúa transitivamente en  $\R^n$. Sin embargo el grupo de Lorentz no actúa de modo transitivo.  Es un buen ejercicio calcular las órbitas de la acción del grupo de Lorentz.

\item $\mathrm{SO}(n)$ actua sobre $\R^n$.  Sabemos que dos puntos estan relacionados por $\mathrm{SO}(n)$ sí y solo si tienen la misma norma.  Así las órbitas de la acción son justamente las esferas de centro el origen.  Cada esfera es por tanto un espacio homogéneo de $\mathrm{SO}(n)$.  Su grupo de isotropía es fácil de calcular y coincide con $\mathrm{SO}(n-1)$. Se escribe con la fórmula
$$
\mathrm{SO}(n)/\mathrm{SO}(n-1) \sim S^{n-1}
$$

\item $\mathrm{U}(n)$ actua sobre $\C^n$ dotado
 de una métrica hermítica.  Esta acción se restringe a una acción
 de $\mathrm{U}(n)$ sobre la esfera unidad $S^{2n-1}$ de $\C^n$,
  que tiene por grupo de isotropía $\mathrm{U}(n-1)$.

\end{itemize}




Vamos a ocuparnos ahora de los asuntos relacionados con el segundo teorema de Lie.  Esto es, que al dar una acción de un grupo tenemos una subálgebra de campos en la variedad donde actúa.  Para estudiar este pro\-blema es aconsejable utilizar acciones por la derecha, que es lo que nosotros vamos a hacer.  Por ello en vez de utilizar la notación $\lambda_g$ utilizaremos $\rho_g$.  Naturalmente ahora $A_x$ multiplicará por el otro lado.

Supongamos que $\varphi :  \V \times G \rightarrow\V$ es una acción y sea $X \in \g$.
Asociado a este elemento y a esta acción tenemos un grupo
 uniparamétrico definido como
$$
\tau_t =\rho_{\exp(tX)}
$$
Designamos por $\overline{X}$ al generador infinitesimal de este grupo y lo llamamos {\sf campo fundamental}\index{campo!fundamental} asociado a $X$. 

Existe otro modo de construir este campo que no recurre a la existencia de grupos uniparamétricos. Sea $X$ un elemento de $\g$ entendido como vector tangente en el neutro.  Entonces
$$
\overline{X}_x = {A_x}'(X)
$$
Esta es la generalización del método de propagar vectores a campos en un álgebra.



\begin{defi}

Sean $\V$ y $\W$ dos variedades y $\varphi : \V \rightarrow\W$ una aplicación diferenciable.  $X$ un campo en $\V$ e $Y$ un campo en $\W$. Diremos 	que $X$ e $Y$ están relacionados \index{campos relacionados} mediante $\varphi$ cuando para cada $x \in \V$ sea $Y_{\varphi(x)} = \varphi^{'} (X_x)$.  También puede expresarse esta condición diciendo que para cada $f \in C^\infty(\W)$ se cumple
$$
 X(f \circ \varphi) =Y(f) \circ \varphi
$$

\end{defi}

De esta última condición se deduce, tras unos cálculos, que si $X_1$ está relacionado con $Y_1$  y $X_2$
está relacionado con $Y_2$ entonces $[X_1,X_2]$ está relacionado con $[Y_1,Y_2]$. Debido a la segunda definición utilizada para definir el campo $\overline{X}$, resulta que $X$ y $\overline{X}$ están relacionados  mediante la aplicación diferenciable $A_x$, donde $x$ es cualquier punto de la variedad.  Con todo esto podemos enunciar.


\begin{propo}

La función de $\g \rightarrow\campos(\V)$ dada por $X \rightarrow\overline{X}$ es un morfismo de álgebras de Lie.
\end{propo}

\begin{defi}

Sea $\g$ un álgebra de Lie sobre $\R$ de dimensión finita.  Una {\sf acción infinitesimal}\index{acción!infinitesimal} de $\g$ en $\V$ es un morfismo de álgebras de Lie de $\g$ en $\campos(\V)$.

\end{defi}

Así hemos probado el segundo teorema de Lie.  A toda acción de un grupo se le asocia una representación de su álgebra.  La imagen de $\g$ por esta representación será una subálgebra de Lie del conjunto de campos en $\V$, que en general será isomorfa a un cociente de $\g$.  En el caso en que la acción del grupo sea efectiva, el álgebra se representa inyectivamente.




\subsection*{Problemas}


\begin{pro}

Sea $G$ un grupo de Lie y $\g$ su álgebra:

\begin{itemize}

\item Si $X$ es un campo invariante por la izquierda entonces $\rho'_g(X)$ también es invariante por la izquierda.

\item La función
$$
\begin{array}{cccc}
\mathrm{ad}: & G & \rightarrow & \mathrm{Aut}(\g)\\
            & g & \rightarrow &\mathrm{ad}_g= \rho'_{g^{-1}}
            \end{array}
            $$
            es una representación lineal \index{representación!adjunta}(llamada {\it representación adjunta}).
            
\item  Sea $I_g$ el automorfismo interno. Entonces ${I_g}'= \mathrm{ad}_g$.

\item Si derivamos el morfimo $\mathrm{ad}$ obtenemos una representación del álgebra $\g$ en si misma, que se denomina {\it representación adjunta} \index{representación!adjunta} y se denota $\mathrm{Ad}$ (con mayúsculas).

\end{itemize}

\end{pro}

\begin{pro}

Si en vez de utilizar acciones por la derecha, utilizamos acciones por la izquierda, para construir los campos fundamentales debemos usar el grupo uniparamétrico
$
\lambda_{\exp(-tX)}
$
donde aparece un signo menos.  Realizar las demostraciones en este caso.

\end{pro}

\begin{pro}

Si la acción es efectiva la correspondencia $X \rightarrow \overline{X}$ es inyectiva.  Si la acción es libre, entonces $\overline{X}$ es no nulo en ningún punto.  Si las órbitas son subvariedades entonces $\overline{X}$ es tangente a dichas subvariadades.

\end{pro}


\begin{pro}
Probar la fórmula 
$
\mathrm{Ker}(\lambda) =  \bigcap_{x \in \mathcal{V}} G_x
$
\end{pro}

\begin{pro}

Si $\V$ es un espacio homogéneo entonces $G/G_x \sim \V$.

\end{pro}

\begin{pro}
Probar que las dos definiciones de campos fundamentales son equivalentes.
\end{pro}

\begin{pro}
Calcular el núcleo de la representación adjunta. 
Demostrar que el centro de un grupo de Lie es un subgrupo cerrado.
Demostrar que un grupo sin centro es isomorfo a un grupo matricial.
\end{pro}

\begin{pro}

Si $x$ y $x'$ están en la misma órbita entonces $G_x$ y $G_{x'}$ son subgrupos conjugados.

\end{pro}

\begin{pro}
Probar que si $X_1$ está relacionado con $Y_1$  y $X_2$ está relacionado con $Y_2$ entonces $[X_1,X_2]$ está relacionado con $[Y_1,Y_2]$.
\end{pro}

\begin{pro}

El espacio proyectivo complejo de dimensión $n$ es un espacio homogéneo de $\mathrm{SU}(n+1)$.  Calcula el grupo de isotropía.
El espacio proyectivo real de dimensión $n$ es un espacio homogéneo de $\mathrm{SO}(n+1)$.  Calcula el grupo de isotropía.

\end{pro}

\begin{pro}

Calcula las órbitas de la acción del grupo de Lorentz para vectores de tipo tiempo, de tipo luz y de tipo espacio.

\end{pro}

\begin{pro}

Sean $\V$ y $\V \: '$ dos variedades donde $G$ actúa por la derecha.  Denotamos por $\overline{X}$ y $\overline{X}\, ' $ los campos asociados a $X \in \g$ por la acción.  Si $\varphi : \V \rightarrow\V \:'$ es equivariante, probar que  $\overline{X}$ y $\overline{X}\, ' $ están relacionados por $\varphi$.

\end{pro}

\begin{pro}

Dada una acción  efectiva de $G$ en $\V$, probar que la correspondencia $X \rightsquigarrow \overline{X}$ es inyectiva. Si la acción  es libre entonces $\overline{X}$ es no nulo en todo punto.

\end{pro}


\newpage

\section{Álgebra envolvente y operadores diferenciales}\label{sec:envolvente}


	
Un gran problema que plantea el estudio de las álgebras de Lie es su no asociatividad. Sin embargo  a toda  álgebra de Lie  $\g$ se le puede hacer corresponder, de un modo canónico, un álgebra asociativa $ \mathrm{U} (\g )$. Este álgebra contiene  a $\g$  y en  $\mathrm{U} (\g  )$ se cumple que $[X,Y]= XY - YX$ para cualquier par de elementos del álgebra de Lie $\g$. Un precio a pagar en este paso es que el álgebra asociativa es de dimensión infinita, aunque $\g$   sea de dimensión finita. El álgebra  $\mathrm{U} (\g  )$ que vamos a construir goza de una  \guillemotleft propiedad universal\guillemotright\   que nos permite definir a $\mathrm{U} ( \g )$ como representante de un cierto functor. Algunos resultados importantes son consecuencia directa de la propiedad universal, sin embargo un teorema de vital importancia, el teorema de Poin\-care-Birkoff-Witt,  no es de demostración simple, por lo que lo expondremos sin su demostración. Este teorema nos da explícitamente la estructura del álgebra  $\mathrm{U}( \g)$  en función de la estructura de $\g$.


\bigskip

Denotaremos por  $\g$ un álgebra de Lie sobre un cuerpo de característica cero y de dimensión arbitraria. Al espacio vectorial $\g$ se le puede asociar un álgebra graduada asociativa, el \index{algebra@álgebra!tensorial} {\sf álgebra tensorial} contravariante que denotamos por~$\tau(\g)$. La dimensión de $\tau(\g)$ es infinita, aunque $\g$  sea de dimensión finita. $\g$  está incluido de modo canónico en el álgebra   $\tau(\g)$, como elementos de grado uno. Como lo necesitaremos posteriormente, recordemos la propiedad universal de  $\tau(\g)$:

\begin{quote}
	Si $ A$  es una $k$-álgebra asociativa cualquiera y  $\phi : \g \rightarrow A$ es lineal, entonces existe una única aplicación de  $k$-álgebras $\overline{\phi} : \tau(\g)  \rightarrow A$  que sobre $\g$  coincide con  $\phi$. La aplicación $\overline{\phi}$  satisface (pues no le queda otro remedio) que
$$
\overline{\phi } (x_1 \otimes x_2 \otimes \cdots \otimes x_n) = \phi(x_1) \phi(x_2) \cdots \phi(x_n)
$$			
\end{quote}
	Designemos ahora por  $U_{XY}$ al siguiente elemento del álgebra tensorial
$$
U_{XY}= X \otimes Y -Y\otimes X -[X,Y]
$$
Llamamos $I$  el ideal bilateral de  $\tau(\g)$   generado por todos los elementos de la forma  $U_{XY}$.


\begin{defi}

Denotaremos por  $\mathrm{U} (\g)$ al álgebra (asociativa) cociente de  $\tau(\g)$   módulo  $I$. Decimos que 
$\mathrm{U} (\g)$  es el {\sf álgebra envolvente} \index{algebra@álgebra!envolvente} universal del álgebra $\g$.

\end{defi}

La proyección canónica de  $\tau(\g)$   en $\mathrm{U} (\g)$  induce una aplicación 
$$
\gamma : \g \rightarrow \mathrm{U}(\g)
$$
En virtud de la definición del ideal  $I$,  tenemos que $\gamma$     es un morfismo de álgebras de Lie (toda álgebra asociativa se entiende como un álgebra de Lie con el conmutador). 
	


\begin{teo}


Sea  $A$  una $k$-álgebra asociativa. Sea  $\phi : \g \rightarrow A$  una aplicación lineal que sea también un morfismo de álgebras de Lie. En estas condiciones existe un único morfismo de $k$-álgebras $\phi_* : \mathrm{U}(\g) \rightarrow A$  que hace conmutativo el diagrama
$$
\xymatrix{
 \g \ar[d]_\gamma\ar[r]^\phi& A \\
\mathrm{U}(\g)\ar[ur]_{\phi_*}
}
$$

%%%%%%%%%%%%diagrama
\end{teo}

\dem

Por la propiedad universal de  $\tau(\g)$    existe un único morfismo   $k$-álgebras
$$
\overline{\phi} : \tau(\g) \rightarrow A
$$
 Si  $\phi$  cumple el enunciado tenemos que $\overline{\phi}(U_{XY}) = 0$ .   Como  $\overline{\phi}$  es morfismo de álgebras, también se anula sobre el ideal generado por los $U_{XY}$. Entonces 
  $\overline{\phi}(I) =0$.	Esta aplicación pasa al cociente módulo  $I$  dando una aplicación
$$
\phi_*: \mathrm{U}(\g) \rightarrow A
$$
que cumple el enunciado y es claramente única. \fin


	Este teorema y la teoría de categorías nos llevan a la siguiente definición.

\begin{defi}

Llamamos {\sf álgebra envolvente}\index{algebra@álgebra!envolvente} de  $\g$  a un par  $(\gamma  , A)$ donde  $A$ es un álgebra asociativa y  $\gamma : \g \rightarrow A$  es un morfismo de álgebras de Lie que satisfaga la propiedad universal que hemos demostrado para el par $( \gamma  , \mathrm{U}(\g))$.

\end{defi}

\begin{cor}

Dos álgebras envolventes de $\g$  siempre son isomorfas.

\end{cor}

\dem

 Denotémoslas por	$(\gamma_1, A_1)$  y $(\gamma_2 , A_2)$. Sea    $\gamma_2 : \g \rightarrow A_2$. Por la propiedad universal existe una única
$$
{\gamma_2}_* : A_1 \rightarrow A_2
$$
  que es morfismo de álgebras. Análogamente construimos ${\gamma_1}_*$. \fin 

\begin{cor}

Sea  $ \phi : \g \rightarrow \End (V)$  una representación lineal. Entonces existe una representación  $\phi_* : \mathrm{U}(\g) \rightarrow \End  (V)$ del álgebra envolvente. Lo mismo con representaciones de grupos de Lie.


\end{cor}

\begin{cor}

Sea  $\g$ y $\g '$ álgebras de Lie. Sea $ \phi : \g \rightarrow\g '$ un morfismo de álgebras de Lie. Existe entonces un único morfismo
$\phi_*: \mathrm{U}(\g) \rightarrow \mathrm{U}(\g ')$  que extiende a  $\phi$.

\end{cor}

\dem

Sea  $\gamma ' : \g ' \rightarrow \mathrm{U}(\g ')$  la aplicación canónica. Aplicando la propiedad universal a  $\gamma ' \circ \phi$  se concluye. \fin


\begin{cor}

Asociando a cada  k-álgebra de Lie $\g$   el álgebra  $\mathrm{U}(\g)$  y a cada morfismo  de álgebras de Lie $\phi$, el morfismo  $\phi_*$,  tenemos un functor covariante de la categoría de k-álgebras de Lie en la categoría de k-álgebras asociativas. Resultado análogo para grupos de Lie.

\end{cor}


Enunciemos ya el teorema de Poincaré-Birkoff-Witt. Supongamos, sólo para aliviar la notación, que   es de dimensión finita. Los elementos de $\g$ los consideramos incluidos en su álgebra envolvente.


\begin{teo}[Poincaré-Birkoff-Witt]

Sea $ \{X_1 , \ldots , X_n\}$  una base de $\g$. Los monomios  $X_1^{i_1}  \cdots   X_n^{r_n}$ forman una base  de  $\mathrm{U}(\g )$ (se sobreentiende $X_i^0=1$).

\end{teo}


La demostración puede hallarse en Varadarajan  pág. 168.

\begin{cor}


El álgebra envolvente de cualquier álgebra de Lie es siempre de dimensión infinita.

\end{cor}


\noindent	{\bf Nota.}

\smallskip

	Nótese que la base que da el teorema de Poincaré-Birkoff-Witt  es   \guillemotleft formal\-mente\guillemotright\  análoga a la base standard del álgebra simétrica sobre $\g$ .


\begin{cor}


La aplicación $\gamma$    es inyectiva. Así se explica el abuso de notación cometido al  \guillemotleft obviar\guillemotright\    $\gamma$.

\end{cor}

\begin{cor}



La multiplicación en  $\mathrm{U}(\g )$  está totalmente determinada por $\g$  y es posible conocerla sin más que tener las constantes de estructura de  $\g$ en una base.

\end{cor}


\dem





Es suficiente conocer el producto de elementos de la base. Sean $X_1^{r_1}  \cdots    X_n^{r_n}$ y $X_1^{s_1}  \cdots    X_n^{s_n}$  dos monomios.
	Entonces $X_1^{r_1}  \cdots    X_n^{r_n}\cdot X_1^{s_1}  \cdots    X_n^{s_n}$   es conocido como combinación lineal de otros monomios de la base sin más que ir  \guillemotleft trasladando  $X_1^{s_1}$  hacia la izquierda\guillemotright\   y lo mismo con los demás. Pero eso se puede hacer conociendo las constantes de estructura en la base, pues estas nos permiten permutar dos elementos cualquiera de la base aplicando la fórmula
$$
X_i X_j - X_j X_i = C_{ij}^k X_k
$$
Es aconsejable que el lector haga algún ejemplo con binomios y trinomios para comprender mejor esta demostración.	\fin

Recordaremos las nociones principales concernientes a los operadores diferenciales. 	Sea  $\V$  una variedad diferenciable de dimensión  $n$   y   $x$  un punto de  $\V$. Por  $A$  designamos el álgebra de funciones diferenciales de  $\V$  en  $\R$. $A_x$  designa el álgebra de gérmenes de dichas funciones en el punto  $x$  y  $A_x^*$ su dual como espacio vectorial. Designemos por  $m_x$  el ideal maximal del anillo local  $A_x$.  $m_x$ está formado por los germenes que se anulan en  $x$.

\begin{defi}

Llamaremos {\sf expresión diferencial} \index{expresión diferencial} de orden menor o igual que  $r$  a un elemento de  $A_x^*$ que se anule sobre $(m_x)^{r + 1}$. Estas expresiones forman un espacio vectorial de dimensión finita que denotamos por $T_x^r (\V)$. Como $T_x^r \subset T_x^n$ si $n>r$, designamos  $T_x^\infty(\V) = \cup T_x^r (\V)$, que es un espacio vectorial de dimensión finita.

\end{defi}

\noindent{\bf Notación.}

\smallskip

Sea  $(x_i )$  un sistema local de coordenadas en un entorno de  $x$. Sea $(\alpha) = (\alpha_1, \ldots , \alpha_n)$ un \guillemotleft multiíndice\guillemotright.  La función que manda al germen  $f$  hasta
$$
 \left (\frac{\partial ^{\mid \alpha \mid} f}{\partial _{x_1}^{\alpha_1} \ldots \partial_{x_n}^{\alpha_n}} \right ) _x
$$
se designa por $\partial_x^{\alpha} $. Tenemos que  
 $\partial_x^{\alpha}    : A_x \rightarrow\R$  es un elemento de  $T_x^r (\V)$ si    $ r \geq \mid \alpha \mid$.


\begin{teo}


Las expresiones diferenciales    $\partial_x^{\alpha} $    con  $\mid \alpha \mid \leq r $  forman una base de  $T_x^r (\V)$.

\end{teo}

\begin{defi}

Llamaremos {\sf operador diferencial} \index{operador!diferencial} $D$  sobre  $\V$ (o sobre un abierto) a una ley que asocia a cada punto  $x$  de  $\V$  un elemento de  $T_x^\infty (\V)$. Decimos que el operador  $D$  es diferenciable si para toda función  $f$  de  $A$, tenemos que  $Df(x) = D_x (f_x )$ es de nuevo un elemento de $A$.  Entonces un operador diferencial y diferenciable induce una aplicación lineal de  $A$  en si misma.

\end{defi}

La propiedad más importante de los operadores  $D$  es que son operadores locales.  Esto es que  $\mathrm{sop} (Df)   \subset    \mathrm{sop} (f)$. Esta propiedad es la que se usa para generalizar el concepto que nosotros hemos introducido. (Teorema de Peetre).

Debido a la localidad de  $D$,  podemos hablar del valor de  $D$  restringido a un abierto. Todo operador diferencial  se expresa en un entorno coordenado como
$$
D_y= \sum a_\alpha (y) . \partial_y ^{\mid \alpha \mid}
$$
El pedir que  $D$  sea diferenciable equivale en este lenguaje a pedir que $a_\alpha$ sean funciones diferenciables en el entorno considerado. Esta propiedad de expresarse localmente de esta manera se toma en muchos libros como definición de operador diferencial. Es claro que la composición de dos operadores diferenciales es de nuevo un operador diferencial. Los operadores diferenciales forman un álgebra asociativa. Los campos vectoriales estan incluidos en este álgebra y su producto cumple que $[X,Y] = XY - YX$.

El álgebra de operadores diferenciales sobre  $\V$  se designará, con una nomenclatura no estandard,  $\mathrm{ODiff} (\V)$.

\bigskip

Supongamos ahora que  la variedad  es un grupo de Lie  G. Cada traslación a la izquierda $\lambda_g$, induce un isomorfismo del álgebra  $A$  mediante la fórmula  $\lambda_g f = f  \circ \lambda_g$. 

\begin{defi}

Decimos que un operador diferencial  $D$  es invariante si $\lambda_g ^* \circ D = D \circ (\lambda_g)_*$ para todo  $g$  de  $G$.

\end{defi}

Los operadores diferenciales invariantes forman una subálgebra asociativa de $\mathrm{ODiff} (G)$. Es claro que un operador invariante  $D$  queda caracterizado por el valor de  $D$  en el neutro. Así el álgebra de operadores invariantes es isomorfa a  $T_ e^\infty (G)$. 



\begin{teo}

El álgebra de operadores diferenciales invariantes sobre  $G$\linebreak ($T_e^\infty (G)$)  es isomorfa al álgebra envolvente  universal de $\g$.

\end{teo}

\dem

Es consecuencia de la  propiedad universal y  del teorema de Poincaré-Birkoff-Witt. \fin


A cada  $X$  de $\g$   le asociamos el operador diferencial invariante  $X_e$, que es un vector tangente en  $ e$.  Esta aplicación cumple que es morfismo de álgebras de Lie y nos da una aplicación de álgebras de  $\mathrm{U}(\g)$  en  $T_e^\infty(G)$. Pero esto debe ser isomorfismo pues los operadores  $\partial_x ^\alpha$ forman una base  de  $T_e^\infty(G)$.




\newpage

\section{Concepto  de representación}

\begin{defi}

Una {\sf representación} de \index{representación} un grupo de Lie en un espacio vectorial  $E$  es un morfismo de grupos de Lie
$$
 \varphi : G \rightarrow\mathrm{Aut} (E)
$$
\end{defi}


Una representación se nota  $(G, E,\varphi )$. Sin embargo nosotros la denotaremos simplemente por $\varphi$, puesto que es el convenio habitual y no suele inducir a error. Denotaremos también por  $\varphi_g$   a   $\varphi(g)$. Como consecuencia del teorema de los morfismos continuos basta pedir la continuidad de $\varphi$  para asegurarnos su diferenciabilidad. 
Diremos que la representación  \index{representación!compleja} es {\sf real} o  {\sf compleja}\index{representación!real} dependiendo del cuerpo sobre el que consideremos la estructura de espacio vectorial de  $E$. La dimensión de  $E$  es la {\sf dimensión} \index{representación!dimensión} o {\sf grado}\index{representación!grado} de la representación.  En estas notas será siempre  finita.

Diremos que la representación es {\sf fiel} \index{representación!fiel} si $\varphi$   es inyectiva. Si  $G$  admite una repre\-sentación fiel, entonces  $G$  es difeomorfo a un grupo matricial.  Es evidente que todos los grupos matriciales, en virtud de su definición, poseen representaciones fieles. Si la representación   no es fiel, es debido a que posee núcleo. El núcleo es un subgrupo cerrado y, aplicando el teorema de factorización, obtenemos una representación fiel del grupo cociente  $G / \mathrm{Ker} (\varphi)$.


\begin{propo}

Una representación  $\varphi$  del grupo  $G$  en el espacio  $E$  queda completamente determinada conociendo el valor de  $\varphi$  sobre un conjunto que genere  $G$.

\end{propo}



\begin{cor}

Sea  $G$  conexo y  $\varphi$  un morfismo local de  $G$  en  $\mathrm{Aut} (E)$. 
Entonces existe a lo más una representación de  $G$  en  $E$  que extiende al morfismo local.

\end{cor}


\begin{cor}


Sea  $G$  un grupo no conexo. Conociendo el valor de   $\varphi$  sobre la componente conexa de  la unidad  y conociendo como actúa  $\varphi$   sobre un único punto de cada una de las otras componente conexas, conocemos   $\varphi$  globalmente.

\end{cor}

Nosotros estudiaremos  las representaciones de los grupos cone\-xos, pues la de los no conexos se reduce a estas y a un estudio de simetrías discretas. De ahora en adelante, y si  no se dice nada en contra, supondremos que los grupos son conexos.



\begin{propo}

Cada representación de  $G$ induce una representación de su recubridor universal, siendo esta correspondencia inyectiva.

\end{propo}

\dem

 Sea  $\varphi : G \rightarrow\mathrm{Aut} (E) $ un morfismo de grupos. Entonces
 $$
 \boldsymbol{\varphi}  = \varphi \circ \pi:  \Gr \rightarrow\mathrm{Aut} (E)
 $$  es una representación del recubridor universal. La inyectividad es consecuencia del isomorfismo local existente entre  $\Gr$  y  $G$  y de que un entorno abierto genera  $G$  por ser conexo. \fin


Podemos decir que las representaciones de  $G$  son un subconjunto de las representaciones de su recubridor universal. Son aquellas representaciones que manda el núcleo de  $\pi$  al elemento neutro (aplíquese el teorema de factorización).  Basta entonces estudiar las representaciones de los grupos simplemente conexos, y estas se reducen a las representaciones de su álgebra de Lie.

\begin{propo}


Sea  $G$  simplemente conexo. Existe una biyección entre las representaciones de  $G$  y las de su álgebra. A cada representación de  $G$  le asocia\-mos su diferencial.

\end{propo}




\begin{defi}

Sean   $\varphi: G \rightarrow\mathrm{Aut} (E)$ y $\phi   : G \rightarrow\mathrm{Aut} (\overline{E})$ dos representaciones del mismo grupo. Un {\sf morfismo} \index{representación!morfismo} de la representación  $\varphi$   en   $\phi$ es una aplicación lineal $h:E \rightarrow\overline{E}$ que hace el siguiente diagrama conmutativo
$$
\xymatrix{
E \ar[r]^h\ar[d]_{\varphi_g}& \overline{E}\ar[d]^{\phi_g}\\
E \ar[r]^h& \overline{E}
}
$$
o expresado en forma más compacta
$$
 h \circ \varphi_g =\phi_g\circ h
$$

\end{defi}


Por analogías algebraicas los morfismos de $\varphi$ en
$\phi$ también se llaman morfismos de $G$-módulos.
Emplearemos la notación $ h : \varphi \rightarrow\phi$ para
denotar los morfismos de representaciones. La composición de morfismos es de nuevo un morfismo y la identidad es un morfismo. Queda pues constituida la categoría de representaciones lineales del grupo  $G$ (distíngase entre la categoría de las representaciones reales y la categoría de las representaciones complejas).     Los isomorfismos de esta categoría son los morfismos biunívocos como puede probar el lector.

\begin{defi}

Dos representaciones \index{representaciones!equivalentes} son {\sf equivalentes} si son isomorfas.

\end{defi}




Generalmente, cuando se desean realizar cálculos en teoría de representaciones, se toma una base en el espacio vectorial sobre el que está representado el grupo. A cada automorfismo de la representación le corresponde una matriz de determinante no nulo. Se dice entonces que hemos dado una repre\-sentación {\sf matricial}. Una representación matricial \index{representación!matricial} de grado  $n$  consiste en   $n^2$  funciones diferen\-ciables de  $G$  en el cuerpo y que cumplen ciertas propiedades para asegurar que definan un morfismo de grupos.

\bigskip

Al introducir representaciones matriciales, a la par que se pierde intui\-ción geométrica, surge el problema de los cambios de bases. Es necesario considerar que la misma representacion escrita en dos bases distintas es una única representación. Para ello nos vemos obligados a introducir una cierta relación de equivalencia. Veamos como la deducimos.
Sea  $M_g$  la matriz de  $g$  en una base. Tomamos otra base y sea  $B$  la matriz de cambio de base. La matriz de  $g$  en la nueva base será  $M'_g = B^{-1} . M_g . B$.
Como consecuencia de esto nos vemos obligados a imponer la siguiente

\begin{defi}

Dos representaciones matriciales  $M$ y $M'$  son {\sf equivalentes}\index{representaciones!equivalentes} si existe una matriz inversible $B$ que cumpla
$$
M'_g = B^{-1} . M_g . B \quad \text{ para todo }\quad g
$$


\end{defi}


En general la teoría de representaciones trata de encontrar bases en las que las representaciones matriciales sean lo más simple posibles. Para esto es conveniente introducir nuevos conceptos en teoría de representaciones.  Aunque nosotros daremos interpretaciones matriciales a muchos resultados, siempre que podamos intentaremos desarrollar la teoría en forma intrínseca.

\subsection*{Problemas}



\begin{pro}

Dos representaciones isomorfas tienen la misma dimensión. Pruébese que el recíproco en general no es cierto.

\end{pro}


\begin{pro}

Si dos representaciones $\varphi$ y $\phi$  (digamos abstractas) son isomorfas, entonces existen bases en los espacios vectoriales tales que las repre\-sentaciones matriciales de   $\varphi$ y $\phi$  son iguales. ¿Es cierto el recíproco? Relacionar los conceptos de isomorfismo y equivalencia.

\end{pro}

\begin{pro}

Sea $\phi$ una acción por la izquierda sobre la variedad $\V$.  Si $\V$ tiene un punto fijo ($gx=x$ para todo $g$) entonces definimos la aplicación
$$
\begin{array}{ccc}
G & \rightarrow & \mathrm{GL}(T_x\V)\\
g & \rightarrow &(\phi_g)'
\end{array}
$$
Porbar que es una representación.

\end{pro}


\begin{pro}


Probar que la definición de equivalencia que hemos introducido para representaciones matriciales es en efecto una relación de equivalencia.

\end{pro}


\begin{pro}

Utilizando la representación adjunta probar que si  $G$  carece de centro, entonces  $G$  es difeomorfo a un grupo matricial. ¿Es cierto el recíproco?

\end{pro}

\begin{pro}


Sea  $\varphi : \mathrm{SO} (3) \rightarrow\mathrm{Aut} (E)$  una representación. Sea  $P$  la paridad. ¿Qué condición debe cumplir  $\varphi (P)$  para que se induzca de este modo una representación de $\mathrm{O} (3)$? Generalizar este resultado a $ \mathrm{SO} (2n + 1)$. Para los pares intentar hacer lo mismo con una simetría especular. Intentar hacer un estudio análogo con el grupo de Lorentz.

\end{pro}

\begin{pro}
Dotar de estructura de espacio vectorial a  $\mathrm{Mor} ( \varphi, \phi)$ y de álgebra a  $\mathrm{End} ( \varphi)$. Pruébese que la categoría de representaciones es abeliana.
¿Podrías hallar las dimensiones de estos  k-espacios?

\end{pro}



\newpage

\section{Subespacios invariantes}

Desarrollaremos primero la teoría desde un punto de vista intrínseco y al final daremos la interpretación matricial.

\begin{defi}

Sea  $\varphi  : G \rightarrow\mathrm{Aut} (E)$  una representación. Un subespacio  \mbox{$F \subset E$}  es \index{subespacio!invariante} {\sf invariante} si es estable para todos los automorfismos  $\varphi_g$. Esto quiere decir que  $\varphi_ g (F)\subset F$.

\end{defi}

\noindent{\bf Ejemplos.}

\begin{itemize}

\item Decimos que un vector $v$ es invariante por la representación si $\phi_g(v)=v$ para todo $g$.  El conjunto de vectores invariantes es un subespacio invariante.

\item El subespacio vectorial generado por los vectores de la forma $\phi_g(v)$, con $v$ un vector fijo, es un subespacio invariante.

\end{itemize}

Naturalmente cada  $\varphi_g$  restringido a  $F$ es inyectivo y por razonamientos de dimensión debe ser epiyectivo. Por ello, si  $F$ es invariante, podemos construir una representación en  $F$ mediante la fórmula
$$
\begin{array}{rcc}
\phi: G & \rightarrow & \mathrm{Aut}(F)\\
g &\rightarrow & \varphi_{\mid F}
\end{array}$$
Esta representación que acabamos de construir se denomina {\sf subrepresentación} \index{subrepresentación} de $\varphi$
  y se nota $\varphi_{  \mid_{F}}$.
Consideremos ahora la inyección canónica $i$ de  $F$ en  $E$. En virtud de la definición,  $i$ es un morfismo de  $\varphi_{  \mid_{F}}$ en $\varphi$. Además la definición de  $\varphi_{  \mid_{F}}$ es la única definición que satisface tal propiedad.

\bigskip

Sabemos por álgebra lineal que si un automorfismo  $\varphi : E \rightarrow E$  deja invariante un subespacio $F$, induce entonces un isomorfismo $ \overline{\varphi }$ de  $E / F$  que hace conmutativo el diagrama.
$$
\xymatrix{
E \ar[r]^\varphi \ar[d]_\pi & E\ar[d]^\pi \\
E/F \ar[r]^{\overline{\varphi}} & E/F
}
$$
Si  $F$ es invariante podemos asociar a $\varphi$  una representación en el cociente $E / F$.
$$
\begin{array}{rcc}
\overline{\varphi}: G&  \rightarrow&\mathrm{Aut} ( E / F)\\
g & \rightarrow &\overline{\varphi}(g)= \overline{\varphi_g}
\end{array}
$$
En efecto $\overline{\varphi}$  es morfismo de grupos y la diferen\-ciabilidad será clara una vez que estudiemos la interpretación matricial.       Decimos que $\overline{\varphi}$  es una representación \index{representación!cociente} {\sf cociente}. Más exactamente, es el cociente de  $\varphi$ módulo  $\varphi_{ \mid_{F}}$.     Se prueba también que la proyección canónica es morfismo de representaciones.

\bigskip

En toda representación existen dos subespacios invariantes, que son el cero y el total. Estos se llaman {\sf triviales} \index{subespacio!trivial} y carecen de interés.

\begin{defi}

Una representación es \index{representación!reducible} {\sf reducible} si posee un subespacio invariante no trivial. Es lo mismo que decir que no posee subrepresentaciones no triviales.

\end{defi}


Una representación es irreducible si sus únicos subespacios invariantes son el cero y el total. Entre las representaciones reducibles hay unas especiales que se llaman {\sf semisimples} \index{representación!semisimple} y vienen caracterizadas porque cada subespacio invariante tiene un complementario también invariante. Estudiaremos más a fondo estas representaciones cuando hayamos dado el concepto de suma directa de representaciones.

\bigskip

Veamos ya la interpretación matricial. Supongamos $F \subset E$ invariante. Tomamos una base $\{e_1, \dots,e_p\}$ de $F$ y la completamos hasta obtener una base \mbox{de $E$}. Sean $e_{p+1},\dots, e_n$ los vectores que añadimos. Entonces  $\{\pi(e_{p+1}), \dots ,\pi(e_n)\}$ es una base de  $E / F$. La representación matricial en esta base es:
\begin{center}
$
  \begin{pmatrix}
  M_1(g)       & M_3(g)    \\
   0       &  M_2(g)    \\
\end{pmatrix}
$
\end{center}
La submatriz  $M_1$ es la representación matricial de $\varphi_{ \mid_{F}}$  y
$M_2 $  es la representación matricial de $\overline{\varphi}$, expresadas en las bases  que anteriormente hemos citado. La submatriz $M_3$ que aparece en la fórmula es desconocida. 
Si ahora $F'$ es un subespacio invariante, complementario de $F$, podemos tomar los vectores $\{e_{p+1},\dots, e_n\}$ como una base de $F'$. En este caso nos queda
\begin{center}
$
  \begin{pmatrix}
  M_1(g)      & 0     \\
   0       & M_2(g)   \\
\end{pmatrix}
$
\end{center}

\subsection*{Problemas}


\begin{pro}
Sean dos representaciones equivalentes. Si  una  es reducible también la otra lo es  . Lo mismo con otras nociones. 
Supongamos que  $\varphi$ es una representación matricial y $\varphi$  posee un subespacio invariante no nulo, entonces existe una matriz  B  invertible que hace que $B^{-1} M_g B$ sea de la forma

\begin{center}
$
\begin{pmatrix}
  M_1(g)       & M_3(g)    \\
   0       &  M_2(g)
\end{pmatrix}
$
\end{center}
donde  $M_1$ y $M_2$ son representaciones matriciales.

\end{pro}


\begin{pro}

Sea $\varphi : G \rightarrow\mathrm{Aut} (E)$  una representación real \index{representación!ortogonal} ortogonal (quiere decir que en  $E$  existe un producto escalar euclideo, tal que $\varphi_g$ conserva ese producto escalar). Entonces si $F \subset E$ es invariante, existe un subespacio complementario invariante, que es el ortogonal de $F$.  Sea  $\varphi : G \rightarrow\mathrm{Aut} (E)$  una representación compleja \index{representación!unitaria} unitaria. Pruébese lo mismo.

\end{pro}

\begin{pro}

Probar, utilizando coordenadas, que tanto $\varphi_{ \mid_{F}}$ como $\overline{\varphi}$  son morfismos diferenciables.

\end{pro}

\begin{pro}

Probar que las subrepresentaciones y representaciones cocientes son subobjetos y objetos cociente en la categoría de representaciones.
Enúnciese un teorema de factorización.

\end{pro}

\newpage

\section{Operaciones con representaciones}

En todo este apartado $\varphi$  y $\phi$  denotarán representaciones de un grupo de Lie $G$ en espacios  $E$  y $\overline{E}$   respectivamente.

\begin{defi}

Llamamos representación \index{representación!dual} {\sf dual} de $\varphi$   a la representación $\varphi^*$  de  $G$  en  $E^*$ (espacio dual de $E$)
$$
\begin{array}{rcc}
\varphi^*: G&  \rightarrow & \mathrm{Aut} ( E^*)\\
g & \rightarrow &{\varphi^*}(g)= ({\varphi_g}^{-1})^*
\end{array}
$$
\end{defi}
Se puede comprobar que es morfismo de grupos y la diferenciabilidad es consecuencia de la estructura de grupo de Lie en  $\mathrm{Aut} ( E^* )$ que nos garantiza que \guillemotleft tomar inverso\guillemotright\ es diferenciable. Una representación se llama {\sf autodual} \index{representación!autodual}  si es equivalente a su dual.

\bigskip

Supongamos ahora que en  $E$  hay una métrica no degenerada  $T_2$. A cada automorfismo  $T$  de  $E$  le podemos hacer corresponder un adjunto  $T^+$ mediante la igualdad.
$$
T_2 (T (e), e') = T_2 (e, T^+ (e'))\: \: \text{ para todo }  e, e' \text{ de } E
$$
Definimos la representación {\sf adjunta} \index{representación!adjunta} de $\varphi$  por la fórmula
$$
\begin{array}{rcc}
\varphi^*: G&  \rightarrow & \mathrm{Aut} ( E)\\
g & \rightarrow &{\varphi^+}(g)= ({\varphi_g}^{-1})^+
\end{array}
$$


\bigskip

Sobre  $E \oplus \overline{E}$    y  $ E \otimes \overline{E}$  se construyen  las siguientes representaciones:
$$
\begin{array}{rcc}
\varphi\oplus \phi: G&  \rightarrow & \mathrm{Aut} ( E\oplus \overline{E})\\
g & \rightarrow &(\varphi \oplus \phi)_g = \varphi_g \oplus \phi_g
\end{array}
$$
donde $ \varphi_g \oplus \phi_g$   actúa del modo
$$
 \varphi_g \oplus \phi_g (e,e')= (\varphi_g(e), \phi_g(e'))
$$
Propiedades elementales de álgebra lineal prueban que  $(\varphi \oplus \phi)_g$ es biuní\-voco y que efectivamente  $ \varphi \oplus \phi$  es morfismo de grupos.
        Análogamente definimos
$$
\begin{array}{rcc}
\varphi\otimes \phi: G&  \rightarrow & \mathrm{Aut} ( E\otimes \overline{E})\\
g & \rightarrow &(\varphi \otimes \phi)_g = \varphi_g \otimes \phi_g
\end{array}
$$
donde $ \varphi_g \otimes \phi_g$   actúa del modo
$$
 \varphi_g \otimes \phi_g (e \otimes e')= \varphi_g(e) \otimes \phi_g(e')
$$
que está bien definido por  la propiedad universal del producto tensorial.


\begin{defi}

Llamamos {\sf suma directa} \index{representaciones!suma directa}  de  $\varphi$  y $\phi$  a la representación  $\varphi \oplus \phi$  que hemos construido. Llamamos {\sf producto tensorial} \index{representaciones!producto tensorial} a la representación $\varphi \otimes \phi$.

\end{defi}

Las interpretaciones matriciales de estas estructuras deben ser claras. El concepto de suma directa va a permitir generalizar el concepto de representación reducible semisimple.

\begin{defi}


Una representación   es {\sf completamente reducible} \index{representación!completamente reducible} si es isomorfa a una suma directa de irreducibles.

\end{defi}

\subsection*{Problemas}

\begin{pro}

Probar que toda representación unitaria u ortogonal es completamente reducible.

\end{pro}

\begin{pro}


Sea  $\varphi^+$ la adjunta asociada a una métrica sin radical. Probar que  $\varphi^+$  y
$\varphi^*$ son isomorfas.

\end{pro}


\begin{pro}

Probar, utilizando representaciones matriciales, que $\varphi \oplus \phi$ y $\varphi \otimes \phi$ son aplicaciones diferenciables.

\end{pro}

\begin{pro}

Estructura algebraica en el conjunto de clases de equivalencia de representaciones.Probar:
\begin{itemize}


\item Si $\varphi \sim \varphi'$ y $\phi \sim \phi'$ entonces tenemos que  $\varphi \oplus \phi \sim  \varphi' \oplus \phi'$ y $\varphi \otimes \phi \sim
 \varphi' \otimes \phi'$

\item Asociatividad y conmutatividad de la suma y del producto tensorial de clases.

\item Distributiva de la suma respecto al producto.

\item ¿Existen elementos neutros para esas operaciones?

\item ¿Existen elementos simétricos?

\item Construir el anillo de \index{Anillo de Grotendieck} Grotendieck asociado.
\end{itemize}
\end{pro}

\begin{pro}

Las inyecciones canónicas de  $E$  y $\overline{E}$   en  $E \oplus \overline{E}$ son morfismos de representaciones. ¿Goza de alguna propiedad universal la suma directa de representaciones?

\end{pro}


\begin{pro}


Sea  $G$  un grupo compacto. Consideremos el $\C$-espacio de las funciones continuas sobre  $G$ y valorada en los números complejos. $(C^0 (G))$.
Sea  $E$  el subespacio vectorial de $(C^0 (G))$  generado por todas las funciones
$a_{ij} (x)$ donde $a_{ij} (x)$  forma parte de una representación irreducible de  $G$.
Probar que dos representaciones matriciales equivalentes generan el mismo subespacio.
Probar que  $E$  es un álgebra.
Suponiendo que  $G$  admita una representación fiel y aplicando el teorema de Stone-Weisstras    concluir que  $E$  es denso en  $(C^0 (G))$.

\end{pro}





\newpage

\section{Lema de Schur}



El lema de Schur es uno de los útiles más potentes para el estudio de representaciones y tiene la característica de ser extraordinariamente sencillo. \mbox{A partir} de él se puede obtener de modo inmediato la estructura del conjunto de morfismo entre dos representaciones completamente reducibles, aunque nosotros no lo haremos aquí. 

\begin{lema}[Schur] \index{lema!de Schur}

Sean   $\varphi: G \rightarrow\mathrm{Aut} (E)$ y $\phi: G \rightarrow\mathrm{Aut} (\overline{E})$   dos representaciones irreducibles. Un morfismo  $h :\varphi \rightarrow\phi$  es nulo o es un isomorfismo.

\end{lema}

\dem


Compruebe el lector que tanto $\mathrm{Ker}( h)$ como $\mathrm{Img}(h)$  son  subespacios invariantes.      Como  $\varphi$ es irreducible puden darse dos casos:
\begin{itemize}
\item  $\mathrm{Ker}( h) = 0$ por lo tanto $ h$  es inyectivo
\item $\mathrm{Ker} (h) = E$ entonces $h$  es nulo.
\end{itemize}
Como  $\phi$  es irreducible:
\begin{itemize}
 \item $\mathrm{Img}(h)=0$ $ \Rightarrow $ $h$ es nulo
\item $\mathrm{Img}(h)=E$ $ \Rightarrow $ $h$ es epiyectivo.
\end{itemize}

Entonces $h$ es nulo o es un isomorfismo. \fin

Este lema se emplea a veces en forma matricial: sea  $M : G \rightarrow \mathrm{GL} (\R^n)$  una representación matricial irreducible y  $B$  una matriz que conmute con todas las matrices  $M_g$. Entonces  $B$  es nula o es invertible.

\begin{cor}

El conjunto de todos los endomorfismos de una representación irreducible es un anillo con división (todo elemento no nulo es inversible).

\end{cor}

\dem

La estructura de anillo la tenemos para cualquier representación y que todo elemento no nulo es invertible es consecuencia del lema de Schur. En  principio no podemos afirmar que este anillo sea conmutativo. \fin

Otros corolarios interesantes de este lema se refieren sólo al caso complejo. Mucho cuidado pues con utilizarlo para representaciones reales.

\begin{cor}

Sea $\varphi$  irreducible y compleja. Todo automorfismo de $\varphi$  es un múltiplo no nulo de la identidad.

\end{cor}

\dem


Sea  $h : \varphi \rightarrow\varphi$  un automorfismo. Por ser  $\C$ algebraicamente cerrado,  $h$  posee un valor propio no nulo  (si fuera nulo  $h$  tendría núcleo). $\mathrm{Ker} (h - \lambda\mathrm{Id})$  es invariante (compruébese)  y no nulo. Entonces es el total, luego  $h = \lambda \mathrm{Id}$. \fin

En el caso complejo el anillo con división antes descrito es conmutativo y coincide con $ \C$.


\begin{cor}


Si  $G$  es abeliano, $\varphi$ irreducible y compleja, entonces  $E$  es de dimensión 1.

\end{cor}

\dem

Como  $G$  es abeliano  $\varphi_g$ conmuta con todos los elementos de la representación y por tanto es un múltiplo de la identidad para todo  $g$.     Como   es irreducible, es necesario que  $E$  sea de dimensión 1. \fin


\subsection*{Problemas}

\begin{pro}

Probar que todas estas cosas fallan en el caso de representaciones reales.

\end{pro}


\begin{pro}



Sea  $T^1$  el toro unidimensional. Probar que todas las representaciones irreducibles complejas y unitarias de  $T^1$  son de la forma  $\vartheta \rightarrow e^{ik\vartheta}$con $k \in \Z$.
\begin{itemize}
 \item Calcular $e^{ik_1\vartheta }  \otimes e^{ik_2\vartheta}$
\item Calcular la representación dual de  $e^{ik\vartheta }$
\item Determinar la estructura del anillo de Grotendieck.
\item Probar que  $e^{ik\vartheta }$ con $k \in \Z$  es una base Hilbertiana de
$L_2 ( T^1, \C)$. Relacionarlo con la teoría de series de Fourier.

\end{itemize}



\end{pro}

\begin{pro}

Analizar el caso  $T^n$, $\R$ y $\R^n$. Intentar relacionar las representaciones de  $\R$  con
 la teoría de la transformada de Fourier.
\end{pro}


\newpage



\section{Reducibilidad completa en compactos}


Sea $E$ un espacio euclídeo. Una representación $\varphi$ es {\sf ortogonal} \index{representación!ortogonal} si $\varphi_g$ conserva el producto escalar para todo $ g \in G$. Si $E$ es un espacio hermítico y $\varphi_g$ conserva el producto escalar, la representación se llama \index{representación!unitaria} {\sf unitaria}.

Los siguientes resultados se prueban en el caso unitario.  Idénticas demostraciones son válidas en el caso ortogonal.

\begin{lema}

Si $\varphi$   es unitaria y  $F \subset E$  es invariante entonces $F^\perp$   es invariante.

\end{lema}

\dem

Por definición de subespacio ortogonal
$$
 e \in F^\perp   \Leftrightarrow  \escalar{e}{e'} =0 \text{ para todo } e' \in F
 $$
 
 Tomemos un elemento $e \in F^\perp$ y preguntémonos si $\varphi_g(e)$ sigue perteneciendo a $F^\perp$. Como  $F$ es invariante, $e'= \varphi_g (e'')$ con $e'' \in F$, siendo esto válido para todo elemento de $F$. Entonces
$$
\escalar{\varphi_g(e)}{ e'}= \escalar{\varphi_g(e)}{ \varphi_g (e'')}=\escalar{e}{e'}=0
$$
Por tanto $F$ es invariante. \fin


\begin{teo}

Si $\varphi$   es unitaria entonces  es completamente reducible.

\end{teo}


\dem

Si $\varphi$   es irreducible acabamos. Si no lo es, posee un subespacio propio  $F$ invariante. Por el lema anterior
$$
 \varphi \sim  \varphi_{ \mid_{F}} \oplus \varphi_{ \mid_{F^\perp}}
$$
Tanto   $ \varphi_{ \mid_{F}}$ como  $\varphi_{ \mid_{F^\perp}}$  son unitarias y de dimensión menor $\varphi$. Luego por indución demostramos el teorema. \fin

Veamos ahora que toda representación de un grupo compacto puede considerarse unitaria o ortogonal. Denotemos por  $\tau$  el elemento de volumen de la medida de Haar. Sea $\escalar{-}{-}$  un producto hermítico o euclídeo arbitrario. Definimos un nuevo producto escalar $(-,-)$.
$$
(x,y) = \int_G<\varphi_g\left(x\right),\varphi_g\left(y\right)>\: \tau
$$
Este nuevo producto está bien definido, puesto que las integrales convergen, y  es euclídeo o hermítico, dependiendo del producto del que partamos. Resulta que $\varphi$ es unitario
$$
\left(\varphi_{g'}(x),\varphi_{g'}(y) \right) = \int_G<\varphi_{g'}\varphi_g(x),
\varphi_{g'}\varphi_g(y)> \: \tau = \int_G<\varphi_{gg'}(x),\varphi_{gg'}(y)> \: \tau
$$
Pero por ser $\tau$   invariante podemos poner  $gg' = \overline{g}$  sin que cambie el elemento de volumen. Por eso la última expresión es igual a $\left(x,y\right)$, lo que prueba que todos los  $\varphi_g$  son unitarios.

\begin{cor}


Toda representación de un grupo compacto es completamente reducible. 

\end{cor}





\newpage

\section{Caracteres}

El carácter es un invariante  del conjunto de clases de equivalencia de representaciones  que nos va a permitir decidir cuando dos representaciones son equivalentes. Para asegurarnos la convergencia de ciertas integrales, supondremos  en esta sección que el grupo es compacto.

\begin{defi}


Sea  $\varphi : G \rightarrow\mathrm{Aut} (E)$  una representación. El {\sf carácter} \index{carácter} de $ \varphi$   es la
función $ \aleph_\varphi :G \rightarrow k$  definida como
$$
\aleph_\varphi   (g) = \mathrm{Tr} ( \varphi_g )
$$

\end{defi}

La función  $ \aleph_\varphi$   es diferenciable, pues es la suma de los coeficientes de la diagonal en una cualquiera de sus representaciones matriciales. Debido a la conocida propiedad de la traza,   $\mathrm{Tr} (AB) = \mathrm{Tr} (BA)$,  podemos observar que si $\varphi \sim \phi$  entonces  $ \aleph_\varphi =  \aleph_\phi$. Luego el carácter está definido sobre el conjunto de clases de equivalencia. Además de todo esto la función carácter tiene la propiedad
$$
\aleph_\varphi(g)=   \aleph_\varphi(h^{-1} g\:  h)
$$
Las funciones que verifican esa propiedad se denominan funciones \index{función!central}{\sf centrales}.


\begin{propo}  

Se cumplen las siguientes reglas operativas:

\begin{itemize}

\item $ \aleph_{\varphi \oplus \phi} = \aleph_\varphi + \aleph_\phi$

\item $ \aleph_{\varphi \otimes \phi} = \aleph_\varphi \cdot \aleph_\phi$

\end{itemize}
\end{propo}

\dem


 Si escribimos en forma matricial la representación 
 $$
\mathrm{Tr}({\varphi \oplus \phi})_g = \mathrm{Tr}(\varphi_g) +\mathrm{Tr}(\phi_g)
$$
lo que prueba la primera igualdad.

\smallskip


Sea $\{e_1, \dots, e_n\}$  una base de $E$,  $\{e'_1, \dots, e'_m\}$  una base de $\overline{E}$. Entonces $e_i \otimes e'_j$   es una base de  $E \otimes \overline{E}$. Calcularemos la matriz de $(\varphi \otimes \phi)_g$ en esta base  suponiendo
$$
\varphi_g (e_i ) = a_{il} e_l \qquad 
\phi_g(e'_j) = b_{jm} e'_m
$$
Tenemos entonces (empleando el convenio de Einstein)
$$
(\varphi \otimes \phi)_g\: (e_i \otimes e'_j) = \varphi_g(ei) \otimes \phi_g (e'_j) =
 a_{il}.b_{jm} e_l \otimes e'_m
$$
Haciendo $l=i$ y $m=j$ observamos que la diagonal es $ a_{ii}.b_{jj}$.  Su suma coincide con el producto de las trazas y esto prueba que
$ \aleph_{\varphi \otimes \phi} = \aleph_\varphi \cdot\aleph_\phi$. \fin


Supongamos ya que  $G$  es compacto. Sea $\tau$    el elemento de volumen que  induce la medida de Haar. En el conjunto  $C^\infty (G,k)$  consideramos la siguiente estructura de espacio prehilbertiano.
$$
\escalar{f}{g} = \int_G(f\cdot\overline{g}) \: \tau
$$
(En el caso de funciones reales sobra la conjugación compleja y el producto es bilineal y no hermitico) 

 La siguiente proposición se deduce del lema de Schur y no la probaremos debido a su tediosidad, no a su dificultad, que no es mucha.

\begin{propo}



Sean  $\varphi$  y $\phi$   representaciones irreducibles de  $G$. Entonces
$\escalar{\aleph_\varphi}{\aleph_\phi} =0$    si $\varphi$   no es equivalente a $\phi$  y $\escalar{\aleph_\varphi}{\aleph_\phi} =1$ si son equivalentes.

\end{propo}


Dada una representación $\varphi$ la escribimos como suma directa de irreducibles, $\varphi  = n_1 \varphi_1 \oplus \dots \oplus n_i \varphi_i$, donde los $\varphi_j$ son irreducibles no isomorfas entre si. Aplicando las propiedades del carácter tenemos
$$
\aleph_\varphi = n_1\aleph_{\varphi_1}+ \dots + n_i\aleph_{\varphi_i}
$$
Hallamos ahora el producto escalar de $\aleph_{\varphi}$  con $\aleph_{\varphi_j}$
$$
<\aleph_{\varphi},\aleph_{\varphi_j}>= n_j
$$

Entonces el producto escalar nos dice cuantas veces \guillemotleft entra\guillemotright\  la representación irreducible  $\varphi_j$ en la descomposición de $\varphi$   como suma directa. Si suponemos conocidos todos los caracteres de todas las representaciones irre\-ducibles, simplemente haciendo productos escalares  podemos obtener la descomposición de  $\varphi$  como suma directa. Entonces nos basta el carácter de una representación completamente reducible para saber que representación es. Tenemos entonces el siguiente corolario.

\begin{cor}

Dos representaciones  $\varphi$  y $\phi$  sobre un grupo compacto son equivalentes sí y sólo si  tienen el mismo carácter.

\end{cor}

En el caso de los grupos finitos, hay un número finito de representaciones irreducibles y se pueden conocer todas mediante la llamada representación regular. Así tenemos que la teoría para grupos finitos es muy simple, pues basta hallar el carácter de la representación y un número finito de productos escalares. Además en el caso finito las integrales se transforman en sumas finitas.



\subsection*{Problemas}

\begin{pro}

Extender la noción de carácter al anillo de Grotendieck y generalizar resultados.

\end{pro}









%%%%%%%%Faltan los capitulos de los operadores diferenciales y de el %% grupo recubridor

\newpage


\begin{defi}

Sea $G$ un grupo de Lie.  Llamamos {\sf subgrupo de Lie inmerso}\index{subgrupo!inmerso} a un morfismo de grupos de Lie $\varphi: H\rightarrow G$ que sea inyectivo.

\end{defi}

La imagen de $H$ por el morfismo $\phi$ es un subgrupo (en el sentido algebraico) pero puede ocurrir que no sea una subvariedad.  Si $\varphi(H)$ es una subvariedad de $G$, entonces es un subgrupo de Lie en el sentido clásico; $H$ y su imagen serían isomorfos como grupos de Lie. Este concepto tiene interés solamente en el caso en que $\varphi(H)$ no es subvariedad. Veamos un ejemplo típico.

Dado un número irracional $\alpha$ construimos
$$
\begin{array}{cccc}
\varphi:& \R & \rightarrow & S^1 \times S^1\\
        & t& \rightarrow& (e^{2\pi it}, e^{2\pi i \alpha t})
 \end{array}
 $$       
 Esta aplicación es un morfismo de grupos de Lie y es inyectiva puesto que $\alpha$ es irracional.  La imagen de este morfismo es un conjunto denso de $T^2$ y por lo tanto no puede ser subvariedad, puesto que un conjunto definido localmente por ceros de funciones no puede ser denso.


\bigskip

Sea $G$ un grupo de Lie y $\g$ su álgebra.  Dada una subalgebra $\h \subset \g$ construimos una distribución de campos sobre $G$
$$
\Delta_x= \h_x
$$
En cada punto, el subespacio asignado es el generado por los vectores de la subálgebra.  Un ligero cálculo demuestra que esta distribución es involutiva, para lo que se utiliza de manera fundamental que $\h$ es una subálgebra.

Aplicando el teorema de Frobenuis, sabemos que por cada punto pasa una subvariedad (inmersa) maximal y conexa.  Denotemos por $H_e$ a la componente conexa que contiene al elemento neutro.

\begin{teo}

$H_e$ es un subgrupo de Lie inmerso.

\end{teo}

\newpage

\section{Cuaterniones}

Salvo isomorfismos, $\C$ es el único cuerpo conmutativo de dimensión finita que contiene a $\R$.  Como $\C$ es algebraicamente cerrado, no puede existir ningún cuerpo conmutativo de dimensión finita que lo contenga. Sin embargo, existe un único cuerpo (no conmutativo) de dimensión finita que contiene a $\R$ y a $\C$.  Dicho cuerpo fue descubierto por Hamilton  a mediados del siglo {\sc xix}.  Daremos varios modelos de este cuerpo y utilizaremos el modelo que más nos convenga en cada ocasión para realizar las demostraciones o dar la definiciones.

\bigskip

Consideramos el subconjunto
$$
\cua= 
\left\{
 \begin{pmatrix} 
a & b \\
-\overline{b}& \overline{a}
\end{pmatrix}
\in M_2(\C)\text{  con  } a, b \in \C\right\} 
$$
La suma y el producto de elementos de $\cua$ es nuevamente un elemento de $\cua$.  El determinante de un elemento
$$
\det
 \begin{pmatrix} 
a & b \\
-\overline{b}& \overline{a}
\end{pmatrix}
= a\overline{a}+b\overline{b}= \abs{a}^2+\abs{b}^2
$$
es no nulo si la matriz es no nula.  El inverso es
$$
 \begin{pmatrix} 
a & b \\
-\overline{b}& \overline{a}
\end{pmatrix}^{-1}= \frac{1}{\abs{a}^2+\abs{b}^2}
 \begin{pmatrix} 
\overline{a} & -b \\
\overline{b}& a
\end{pmatrix}
$$
que es un elemento de $\cua$.

\begin{defi}

Llamamos {\sf cuerpo\footnote{Algunos autores llaman {\sf álgebras con división} \index{algebra@álgebra!con división} a los cuerpos no conmutativos} de los cuaterniones} al conjunto $\cua$ dotado de la suma y el producto matriciales.

\end{defi}

Este será nuestro primer modelo de $\cua$.  Lo llamaremos {\sf modelo matricial}.


\bigskip

\noindent{\bf Propiedades.}

\begin{itemize}

\item Tomando dos elementos arbitrarios se observa que, en general, el producto no es conmutativo.

\item Los múltiplos reales de la identidad conmutan con todos los elementos de~$\cua$.  Es más, si $q $ conmuta con todos los elementos de $\cua$, entonces $q = \lambda \mathrm{Id}$ con $\lambda \in \R$ (realizar la comprobación en dos casos: el primero con $a=i$ y $b=0$ y el segundo con $a=0$ y $b=1$).  Hemos calculado el centro de $\cua$.

\end{itemize}





El morfismo natural de $\C$ en $M_2(\C)$ es
$$
c \rightarrow  \begin{pmatrix} 
c& 0 \\
0& c
\end{pmatrix}
$$
Sin embargo, la imagen de este morfismo no está contenida en $\cua$.  Por ello cons\-truimos otro morfismo
$$
c \rightarrow  \begin{pmatrix} 
c& 0 \\
0& \overline{c}
\end{pmatrix}
$$
Este es un morfismo de anillos inyectivo y su imagen si está contenida en $\cua$.  {\bf Via este nuevo morfismo} podemos considerar que $\C \subset \cua$.  Por restricción, también tenemos que $\R \subset \cua$.  Utilizando estas inclusiones, podemos dotar a $\cua$ de una estructura de espacio vectorial real o complejo, realizando la multiplicación de escalares por la izquierda.  En el caso real es indiferente que la multiplicación se realice  por la derecha o por la izquierda, puesto que $\R$ está contenido en el centro de $\cua$.  Pero en el caso complejo es fundamental, pues si multiplicamos por la derecha obtenemos otra estructura de espacio vectorial.

\bigskip

La dimensión de $\cua$ como espacio complejo es 2 y la {\sf base estandar} está formada por los elementos
$$
1=
\begin{pmatrix}
 1 & 0 \\
0& 1
\end{pmatrix}
\qquad j= 
\begin{pmatrix}
0 & 1 \\
-1 & 0
\end{pmatrix}
$$
Todo elemento de $\cua$ se puede escribir  en la forma
$$
a + bj \text{ con } a, b \in \C
$$
Este será nuestro segundo modelo de los cuaterniones, que lo llamaremos {\sf modelo complejo}.  Operando (y teniendo en cuenta la inclusión de $\C$ en $\cua$).
$$
a+bj= \begin{pmatrix}
a & 0\\
0 &\overline{a} 
\end{pmatrix}
 \begin{pmatrix}
1 & 0\\
0 &1 
\end{pmatrix}+
 \begin{pmatrix}
b & 0\\
0& \overline{b} 
\end{pmatrix}
 \begin{pmatrix}
0 & 1\\
-1 &0 
\end{pmatrix}=
 \begin{pmatrix}
a & b\\
-\overline{b}& \overline{a} 
\end{pmatrix}
$$
Para operar en un álgebra necesitamos saber dos cosas:

\begin{enumerate}[\indent 1.- ]

\item El producto de todos los elementos de una base.

\item Si el cuerpo no está contenido en el centro del álgebra, necesitamos, si existe, una fórmula que nos permita conmutar los elementos del cuerpo y los de una base.

\end{enumerate}


\noindent{\bf Reglas de conmutación.}

\begin{itemize}

\item $j^2=-1$

\item $zj = j \overline{z}$ donde $\overline{z} $ es el complejo conjugado de $z$. La multiplicación por $j$ es una operación antilineal.

\end{itemize}
Con estas dos reglas, y aplicando la propiedad distributiva, podemos realizar cualquier producto de cuaterniones.

\bigskip

Considerando $\cua$ como un espacio vectorial real tenemos la {\sf base estandard}
$$
1=
\begin{pmatrix}
 1 & 0 \\
0& 1
\end{pmatrix}
\qquad i= 
\begin{pmatrix}
i & 0 \\
0 & -i
\end{pmatrix}
\qquad
j=
\begin{pmatrix}
 0 & 1 \\
-1& 0
\end{pmatrix}
\qquad k= 
\begin{pmatrix}
0 & i \\
i & 0
\end{pmatrix}
$$
Todo cuaternion se escribe de modo único como
$$
q= a+bi+cj+dk \text{ con } a,b,c,d \in \R
$$
Este es nuestro tercer modelo de $\cua$ y es con el que inicialmente trabajo Hamilton.  Lo llamaremos {\sf modelo real} o {\sf modelo hamiltoniano}.

\bigskip

\noindent{\bf Reglas de conmutación.}

\begin{itemize}

\item $i^2= j^2=k^2=-1$

\item $ij=-ji=k; \quad jk=-kj=i; \quad ki=-ik=j$

\end{itemize}
Operando, observamos que
$$
q=a+bi+cj+dk=
\begin{pmatrix}
a+bi & c+di\\
-c+di& a-bi
\end{pmatrix}= (a+bi)+ (c+di)j
$$

\noindent{\bf Observación.}

\smallskip

En algunos casos la letra $i$ indica una matriz y en otros indica la unidad imaginaria.  No existe peligro de confusión, puesto que al inyectar $\C$ en $\cua$ a la unidad imaginaria $i$ le corresponde la matriz $i$.\fin



La notación empleada, sugiere que todo cuaternión se puede escribir, formalmente, como la suma de un escalar y de un vector tridimensional del espacio euclídeo
$$
q= a+\mathbf{v} \text{ donde } \mathbf{v} \text{ es el vector } bi+cj+dk
$$
Este es el {\sf modelo vectorial}\index{modelo!vectorial} de $\cua$. La multiplicación de dos cuaterniones se realiza en este modelo mediante la fórmula
$$
(a+\mathbf{u})(b+\mathbf{v})= (ab-\escalar{\mathbf{u}}{\mathbf{v}})+(a\mathbf{v}+b\mathbf{u}+\mathbf{u}\wedge \mathbf{v})
$$
donde tanto el producto escalar como vectorial son los canónicos ($\{i,j,k\}$ es una base ortonormal orientada positivamente).
En particular, si los dos cuaterniones tienen únicamente parte vectorial
$$
\mathbf{u}\mathbf{v}=-\escalar{\mathbf{u}}{\mathbf{v}}+\mathbf{u}\wedge\mathbf{v}
$$



A la hora de dar definiciones muchas veces necesitamos recurrir a un modelo. Cuando hagamos esto veremos cual es la definición adecuada en los otros modelos.  La cuestión de elegir un modelo u otro es simplemente la comodidad.

\begin{defi}

Dado $q=\left(
\begin{smallmatrix}
a & b \\
-\overline{b}& \overline{a}
\end{smallmatrix}\right)
$ llamamos {\sf congujado}, y denotamos $\overline{q}$, a la matriz traspuesta conjugada
$$
\overline{q}= q^*= \begin{pmatrix}
\overline{a}& -b \\
\overline{b}& a
\end{pmatrix}
$$
\end{defi}




\noindent{\bf Propiedades.}

\begin{itemize}

\item Si $q \in \cua$ entonces $\overline{q} \in \cua$. Además $\overline{q+r}= \overline{q}+\overline{r}$

\item $\overline{q\cdot r}= (q\cdot r)^*= r^*\cdot q^* =\overline{r}\cdot \overline{q}\,\, $ aplicando la conocida propiedad de la trasposición.  La conjugación $q \rightarrow \overline{q}$ es un antimorfismo de anillos.

\item $\overline{\overline{q}}= (q^*)^* =q$.  La conjugación es involutiva.  Es inyectiva y epiyectiva.

\item Si $q \in \C$ esta conjugación coincide con la conjugación compleja de toda la vida.  No puede existir confusión entre las dos, a pesar de que denoten del mismo modo.

\item Si $q= a +b\cdot j$ entonces $\overline{q}= \overline{a}+\overline{b\cdot j}= \overline{a}+ \overline{j}\cdot \overline{b} = \overline{a}-b\cdot j$
donde hemos utilizado que $\overline{j}= -j$ y las reglas de conmutación.  Pasando al modelo real, si
$$
q= a+bi+cj+dk \quad \Rightarrow \quad\overline{q}= a-bi-cj-dk
$$
puesto que $\overline{i} = -i$, $ \overline{j}=-j$ y $\overline{k}= -k$ y que la conjugación sobre los reales es la identidad. En el modelo vectorial
$$
q=a+\mathbf{v}\quad \Rightarrow \quad \overline{q}=a-\mathbf{v}
$$


\end{itemize}


\begin{defi}

Llamamos {\sf norma}\footnote{Algunos autores llaman norma a la raíz cuadrada de nuestra definición de norma.  Esta definición se ajusta más con la idea intuitiva de norma como longitud de un vector, pero se maneja peor operativamente hablando} de un cuaternión $q$ a
$$
N(q)=  q\overline{q}
$$

\end{defi}

Si $q = a+bj$ operando se obtiene que $N(q)= \abs{a}^2+\abs{b}^2$.  En el modelo matricial se tiene que
$$
N\begin{pmatrix}
a & b \\
-\overline{b}& \overline{a}
\end{pmatrix}=
\begin{pmatrix}
\abs{a}^2+\abs{b}^2 & 0 \\
0 & \abs{a}^2+\abs{b}^2
\end{pmatrix}
$$
 en el modelo real
$$
N(a+bi+dj+dk)= a^2+b^2+c^2+d^2
$$
y en modelo vectorial
$$
N(a+\mathbf{v})= a^2+\escalar{\mathbf{v}}{\mathbf{v}}
$$
donde el producto escalar de vectores es el canónico.

\bigskip

La norma de un cuaternión coincide con el cuadrado de la longitud del correspondiente vector de $\R^4$.  Esto permite introducir una estructura euclídea en $\cua$ definida a partir de la estructura algebraica de $\cua$.  Recordemos que conocida la norma de un espacio euclídeo, se puede recuperar el producto escalar utilizando la {\sf identidad de polarización}
$$
\escalar{x}{y}= \frac{\norma{x+y}^2-\norma{x}^2-\norma{y}^2}{2}
$$
que aplicado a nuestro caso
$$
\escalar{q}{r}= \frac{N(q+r)-N(q)-N(r)}{2}= \frac{q\overline{r}+r\overline{q}}{2}
$$

Respecto a este producto escalar, $\cua$ es un modelo de espacio euclídeo de dimensión cuatro. La base canónica  $\{1,i,j,k\}$ es una base ortonormal.

\bigskip

\noindent{\bf Propiedades.}


\begin{itemize}

\item Naturalmente $N(q)=\escalar{q}{q}$.  


\item $N(q)= 0$ si y solo si $q=0$ y además $N(q)=N(\overline{q})$ utilizando, por simplicidad, el modelo real.

\item En el modelo matricial, la norma se puede calcular con determinantes
$$
N
\begin{pmatrix}
a & b \\
-\overline{b}&\overline{a}
\end{pmatrix} = a\overline{a}+ b\overline{b}= \det\begin{pmatrix}
a & b \\
-\overline{b}&\overline{a}
\end{pmatrix}
$$
y el producto escalar con la traza
$$
\escalar{q}{r}= \frac{1}{2}\mathrm{Tr}(q\overline{r})
$$

\item $N(qr) = N(q)N(r)$ aplicando propiedades de determinantes.  

\item Si $q$ es no nulo, construimos $q'= \overline{q}/N(q)$.  Observamos que
$$
q q'= q \frac{\overline{q}}{N(q)}= \frac{q\overline{q}}{N(q)}=1
$$
de donde $q^{-1}= \overline{q}/N(q)$, fórmula análoga a la  del análisis complejo.



\end{itemize}



\begin{defi}

Llamamos {\sf parte real} de un cuaternión a 
$$
\mathrm{Re}(q)= \frac{1}{2}\mathrm{Tr}(q)
$$
y {\sf parte imaginaria} a 
$$
\mathrm{Im}(q)=q-\mathrm{Re}(q)
$$

\end{defi}

Por construcción es claro que $q= \mathrm{Re}(q)+\mathrm{Im}(q)$.  Los cuaterniones que no tienen parte real se dice que son {\sf imaginarios puros}.  Denotaremos por $\mathrm{Im}(\cua)$ al conjunto de cuaterniones imaginarios puros
$$
\mathrm{Im}(\cua)=\{ q \in\cua \text{ tales que } \mathrm{Re}(q)=0\}
$$
Es un subespacio vectorial real, pero no es un subespacio complejo. Respecto al producto escalar, el subespacio de los cuaterniones imaginarios puros es justamente el ortogonal de $\R$.  Restringiendo el producto escalar a $\mathrm{Im}(\cua)$ obtenemos un modelo de espacio euclídeo tridimensional.

\bigskip



\noindent{\bf Propiedades.}

\begin{itemize}

\item $\mathrm{Re}
\left(\begin{smallmatrix}
a & b \\
-\overline{b}& \overline{a}
\end{smallmatrix}
\right)= \frac{1}{2} (a+\overline{a})= \mathrm{Re}(a)
$.  

\item $\mathrm{Re}(a+bj)= \mathrm{Re}(a)$.  El conjunto de los imaginarios puros en este modelo es
$$
\mathrm{Im}(\cua)= \{\lambda i +bj\text{ donde } \lambda \in \R \text{ y } b \in \C\}
$$

\item $\mathrm{Re}(a+bi+cj+dk)= a$. $\mathrm{Im}(a+bi+cj+dk)=bi+cj+dk$.

\item En el modelo vectorial, la parte imaginaria se identifica con la parte vectorial.  Por eso algunos autores llaman parte vectorial de un cuaternión a lo que nosotros llamamos parte imaginaria.

\item Utilizando el modelo real es claro que
$$
\overline{(\mathrm{Re}(q)+\mathrm{Im}(q))}= \mathrm{Re}(q)-\mathrm{Im}(q)
$$
Entonces
$$
N(q)= q \overline{q}= (\mathrm{Re}(q)+\mathrm{Im}(q))(\mathrm{Re}(q)-\mathrm{Im}(q))= \mathrm{Re}(q)^2-\mathrm{Im}(q)^2
$$
puesto que las partes reales e imaginarias conmutan.  También son bastante evidentes las relaciones
$$
\mathrm{Re}(q)= \frac{q+\overline{q}}{2} \qquad \mathrm{Im}(q)= \frac{q-\overline{q}}{2}
$$
que son las que habitualmente se utilizan para definir estos conceptos por su analogía con el análisis complejo.

\end{itemize}


\bigskip

Denotamos por $\mathrm{Sp}(1)$ al conjunto de los cuaterniones de norma unidad.  Este conjunto es un subgrupo respecto a la multiplicación y su variedad soporte es la esfera tridimensional $S^3$.  Es un grupo de Lie.  En forma matricial
$$
\mathrm{Sp}(1)=\left\{ \begin{pmatrix}
a & b \\
-\overline{b}& \overline{a}
\end{pmatrix} \text{ tales que } \abs{a}^2+\abs{b}^2=1\right\}
$$
que coincide exactamente con el grupo $\mathrm{SU}(2)$.



\begin{propo}

Dado $q\in\mathrm{Sp}(1)$ denotamos por $R_q:\mathrm{Im}(\cua) \rightarrow \mathrm{Im}(\cua)$ a la función $R_q(u)= quq^{-1}$.  La aplicación
$$
\begin{array}{cccc}
R: & \mathrm{Sp}(1)& \rightarrow & \mathrm{SO}(3,\R)\\
   & q & \rightarrow& R_q
   \end{array}
   $$
es un morfismo de grupos de Lie epiyectivo con núcleo $\Z_2$.

\end{propo}

\dem

Dividiremos la demostración en varios pasos simples, pues este esquema se utilizará  más adelante.

\smallskip

$\qquad$ 1) $R_q$ deja invariante $\mathrm{Im}(\cua)$.

\noindent $u \in \mathrm{Im}(\cua)$ si y solo si $\mathrm{Tr}(u)=0$.  Calculamos la traza de $R_q(u)$
$$
\mathrm{Tr}(R_q(u))= \mathrm{Tr}(quq^{-1})= \mathrm{Tr}(qq^{-1}u) = \mathrm{Tr}(u)= 0
$$
\smallskip

$\qquad$  2) $R_q$ es lineal y biyectiva.
   
   \noindent Es $\R$-lineal, puesto que $\R$ pertenece al centro de $\cua$.  Si $R_q(u)=0$, entonces $quq^{-1}=0$ y despejando $u$ obtenemos que $u=0$.  $R_q$ es inyectiva y por razones dimensionales también es epiyectiva.
   
   \smallskip
   
$\qquad$  3) $R_q$ es una isometría.
   
   \noindent Para que una aplicación conserve el producto escalar basta con que conserve la norma que induce dicho producto escalar. Como $N(q)= \norma{q}^2$, basta con la conservación de la norma cuaterniónica.
   $$
   N(R_q(u))= \det(quq^{-1})= \det(u)= N(u)
   $$
   
   \smallskip
   
$\qquad$  4) $R$ es morfismo de grupos de Lie.
   
   \noindent Tenemos que $R_{qr}= R_qR_r$.  Si escribimos $R_q$ en coordenadas vemos que es diferenciable
   
   \smallskip
   
 $\qquad$ 5) Núcleo de $R$
   
    \noindent Si $q$ está en el núcleo tenemos.
    $$
    R_q(u)= u \quad \text{ si y solo si }\quad q uq^{-1}=u \quad \text{ si y solo si } \quad qu=uq
    $$
    El núcleo es entonces $\mathrm{Ker}(R)= \mathrm{Sp}(1)\cap \R= \{\mathrm{Id},-\mathrm{Id}\}$
    
    \smallskip
    
   $\qquad$  6) Epiyectividad sobre la componente conexa.
    
    Como $R$ es continuo y el grupo $\mathrm{Sp}(1)$ es conexo, la imagen de $R$ debe pertenecer a la componente conexa, que este caso es $\mathrm{SO}(3,\R)$.  Como el núcleo es discreto y los dos grupos en cuestión son de la misma dimensión, la aplicación tangente en el neutro es un isomorfismo.  $R$ es localmente un difeomorfismo y su imagen contiene a un entorno del neutro.  Como la imagen  es un subgrupo que contiene a un entorno del neutro, al ser $\mathrm{SO}(3,\R)$ conexo, se deduce la epiyectividad. \fin
    
    
    \begin{cor}
    
    $\mathrm{SO}(3,\R)$ es difeomorfo al espacio proyectivo real tridimensional.
    
    \end{cor}
    
    \dem
    
    En efecto, ambos son difeomorfos al espacio cociente obtenido identificando dos puntos antipodales de la esfera $S^3$.\fin
    
    
    Hemos visto que a todo cuaternión de módulo unidad se le puede asociar una rotación.  Estamos interesados en conocer efectivamente cual es dicha rotación.  Recordemos que una rotación del espacio euclídeo tridimensional viene determinado por dos datos: el primero es el eje de giro y el segundo es el ángulo  de giro.  Como el eje no varia en una rotación, cada vector del eje es un vector propio (de valor propio $1$).  

Todo cuaternión de módulo unidad se puede escribir en la forma
$$
q= \cos(\theta/2)+ \sen(\theta/2)\mathbf{v} \text{ siendo } \mathbf{v} \text{ un vector unitario}
$$

\begin{propo}

La rotación correspondiente al cuaternión 
$$
q= \cos(\theta/2)+ \sen(\theta/2)\mathbf{v}
$$
tiene por el eje la recta generada por el vector $\mathbf{v}$ y  eje de giro $\theta$.

\end{propo}

\dem

Como $q$ es unitario, $q^{-1}=\overline{q}$. Veamos que $\mathbf{v}$ es un vector propio. 
$$
R_q(v)= q\cdot \mathbf{v}\cdot \overline{q}=(\cos(\theta/2)+ \sen(\theta/2)\mathbf{v})\cdot \mathbf{v}\cdot (\cos(\theta/2)- \sen(\theta/2)\mathbf{v})=\mathbf{v}
$$
donde hemos tenido en cuenta que $\mathbf{v}\cdot\mathbf{v}=-1$.

\smallskip

Si $\mathbf{u}$ es un vector unitario y ortogonal a $\mathbf{v}$, entonces
$$
R_q(\mathbf{u})= q\cdot \mathbf{u}\cdot \overline{q}= \cos(\theta)\cdot \mathbf{u}+ \sen(\theta)\cdot \mathbf{v} \wedge \mathbf{u}
$$
Como $\{\mathbf{v},\mathbf{u},\mathbf{v\wedge u}\}$ es una base positivamente orientada, el efecto de $R_q$ sobre el plano perpendicular a $\mathbf{v}$ es una rotación de ángulo $\theta$. \fin



    \noindent{\bf Observación 1.}
    
    \smallskip

Este resultado nos permite demostrar que $R:\mathrm{Sp}(1) \rightarrow \mathrm{SO}(3,\R)$ es epiyectiva, sin utilizar para nada razonamientos topológicos.  Dada una rotación  arbitraria $A$ (que no sea la identidad) tendrá un eje y un ángulo de giro.  Si $\mathbf{v}$ es un vector unitario del eje y $\theta$ el ángulo de giro, se tiene que 
$R_q=A$, donde \linebreak $q= \cos(\theta/2)+\sen(\theta/2)\cdot \mathbf {v}$. \fin

    
    \noindent{\bf Observación 2.}
    
    \smallskip
    
    Matricialmente resulta que $\mathrm{Im}(\cua)$ es justamente el álgebra de Lie  $\mathfrak{su}(2)$.  Teniendo en cuenta esto, resulta que si $q\in \mathrm{Im}(\cua)$ entonces $\exp(q) \in \mathrm{Sp}(1)$.  Si entendemos $\exp(q)$ como una rotación (en realidad consideramos $R_{\exp(q)}$) obtenemos 
    $$
    R_{\exp(q)}(q)= \exp(q)\cdot q\cdot \exp(-q)= \exp(q)\cdot \exp(-q)\cdot q=q
    $$
    puesto que la exponencial de una matriz conmuta con la matriz.   Dicha rotación tiene como eje el propio vector $q$, entendido  como vector del espacio euclídeo tridimensional. Mediante un tedioso cálculo también se puede obtener el ángulo de giro.    \fin
    
    
    Del mismo modo $\cua$ es un modelo de espacio euclídeo de dimensión cuatro.  Siguiendo argumentos análogos se demuestra la 
    
    \begin{propo}
    
    La aplicación 
    $$
    \begin{array}{cccc}
    R: & \mathrm{Sp}(1) \times \mathrm{Sp}(1) & \rightarrow & \mathrm{SO}(4)\\
     & (q_1,q_2) & \rightarrow & R_{q_1,q_2}
     \end{array}
     $$
     donde $R_{q_1,q_2}(u)= q_1\cdot u\cdot q_2^{-1}$, es un morfismo de grupos de Lie epiyectivo cuyo núcleo es $\Z_2$.
     
     \end{propo}
     
     
     \noindent{\bf Observación.}
     
     \smallskip
     
     Denotemos por $\lambda_q$ la aplicación $\lambda_q(r) = qr$.  Si $q$ es unitario, esta aplicación es una isometría de $\cua$.  Sea $T$ una isometría arbitraria de $\cua$.  El cuaternión $q= T(1)$ es de norma unidad.  La composición $\lambda_{\overline{q}}T$ es una isometría que cumple
     $$
     \lambda_{\overline{q}}T(1)= \lambda_{\overline{q}}q=1
     $$
     y deja invariante el subespacio $\R$.  Por lo tanto también deja invariante su ortogonal, que es $\mathrm{Im}(\cua)$.  Por la proposición anterior $\lambda_{\overline{q}}T= R_r$ y esto prueba la epiyectividad sin recurrir a argumentos topológicos. \fin
     
     En sus trabajos sobre relatividad especial, Minkoski multiplicó la coordenada temporal por la unidad imaginaria.  Operando formalmente, al calcular la norma de un vector, aparecían tres cuadrados con signo más y uno con signo menos. Sin en vez de multiplicar la coordenada temporal por $i$,  multiplicamos las otras tres por $i$ obtenemos también un modelo del espacio de Minkoski. Vamos a seguir esta intuición para construir un modelo matricial de una métrica de signatura $(3,1)$. 
     
     Sea $F$ el subespacio real de $M_2(\C)$ generado por las matrices
     $$
1=
\begin{pmatrix}
 1 & 0 \\
0& 1
\end{pmatrix}
\qquad \alpha= 
\begin{pmatrix}
1 & 0 \\
0 & -1
\end{pmatrix}
\qquad
\beta=
\begin{pmatrix}
 0 & -i \\
i& 0
\end{pmatrix}
\qquad \gamma= 
\begin{pmatrix}
0 & 1 \\
1 & 0
\end{pmatrix}
$$
Todo elemento de $F$ se expresa como
$$
a1+b\alpha+c\beta+d\gamma= \begin{pmatrix}
a-b & ic-d\\
-ic-d& a+b
\end{pmatrix}
$$
 Pero este espacio $F$ que acabamos de construir no es ni más ni menos que el subespacio de las matrices hermíticas bidimensionales. Si calculamos el determinante de un elemento de este espacio obtenemos $a^2-b^2-c^2-d^2$.  Hemos construido un modelo de espacio de signatura $(3,1)$ donde la forma cuadrática se calcula con un determinante. Además la base que hemos construido es ortogonal respecto a este producto escalar.
 
 \begin{propo}

Sea
$$
\begin{array}{cccc}
R:& \mathrm{Sl}(2, \C)& \rightarrow & \mathrm{O}(\mathrm{Herm}(2))\\
    & g & \rightarrow & R_g
    \end{array}
    $$
    donde $R_g(u)= g\cdot u\cdot g^*$.  $R$ es un morfismo epiyectivo sobre la componente conexa\footnote{En física, la componente conexa se suele denotar $\mathrm{SO}^\uparrow(3,1)$, aunque no existe una notación estándar.} con núcleo $\Z_2$.
    
    
 \end{propo}
 
 El espacio $F'$ generado por las matrices $\{1,\alpha,\gamma\}$ es justamente el ortogonal a $\beta$. Si $g\in \mathrm{Sl}(2,\R)$ se tiene que $R_g(\beta)=\beta$ y por lo tanto induce una isometría del espacio $F'$ que es un modelo de espacio con signatura $(2,1)$. Con estas observaciones se prueba la
 
 
  \begin{propo}

Sea
$$
\begin{array}{cccc}
R:& \mathrm{Sl}(2, \R)& \rightarrow & \mathrm{O}(2,1)\\
    & g & \rightarrow & R_g
    \end{array}
    $$
    donde $R_g(u)= g\cdot u\cdot g^t$.  $R$ es un morfismo epiyectivo sobre la componente conexa con núcleo $\Z_2$.
    
    
 \end{propo}
 
 
 
 
\newpage

\section{Problemas}



\begin{pro}

Demostrar que si un cuaternión conmuta con $i$, con $j$ y con $k$, entonces necesariamente $q$ es un cuaternión real.

\end{pro}

\begin{pro}

Sea $q \in \mathrm{Sp}(1)$. La aplicación
$$
\begin{array}{cccc}
\lambda_q :& \cua & \rightarrow & \cua \\
  & Y& \rightarrow& qY
  \end{array}
  $$
  es una isometría.  Demostrar que la multiplicación por la derecha, $\rho_q$, es también una isometría.
  
  \end{pro}
  
  \begin{pro}
  
  Encontrar la matriz de $\lambda_q$ en la base estandard.  Demostrar que los vectores fila de dicha matriz son perpendiculares entre si.  Si $q$ tiene módulo unidad, entonces la matriz es ortogonal.  Probar también que su determinante es positivo.  Concluir que $\lambda_q$ y $\rho_q$ pertenecen a $\mathrm{SO}(4,\R)$ si $q$ es unitario.
  
  \end{pro}
  
  \begin{pro}
  
  Demostrar que matricialmente $\lambda_{\overline{q}}= (\lambda_q)^t$.  Utilizar este hecho para demostrar que $\lambda_q$ es una matriz ortogonal si $q$ es unitario.
  
  \end{pro}
  
  
  \begin{pro}
  
  Asignemos a cada cuaternión una matriz real de orden $4$
  $$
  q= a+bi+cj+dk \quad \rightarrow 
  \begin{pmatrix}
  a &-b & -c &-d\\
  b &a &-d& c\\
  c&d&a& -b\\
  d&-c&b&a
  \end{pmatrix}
  $$
  Este es otro modelo matricial de $\cua$ que generalmente no se emplea debido a su tamaño. Sin embargo puede ser útil cuando se estudian las rotaciones generadas por los cuaterniones.
  
  \end{pro}

\begin{pro}

Demostrar que $q \in \R \Leftrightarrow q= \overline{q}$ y que  $q \in \mathrm{Im}(\cua)\Leftrightarrow q = -\overline{q}$

\end{pro}

\begin{pro}

Demostrar que $ q\in\mathrm{Im}(\cua)$ si y solo si $q^2\leq 0$.

\end{pro}

\begin{pro}

Demostrar que $q$ y su conjugado tienen la misma norma.

\end{pro}

\begin{pro}

Si $\mathbf{v}$ es un cuaternión unitario y $\theta$ un número real
$$
\exp(\theta\cdot\mathbf{v})=\cos(\theta)+\sen(\theta)\cdot \mathbf{v}
$$

\end{pro}


\begin{pro}

Demostrar que $\mathrm{Re}(qr)=\mathrm{Re}(rq)$.  Calcular dicho valor en algún modelo.

\end{pro}



\begin{pro}

Pretendemos demostrar que todo automorfismo de álgebras de $\cua$ es un rotación de $\mathrm{Im}(\cua)$ y viceversa.

\begin{itemize}

\item Si $q$ es unitario entonces $R_q$ es un automorfismo de álgebras ( $\mathrm{SO}(\mathrm{Im}(\cua) \subset \mathrm{Aut}(\cua)$)




\item Sabiendo que $q \in \mathrm{Im}(\cua) \Leftrightarrow q \cdot q \leq 0$ demostrar que todo automorfismo  deja invariante el subespacio imaginario y sobre $\R$ es la identidad.

\item Si $\varphi$ es automorfismo, entonces $\{\varphi(i),\varphi(j),\varphi(k)\}$  es una base ortonormal del hiperplano imaginario. Demostrar que dicha base está positivamente orientada, lo que implica la inclusión que nos faltaba.



\end{itemize}

\end{pro}


\begin{pro}

Si $q=a+\mathbf{u}$ y $r= b+\mathrm{v}$ entonces
$$
q\cdot r- r\cdot q= 2 \mathbf{u}\wedge\mathbf{v}
$$

\end{pro}

\begin{pro}

Sean $q$ y $r$ dos cuaterniones imaginarios puros.  El producto $q\cdot r$ es imaginario puro si y solo si $q$ y $r$ son ortogonales (entendidos como vectores o como cuaterniones).  En este caso $qr$ es perpendicular tanto a $r$ como a $q$.

\end{pro}

\begin{pro}

Consideremos la ecuación $x^2+1=0$ en el cuerpo de los cuaterniones.  La solución de esta ecuación forma una variedad bidimensional difeomorfa a $S^2$.  En general la ecuación $x^2=-r$ con $r \in \R^+$ tiene infinitas soluciones en $\cua$.  Sin embargo $x^2=r$ con $r \in\R^+$ tiene solamente dos soluciones.

\end{pro}

\begin{pro}

Probar que con la inclusión $\C \subset \cua$, los cuaterniones no forman una $\C$-álgebra (la multiplicación no es $\C$-lineal en ambos miembros).

\end{pro}

\begin{pro}

El conmutador $[q,r]=qr-rq$ dota a los cuaterniones de una estructura de álgebra de Lie.  $\mathrm{Im}(\cua)$ es una subálgebra de Lie.

\end{pro}

\begin{pro}

Sea $\mathbf{v}$ un vector de módulo unidad.  Entonces
$$
\exp(\theta \mathbf{v})= \cos(\theta)+\sen(\theta)\mathbf{v}
$$
fórmula análoga a la del análisis complejo.

\end{pro}

\begin{pro}

Sea $\xi_q$ el endomorfismo adjunto
$$
\xi_q(p) =qpq^{-1}
$$
Demostrar que $\xi_q=\xi_r$ si y solo si $q$ y $r$ son proporcionales.

\end{pro}

\begin{pro}

Probar que 
$$
\mathrm{Re}(p)=\mathrm{Re}(qpq^{-1})
$$
y que
$$
\mathrm{N}(\mathrm{Im}(p))= \mathrm{N}(\mathrm{Im}(qpq^{-1}))
$$

\end{pro}

\begin{pro}

Si $q$ es un cuaternión puro de norma unidad, entonces $R_q$ es el giro de $\pi$ radianes alrededor del eje $q$.  Como estas rotaciones generan el grupo de las rotaciones, esta es otra forma de demostrar la epiyectividad de la aplicación $R$.


\end{pro}

\begin{pro}

Si $q$ es un cuaternión imaginario puro, entonces
$$
\sigma(r)=-q\cdot r\cdot q^{-1}
$$
es la simetría respecto al hiperplano perpendicular a $q$.  Como el grupo ortogonal está generado por simetrías, utilizar este resultado para probar la epiyectividad del morfismo $R:\mathrm{Sp}(1)\rightarrow \mathrm{SO}(3,\R)$.

\end{pro}

\begin{pro}

Sea $q=a+\mathbf{v}$ un cuaternión no necesariamente unitario.  $R_q$ es una rotación cuyo eje es $\langle \mathbf{v}\rangle$ y cuyo ángulo de giro $\theta$ se obtiene de 
$$
\tan\left(\frac{\theta}{2}\right)=\frac{\sqrt{N(\mathbf{v})}}{\abs{a}}
$$

\end{pro}

\begin{pro}

Dados dos cuaterniones imaginarios puros, $q$ y $q'$, que tengan la misma norma, existe un cuaternión imaginario $r$ puro tal $q'=r\cdot q\cdot r^{-1}$

\end{pro}



 
 \newpage
 
 \section{Grupos lineales cuaternionicos}
 

El estudio de los espacios vectoriales sobre $\cua$ puede realizarse sin utilizar para nada el cuerpo $\C$.  Sin embargo nosotros estudiaremos el álgebra lineal sobre $\cua$ en función del álgebra lineal sobre $\C$.  Con esto conseguimos dar interpretaciones complejas de resultados cuaternionicos.  Como $\cua$  no es conmutativo debemos diferenciar entre estructuras de espacio vectorial por la derecha y por la izquierda. Prácticamente todos los resultados elementales conocidos para los espacios vectoriales  sobre cuerpos conmutativos son válidos para espacios vectoriales cuaternionicos. La única excepción es la no existencia de una definición satisfactoria y simple del determinante de una aplicación lineal.  También la representación matricial de aplicaciones lineales presenta algunas sutilezas  que pasamos a exponer.  Sea $V$ un espacio vectorial  cuaternionico (por la izquierda) y $\{e_1,\dots,e_n\}$ una base.  Si $\varphi:V \rightarrow V$ es una aplicación lineal se tiene que 
$$
\varphi(e_i)= q_{ij} e_j
$$
Denotamos por $Q=\{q_{ij}\}$ a la matriz asociada.  Por analogía con el caso conmutativo intentamos asociar una aplicación lineal $\hat Q$ a la matriz $Q$ mediante
$$
\hat Q(v)=q_{ij}v_j \text{ siendo } v= v_ie_i
$$
Esta aplicación es morfismo de grupos, pero  es falso que $\hat Q(rv)=r\hat Q(v)$, debido a la falta de conmutatividad.  Este problema se puede solucionar de dos formas:

\begin{itemize}

\item En vez de poner primero la matriz y despues el vector en forma de columna, se puede colocar primero el vector en forma de fila y despues la matriz.  Esto nos llevaría a definir $\hat Q$ mediante la fórmula
$$
\hat Q(v)=v_jq_{ij}
$$

\item  Realizar la multiplicación por escalares a la derecha en vez de por la izquierda.  En este caso se demuestra fácilmente que
$$
\hat Q(vr)=\hat Q(v)r
$$

\end{itemize}

Nosotros vamos a optar por la segunda solución.  {\bf Trabajaremos siempre con espacio vectoriales cuaternionicos a la derecha}. Para ello, en esta sección, escribiremos los cuaterniones en la forma $q= a+jb$ poniendo la $j$ a la izquierda.  Esto es perfectamente válido, pero debemos tener cuidado al pasar a los otros modelos.  Simplemente recordemos que $a+jb= a+\overline{b}j$.

\smallskip

Si tenemos un espacio complejo y deseamos multiplicar cuaterniones por vectores, utilizando el modelo complejo, observamos que basta conocer como debemos multiplicar $j$ por un vector.  Teniendo en cuenta las propiedades de la multiplicación por $j$, introducimos la 

\begin{defi}

Sea $V$ un espacio complejo.  Una {\sf estrutura cuaternionica}\index{estructura!cuaternionica} es una aplicación $J:V \rightarrow V$ que cumple:

\begin{itemize}


\item Es antilineal: $J(zx+y)= \overline{z}J(x)+J(y)$ donde $z \in\C$.

\item $J^2=-\mathrm{Id}$

\end{itemize}

\end{defi}

De ahora en adelante cuando escribamos $(V,J)$ sobreentenderemos que $V$ es un espacio vectorial  complejo y que $J$ es una estructura cuaternionica en $V$.

Con ayuda del operador $J$ definimos un producto de un elemento de $\cua$ por un vector
$$
v\cdot (a+jb)= v\cdot a+J(u)\cdot b= a\cdot v+b\cdot J(v)
$$
(los números complejos si que se pueden permutar). Con esta definición $V$ es un espacio vectorial  sobre $\cua$.  Vemos que llamar a $J$ estructura cuaternionica está plenamente justificado. Como  $V$ admite una estructura de espacio vectorial sobre dos cuerpos, si existe peligro de confusión, lo denotaremos por $V_\C$ cuando lo consideremos como espacio vectorial complejo y mediante $V_\cua$ cuando consideremos en él la estructura cuaternionica.

Recíprocamente, si $V$ es un espacio vectorial  sobre $\cua$, como $\C \subset \cua$, $V$ es también un espacio complejo, que denotamos $V_\C$.  Si denotamos por $J$ el operador que consiste en multiplicar por $j$,  se tiene que $J$ es una estructura cuaternionica en $V_\C$

\bigskip

\noindent{\bf Observación.}

\smallskip

Hasta ahora hemos considerado que $\C$ es un subconjunto de $\cua$, haciendo corresponder al número imaginario $i$ el cuaternión $i$.  Sin embargo, si $q$ es un cuaternión puro de norma unidad, el morfismo $a+bi\rightarrow a+bq$ es también una inyección de $\C$ en $\cua$.  Si consideramos esta inclusión, la estructura de espacio vectorial sobre $\C$ depende de $q$.  Mientras no se diga lo contrario, nosotros entenderemos que $\C$ está contenido en $\cua$ de la forma que siempre lo hemos considerado. \fin

\bigskip

\noindent{\bf Ejemplos.}

\begin{itemize}

\item $\cua^n$ es de modo natural un $\cua$-espacio vectorial.  Como  todo espacio vectorial de dimensión finita sobre $\cua$ tiene una base, todo espacio vectorial cuaternionico es isomorfo a un cierto $\cua^n$.  

\item El conjunto $M_{n,m}(\cua)$ de las matrices $n\times m$ con coeficientes cuatenionicos es un espacio vectorial sobre $\cua$.

\end{itemize}

\begin{lema}

Dado $(V,J)$ y $e \neq 0$, entonces $e$ y $J(e)$ son linealmente independientes (sobre $\C$).

\end{lema}

\dem

Supongamos lo contrario. Entonces existe $\lambda \in \C$ tal que $J(e)=\lambda e$.  Aplicando $J$ a esta identidad
$$
\begin{array}{c}
J(J(e))= J(\lambda e) \Leftrightarrow\\
-e = \overline{\lambda}(J(e) \Leftrightarrow\\
-e= \overline{\lambda}\lambda e= \abs{\lambda}^2e
\end{array}
$$
lo cual es imposible si $e$ es no nulo. \fin

\begin{propo}

Si $(V,J)$ es dimensión finita, existen bases (con respecto a la estrutura compleja) de la forma
$$
\{e_1, \dots, e_n, J(e_1)\dots, J(e_n)\}
$$

\end{propo}

\dem


Como $J$ es una estructura cuaternionica, podemos considerar que $V$ es un espacio vectorial sobre $\cua$.  Como es de dimensión finita, este espacio vectorial sobre $\cua$ posee una base que denotamos $\{e_1,\cdots, e_n\}$.  Entonces
$$
\{e_1, \dots, e_n, J(e_1), \dots, J(e_n)\}
$$
es una base de la estructura compleja de $V$. \fin

\begin{defi}

Llamamos {\sf bases complejas cuaternionicas} a las dadas en la proposición anterior.

\end{defi}



\begin{cor}

$(V,J)$ es siempre de dimensión par como espacio complejo.

\end{cor}

\begin{cor}

La aplicación $J$ en una base compleja cuaternionica tiene por matriz
$$
W=\begin{pmatrix}
0 & -\mathrm{Id}\\
\mathrm{Id}& 0
\end{pmatrix}
$$

\end{cor}

\begin{cor}

Si $u$ es un vector columna que expresa un vector de $V_\C$ en una base compleja cuaternionica, entonces 
$$
J(u)= W\cdot \overline{u}
$$
es la expresión de la imagen de $J$ en la misma base.  La conjugación de $u$ deriva de la antilinealidad de $J$.

\end{cor}


Toda aplicación $\cua$-lineal es también $\C$-lineal.  El recíproco no es válido.  Sin embargo basta con que la aplicación conmute con $J$ para que sea $\cua$-lineal.

\begin{lema}

$\mathrm{End}_\cua(V,J)=\{\varphi\in \mathrm{End}_\C(V)\text{ tales que } \varphi J= J\varphi\}$

\end{lema}


\dem

Si $\varphi$ es $\cua$-lineal entonces
$$
\varphi(uj) =  \varphi(u)j \text{ para todo } u \in V
$$
por lo tanto $\varphi J=J\varphi$.

\smallskip

Si $\varphi$ conmuta con $J$ 
$$
\varphi(u(a+jb))=\varphi(au+bJ(u))=a\varphi(u)+bJ(u)= a\varphi(u)+bJ\varphi(u)=\varphi(u)(a+jb)
$$
y la aplicación es $\cua$-lineal.  \fin

Del mismo modo es claro que
$$
\mathrm{GL}_\cua(V,J)=\{\varphi \in \mathrm{GL}_\C(V) \text{ tales que } \varphi J=J\varphi\}
$$
El grupo lineal cuaternionico, gracias a esta identificación, se puede considerar como un subgrupo cerrado del grupo lineal complejo.  

El concepto de determinante sobre cuerpos no conmutativos no posee todas las propiedades a la que estamos acostumbrados.  En vez de utilizar dicho determinante para definir el grupo especial lineal, utilizaremos determinantes de aplicaciones $\C$-lineales.  Definimos entonces
$$
\mathrm{Sl}(V,J)= \{\varphi\in \mathrm{GL}(V,J) \text{ tales que } {\det}_\C(\varphi)=1  \}
$$

A nivel matricial, el que $J$ sea antilineal se refleja en que 
$$
(JA)(u)= J(A\cdot u )= J\cdot \overline{A\cdot u}=J\cdot \overline{A}\cdot \overline{u}
$$
y
$$
(AJ)(u)= A(J\cdot \overline{u})= A\cdot J \cdot \overline{u}
$$
Por ello, matricialmente
$$
M_n(\cua)=\{ A\in M_{2n}(\C) \text{ tales que } AJ=J\overline{A}\}
$$ 
Tomando determinantes en el resultado anterior
$$
\det(A)\det(J)=\det(J)\det(\overline{A})
$$
y el determinante de la matriz compleja asociada es siempre real.

Analizando por bloques la condición matricial anterior toda matriz asociada a una matriz cuaternionica es de la forma
$$
\begin{pmatrix}
A & B \\
-\overline{B}& \overline{A}
\end{pmatrix}
$$
donde $A$ y $B$ son matrices complejas invertibles de orden $n$.







\newpage

\section{Descomposición polar}

Sea $V$ un espacio vectorial complejo y $\escalar{\cdot}{\cdot}$ una forma hermítica definida positiva.  Dado un endomorfismo $A$, su adjunto, $A^*$, es el único endomorfismo que cumple
$$
\escalar{A(u)}{v}=\escalar{u}{A^*(v)} \text{ para todo } u, v \in V
$$
Si aplicamos esta definición a un producto de endomorfismos
$$
\escalar{AB(u)}{v}= \escalar{B(u)}{A^*(v)}= \escalar{u}{B^*A^*(v)}
$$
Entonces $(AB)^*=B^*A^*$ para cualquier par de endomorfismos.  Si uno de los dos endomorfismo es una homotecia
$$
\escalar{\lambda A(u)}{v}= \lambda\escalar{u}{A^*(v)}= \escalar{u}{\overline{\lambda}A^*(v)}
$$
y $(\lambda A)^*= \overline{\lambda}A^*$.  De modo análogo  se prueba que $(A^*)^*=A$.

\smallskip

Los endomorfismos que coinciden con su adjunto se denominan {\sf autoadjuntos} 
\index{endomorfismo!autoadjunto} o \index{endomorfismo!hermítico} {\sf hermíticos}. Los denotaremos por $\mathrm{Herm}(V)$.  Este conjunto es un subespacio real (pero no complejo) de $\mathrm{End}(V)$ de dimensión real $n^2$, siendo $n$ la dimensión compleja de $V$. 
Si tomamos una base ortonormal, la matriz del adjunto es la traspuesta conjugada ($A^*= 
\overline{A^t}$).  


\begin{propo}

Todo endomorfismo hermítico diagonaliza en una base ortonormal, siendo reales todos sus autovalores. 


\end{propo}

\sdem

Sea $\lambda$ un autovalor de $A$ y sea $v$ un autovector no nulo.
$$
\escalar{A(v)}{v}=\escalar{\lambda v}{v}= \lambda\escalar{v}{v}
$$
Aplicando que es  hermítico
$$
\escalar{A(v)}{v}= \escalar{v}{A(v)}=\escalar{v}{\lambda v}= \overline{\lambda}\escalar{v}{v}
$$
y $\lambda$  es real.

\smallskip

Como $A$ actua en un espacio complejo, necesariamente tiene un autovalor no nulo.  Si $v$ es un vector propio, resulta que $v^\perp$ es un subespacio invariante por~$A$, que restingido a dicho subespacio sigue siendo hermítico. Un proceso inductivo  muestra que es posible hallar la base ortonormal. \fin





 Si todos los autovalores de  $A$ son positivos, decimos que  el endomorfismo es\index{endomorfismo!positivo} {\sf positivo}.  Esto equivale a que $\escalar{A(u)}{u} >0$ si $u \neq 0$.  El conjunto de endomorfismos positivos se denota $\mathrm{Herm}_+(V)$.

\begin{lema}\label{lema:hermitico}

La exponencial
$$
\begin{array}{ccc}
\mathrm{Herm}(V)& \rightarrow &\mathrm{Herm}_+(V)\\
 A & \rightarrow &\exp(A)
  \end{array}
  $$
es una biyección.

\end{lema}  

\dem

Sea $A$ hermítico.  Lo diagonalizamos
$$
A=
\begin{pmatrix}
\lambda_1 & 0 &0 \\
0 & \ddots & 0\\
0&0 &\lambda_n
\end{pmatrix}
$$
En dicha base
$$
\exp(A)=
\begin{pmatrix}
e^{\lambda_1} & 0 &0 \\
0 & \ddots & 0\\
0&0 &e^{\lambda_n}
\end{pmatrix}
$$
y la imagen está contenida en $\mathrm{Herm}^+(V)$.

\smallskip

Para ver la epiyectividad, tomamos $P \in \mathrm{Herm}_+(V)$ y lo diagonalizamos
$$
P=
\begin{pmatrix}
\lambda_1 & 0 &0 \\
0 & \ddots & 0\\
0&0 &\lambda_n
\end{pmatrix}
$$
Como $\lambda_i$ es positivo, podemos tomar logaritmos.  Sea $A$ la aplicación que en dicha base tenga por matriz
$$
A=
\begin{pmatrix}
\ln(\lambda_1) & 0 &0 \\
0 & \ddots & 0\\
0&0 &\ln(\lambda_n)
\end{pmatrix}
$$
Se tiene que $\exp(A)=P$.

\smallskip

Veamos que también es inyectiva.  Si $\exp(A)=P$, los endomorfismos $A$ y $P$ conmutan.  Sea $V_i$ el subespacio de vectores propios de $P$ de valor $\lambda_i$.  Se tiene que $V= \oplus V_i$, siendo la suma ortogonal.  Como $A$ y $P$ conmutan, $A(V_i)\subset V_i$ y $A$ diagonaliza en cada subespacio $V_i$.  Hemos probado que existe una base ortonormal que diagonaliza tanto a $P$ como  a $A$.  En dicha base
$$
P=
\begin{pmatrix}
\lambda_1 & 0 &0 \\
0 & \ddots & 0\\
0&0 &\lambda_n
\end{pmatrix}
$$
necesariamente
$$
A=
\begin{pmatrix}
\ln(\lambda_1) & 0 &0 \\
0 & \ddots & 0\\
0&0 &\ln(\lambda_n)
\end{pmatrix}
$$
y $A$ es única. \fin


\noindent{\bf Observación.}

\smallskip

Analizando la serie que define la exponencial, $(e^A)^*= e^{A^*}$.  Si $A$ es hermítica, $e^A$ también lo es.  Si $A$ es hermítica y $u \neq 0$
$$
\escalar{e^A(u)}{u}= \escalar{e^{A/2}e^{A/2}(u)}{u}= \escalar{e^{A/2}(u)}{e^{A/2}(u)}>0
$$
puesto que $e^{A/2}$ es invertible. La exponencial de un endomorfismo hermítico es otro endomorfismo hermítico, que siempre es positivo. \fin


Un endomorfismo $A$ conserva la métrica si $\escalar{A(u)}{A(v)}=\escalar{u}{v}$ para todo par de vectores.  Los endomorfismos que conservan la forma hermítica se llaman \index{endomorfismo!unitario} {\sf unitarios}.  En una base ortonormal, su matriz cumple la condición $A^*A= \mathrm{Id}$.  El conjunto de endomorfismos unitarios de $V$ se denota $\mathrm{U}(V)$ y es un grupo de Lie.  Los unitarios de determinante unidad forman el {\sf grupo unitario especial}
$$
\mathrm{SU}(V)=\{ g \in \mathrm{U}(V) \text{ tales que } \det(g)=1\}
$$

\begin{lema}\label{lema:unitario}

Se tiene un isomorfismo de grupos
$$
\mathrm{U}(V) \rightarrow \mathrm{U}(1)\times \mathrm{SU}(V)
$$

\end{lema}

\sdem

Construimos el isomorfismo \guillemotleft extrayendo\guillemotright\ el determinante del endomorfismo unitario
$$
\begin{array}{ccc}
\mathrm{U}(V) & \rightarrow &\mathrm{U}(1) \times \mathrm{SU}(V)\\
A& \rightarrow & (\det(A),\det(A^{-1})\cdot A)
    \end{array}
$$
La aplicación
$$
\begin{array}{ccc}
\mathrm{U}(1)\times \mathrm{SU}(V)& \rightarrow& \mathrm{U}(V)\\
(z,A)& \rightarrow& z\cdot A
    \end{array}
    $$
    es la inversa. \fin
    
\begin{teo}[Descomposición polar]

La aplicación 
$$
\begin{array}{ccc}
\mathrm{U}(V) \times \mathrm{Herm}(V) & \rightarrow& \mathrm{Gl}(V)\\
 (u,A)& \rightarrow& u \cdot e^A
  \end{array}
  $$
  es un difeomorfismo (no es isomorfismo de grupos).
  
  \end{teo}
  
  
  \dem
  

Si $g= u \cdot e^A$ entonces $g^*= e^A\cdot u^*$.  Multiplicamos
$$
g^*\cdot g= e^A\cdot u^*\cdot u\cdot e^A= e^{2A}
$$
debido a que $u$ es unitaria.  Por el lema \ref{lema:hermitico} $A$ es única.  En este caso\linebreak $u= g\exp(-A)$ también es única.

\smallskip

Este análisis nos indica un método para probar la epiyectividad.  Tomamos $g \in\mathrm{Gl}(V)$.  El producto $g^*\cdot g$ es hermítico positivo y existe $A$ tal que $g^*\cdot g= e^{2A}$.  Si llamamos $u= g\cdot e^{-A}$ es claro que $g= u\cdot e^A$.  Solo falta comprobar que $u$ es unitario
$$
u^*\cdot u = e^{-A}\cdot g^*\cdot g\cdot e^A = e^{-A}\cdot e^{2A}\cdot e^A= \mathrm{Id}
$$
y efectivamente es unitario. Claramente es diferenciable. \fin

 A nivel matricial se tiene un difeomorfismo
 $$
\mathrm{U}(n,\C) \times \mathrm{Herm}(n) \rightarrow \mathrm{Gl}(n, \C)
 $$
 Como $\mathrm{dim}(\mathrm{Herm}(n))= n^2$, este teorema demuestra que $\mathrm{dim}(\mathrm{U}(n,\C))=n^2$ y por el lema \ref{lema:unitario} $\mathrm{dim}(\mathrm{SU}(n, \C))= n^2-1$.
 
 \bigskip
 
 \noindent{\bf Observación.}
 
 \smallskip
 
 Haciendo un estudio análogo se demuestra que todo elemento $g \in \mathrm{Gl}(V)$ se puede escribir en la forma
 $$
 g= e^A  u
 $$
 Sin embargo, en principio, estos endomorfismos no coinciden con los de nuestro teorema, puesto que los endomorfismos no conmutan.  
 
 
 Sea $ad_u$ el automorfismo $ad_u(g)= ugu^{-1}$.  Le aplicamos $ad_u$ a la descomposición polar 
 $$
 ad_u(g)=u\cdot e^A\cdot u\cdot u^{-1}=u\cdot e^A
 $$
 y por otra parte, teniendo en cuenta que es morfismo de grupos
 $$
 ad_u(g) =ad_u(e^A)\cdot ad_u(u)= e^{Ad_u(A)}\cdot  u
 $$
 siendo $Ad_u$ la derivada del morfismo $ad_u$.  Se tiene 
 $$
 u\cdot e^A= e^{Ad_u(A)}\cdot u
 $$
 Como $Ad_u(A)$ es hermítica\footnote{Calculemos como actua $Ad_u$ en el álgebra de Lie. Si $X\in \mathfrak{gl}(V)$
 $$
 Ad_u(X)= \frac{d}{dt}\left(ad_u\exp(tX)\right)_{\mid t=0}=
 \frac{d}{dt}(\left(u\exp(tX)u^{-1}\right)_{\mid t=0}= u Xu^{-1}
 $$
 De este modo $Ad_u(X)= uXu^{-1}$.  Como $X$ es hermítica y $u$ unitaria, es fácil ver que $Ad_u(X)$ es también hermítica.},  la parte unitaria es la misma en cualquiera de los dos casos. 
 
 \bigskip
  
  

  \begin{cor}
  
  Como $\mathrm{U}(V)$ es conexo, $\mathrm{Gl}(V) $ también lo es.  El grupo fundamental del grupo lineal es $\Z$ (el mismo que el del grupo unitario).
  
  \end{cor}
  
  En el caso euclídeo, los endomorfismos autoadjuntos se llaman también simétricos (pues su matriz en una base ortonormal es simétrica). Un estudio similar (o matricialmente, considerando las matrices reales de $\mathrm{Gl}(n,\C)$)
  prueba el
  
  \begin{teo}[Descomposición polar]
  
  Sea $V$ un espacio vectorial real.  La aplicación
  $$
\begin{array}{cccc}
 \mathrm{O}(V) \times \mathrm{Sim}(V) & \rightarrow& \mathrm{Gl}(V)\\
  (u,A)& \rightarrow& u \cdot e^A
  \end{array}
  $$
  es un difeomorfismo.
  
  \end{teo}
   
  A nivel matricial,  es la restricción de la descomposición polar compleja.
  
  \begin{cor}
  
  Como $\mathrm{O}(V)$ tiene dos componentes conexas, $\mathrm{Gl}(V)$ también.  El subgrupo $\mathrm{Gl}^+(V)$ de las aplicaciones  de determinante positivo es la componente conexa de la unidad y su grupo fundamental coincide con el de $\mathrm{SO}(V)$.
  
  \end{cor} 
  
  
  \noindent{\bf Observación.}
  
  \smallskip
  
  Recordemos que $\pi_1(\mathrm{SO}(1,\R))=1$ puesto que el grupo ortogonal consta de un solo punto.  Como $ \mathrm{SO}(2,\R)$ es isomorfo a la circunferencia, su grupo fundamental es $\Z$. Para $n>3$ el grupo fundamental\footnote{El revestimiento universal de $\mathrm{SO}(3,\R)$ es grupo de las unidades cuaternionicas (o también $\mathrm{SU}(2)$). De este revestimiento obtenemos que su grupo fundamental es $\Z_2$.  Como los grupos ortogonales actuan de modo transitivo sobre las esferas (que son simplemente conexas a partir de $n=2$) y los grupos de isotropía son también grupos ortogonales, por inducción se prueba nuestra afirmación.} de $\mathrm{SO}(n,\R)$ es $\Z_2$.\fin
  
  
Analicemos ahora el caso del grupo especial.  Como para definir el grupo especial tomamos determinantes, tomemos determinantes en la descomposición polar.   En el caso complejo
  $$
  \det(g)= \det(u)\det(e^A)= \det(u) e^{\mathrm{Tr}(A)}
  $$
 $\det(u)$ es un complejo de módulo unidad y $e^{\mathrm{Tr}(A)}$ es un número real positivo. Para que $g \in \mathrm{Sl}(V)$ debe cumplirse que $\det(u)=1$ y que $A$ tenga traza nula.  Denotando por $\mathrm{Herm}_0(V)$ al conjunto de endomorfismos hermíticos de traza nula, obtenemos, por restricción, un difeomorfismo
  $$
  \mathrm{SU}(V) \times \mathrm{Herm}_0(V) \rightarrow \mathrm{Sl}(V)
  $$
 En el caso real el difeomorfismo es
  $$
  \mathrm{SO}(V) \times \mathrm{Sim}_0(V) \rightarrow \mathrm{Sl}(V)
  $$ 
     
 \begin{cor}
 
 Si $V$ es complejo, $\mathrm{Sl}(V)$ es conexo y simplemente conexo.  Si $V$ es real, $\mathrm{Sl}(V)$ es conexo y su grupo fundamental coincide con el de $\mathrm{SO}(V)$.
 
 \end{cor}
 
 Analicemos ahora el grupo ortogonal complejo.  Para simplificar los cálculos, tomaremos una versión matricial de dicho grupo.  
 Sea $g \in \mathrm{SO}(n,\C)$.  Matricialmente debe cumplirse que $(g^t)^{-1}=g$.  Tomamos la descomposición polar y le aplicamos la identidad anterior
 $$
 u\cdot e^A= \left((u\cdot e^{A})^t\right)^{-1}    = (u^t)^{-1}e^{-A^t}
 $$
 Dado que $-A^t$ es hermítica y que $(u^t)^{-1}$ es unitaria,  por la unicidad de la descomposición
 $$
 u= (u^t)^{-1} \qquad \text{ y } \qquad A= -A^t
 $$
 Como $u$ es unitaria también se cumple que $(u^*)^{-1}=u$ de donde se deduce que $u=\overline{u}$. La matriz $u$ debe ser real y por lo tanto pertenece al grupo $\mathrm{SO}(n,\R)$. La matriz $A$ debe ser hermítica y antisimétrica.  Se tiene un descomposición polar de $\mathrm{SO}(n,\C)$, donde el subgrupo es $\mathrm{SO}(n,\R)$ y el espacio vectorial esta formado por las matrices hermíticas antisimétricas.
 
 \begin{cor}
 
 $\mathrm{SO}(n,\C)$ es conexo y su grupo fundamental coincide con el de $\mathrm{SO}(n,\R)$.
 
 \end{cor}
 
 Hemos visto que a partir de una descomposición polar del grupo lineal complejo se pueden obtener muchas otras descomposiciones polares.  En vez de seguir tratando casos particulares, formularemos una teoría que englobe muchos casos particulares. Para ello debemos dar introducir algunos conceptos.
 

 
 Sea $f:G \rightarrow G$ un automorfismo de grupos de Lie.  El conjunto de puntos fijos de $f$
 $$
 G^f= \{g \in G \text{ tales que } f(g)=g\}
 $$
 es un subgrupo cerrado de $G$.  Muchos grupos clásicos se obtienen de este modo para una determinada $f$.  También es bastante frecuente que el morfismo $f$ sea una involución.  Analicemos en particular la involución que da lugar al grupo unitario.
 
 
 
 Sea
$$
\begin{array}{cccc}
\theta: & \mathrm{Gl}(V)& \rightarrow & \mathrm{Gl}(V)\\
   & g & \rightarrow& (g^*)^{-1}
   \end{array}
   $$
 El grupo de invariantes de $\theta$ es precisamente $\mathrm{U}(V)$, que es el subgrupo que aparece en la descomposición polar.  Como $\theta^2=\mathrm{Id}$, derivando se obtiene que $d\theta^2=\mathrm{Id}$.  Esta es una aplicación lineal involutiva y por lo tanto el espacio vectorial se descompone en suma directa de los autoespacios de valor $1$ y $-1$.  Calculemos explícitamente $d\theta$.
 $$
 d\theta(X)= \frac{d}{dt}(\theta(\exp(tX))_{\mid t=0}=
 \frac{d}{dt}(\exp(-tX^*))_{\mid t=0}= -X^*
 $$
 Por seguir la notación habitual en la literatura matemática, denotaremos por $\mathfrak{p}$ al subespacio de valor propio $-1$
 $$
 \mathfrak{p}=\{X \in \mathfrak{gl}(V)\text{ tales que } d\theta(X)=-X\}= \{X\in \mathfrak{gl}(V)\text{ tales que } -X^*=-X\}
 $$
 que es precisamente el espacio vectorial que aparece en el teorema de  descomposición polar.  Todo esto nos lleva a la
 
 \begin{defi}
 
 Sea $G$ un grupo de Lie. Una involución $\theta:G \rightarrow G$ es {\sf polar} \index{involución polar} si $G^\theta$ es compacto y  la aplicación
 $$
 \begin{array}{ccc}
 G^\theta \times \mathfrak{p}& \rightarrow & G \\
 (u,A) & \rightarrow & u\cdot \exp(A)
 \end{array}
 $$
es un difeomorfismo.
 
 \end{defi}
 
 
\noindent{\bf Observación.}

\smallskip
 
 Como $\mathfrak{p}$ es un espacio vectorial, $G$ y $G^\theta$ son homotópicamente equivalentes. A groso modo se puede decir que la topología de $G$ se reduce a la de $G^\theta$. \fin
 
 Dada una involución $\theta$, hemos denotado por $\mathfrak{p}$ al subespacio de autovalor~$-1$.  El subespacio de autovalor $1$ se denota
 $$
 \g^\theta= \{X \in \g \text{ tales que } d\theta(X)=X\}
 $$
 Pero este conjunto es más que un subespacio.
 
 \begin{lema}
 
Si $f:G \rightarrow G $ un morfismo, $\g^f$ es el álgebra de Lie del grupo $G^f$.
 
 \end{lema}
 
 \dem
 
 Si $X, Y\in \g^f$, entonces
 $$
 df([X,Y])= [df(X),df(Y)]= [X,Y]
 $$
 y $\g^f$ es subálgebra.
 
 \smallskip
 
 Comprobemos que si $X\in \g^f$, entonces $\exp(X)\in G^f$
 $$
f(\exp(X))=\exp(df(X))= \exp(X)
 $$
 
 Recíprocamente, si $\exp(tX)\in G^f$ para todo $t$
 $$
 df(X)=\frac{d}{dt}\big(f(\exp(tX))\big)_{\mid t=0}= \frac{d}{dt}(\exp(tX))_{\mid t=0}= X
 $$
 lo que prueba el lema. \fin
 

 
 

 
 \noindent{\bf Ejemplos.}
 
\begin{itemize}

\item Naturalmente $\theta(g)= (g^*)^{-1}$ es una involución polar del grupo lineal complejo.

   \item En el caso de $\mathrm{Sl}(n, \C)$, la restricción de $\theta$ es una involución polar.
   
   \item En el caso real debemos tomar otra involución para obtener las descomposiciones polares.  La involución en este caso es $g\rightarrow (g^t)^{-1}$. Sin embargo, a nivel matricial, esta involución es simplemente la restricción de $\theta$.   
      
      
      \item En el caso del grupo ortogonal complejo la involución es $g \rightarrow \overline{g}$. Pero si lo pensamos un poco, esta involución es nuevamente la restricción de $\theta$ al grupo ortogonal.
      
      
\end{itemize}




\begin{teo}

Sea $\theta:G\rightarrow G$ una involución polar.  Sea $f$ un automorfismo que conmute con $\theta$.  Entonces $\theta:G^f\rightarrow G^f$ es una involución polar, cuyo compacto polar es $G^\theta \cap G^f$.

\end{teo}


\dem

$\qquad$ 1) $G^f$ es invariante por $\theta$.

\noindent Sea $g\in G^f$.  Entonces
$$
f(\theta(g))= \theta(f(g)=\theta(g)
$$
Por lo tanto $\theta(g) \in G^f$.  Del mismo modo se prueba que $G^\theta$ es invariante por $f$.

\smallskip

$\qquad$ 2) $\mathfrak{p}$ es invariante por $df$.

\noindent Derivando $\theta f= f\theta$ se obtiene que $d\theta\cdot df= df\cdot d\theta$. Si $X \in \mathfrak{p}$
$$
d\theta(df(X))= df(d\theta(X))= df(-X)= -df(X)
$$
y $df(X)\in \mathfrak{p}$.

\smallskip

$\qquad 3)$ La restricción es un difeomorfismo.

\noindent Sea $g \in G^f$ y $g= u\exp(A)$ su descomposición polar.  Le aplicamos $f$ a dicha descomposición
$$
u\exp(A)=f(g)= f(u\exp(A))= f(u)\exp(df(X))
$$
En vista de los puntos anteriores
$$
u= f(u) \qquad A= df(A)
$$
de esta forma $u \in G^\theta \cap G^f$ y $A \in \mathfrak{p} \cap \g^f$. Por restricción se tiene un difeomorfismo
$$
(G^f\cap G^\theta)\times(\mathfrak{p} \cap \g^f) \rightarrow G^f
$$
Esto es lo mismo que decir que $\theta:G^f \rightarrow G^f$ es una involución polar. \fin


Algunas de las involuciones polares que ya hemos obtenido se pueden obtener también a través de este teorema.  Partimos de la involución polar $\theta(g)=(g^*)^{-1}$ en el grupo $\mathrm{Sl}(n,\C)$.  Sea $f(g)= \overline{g}$.  Entonces $G^f=\mathrm{Sl}(n,\R)$ y el compacto polar es $G^\theta\cap G^f= \mathrm{SO}(n,\R)$, obteniendo el resultado ya conocido.  El caso del grupo lineal complejo es identico.  Para el caso del grupo ortogonal complejo tomamos $f(g)= (g^t)^{-1}$ y obtenemos, tras unos cálculos, los mismos resultados que anteriormente.

\smallskip
      
Intentaremos definir los grupos matriciales clásicos como subgrupos de invariantes de un cierto morfismo y aplicaremos el teorema anterior.  Todos los cálculos se pueden hacer sin utilizar el teorema, comprobando en todos los casos  que tras aplicar el morfismo a la descomposición polar, la parte unitaria sigue siendo unitaria y lo mismo con la hermítica.

Sea $S$ la  matriz
$$
S=
\begin{pmatrix}
\mathrm{Id}_p & 0 \\
0 &-\mathrm{Id}_q
\end{pmatrix}
$$
La traspuesta, la inversa, la conjugada y la adjunta de $S$ coinciden nuevamente con $S$. Sea
$$
\begin{array}{cccc}
f: & \mathrm{Sl}(n,\C)& \rightarrow & \mathrm{Sl}(n,\C)\\
   & g & \rightarrow& S (g^*)^{-1}S
   \end{array}
   $$      
El grupo $G^f$ coincide con los elementos del grupo especial que cumplen
$$
g^*Sg=S
$$
Este grupo es $\mathrm{SU}(p,q)$, el grupo que deja invariante la métrica hermítica de signatura $(p,q)$. Estamos en las condiciones del teorema. Apliquemos que  \mbox{$f(u)=u$} para obtener el compacto polar
$$
u=S(u^*)^{-1}S=SuS \Leftrightarrow  Su=uS
$$
Entonces $u$ es un operador unitario que conmuta con $S$.  Escribiendo $u$ en bloques, obtenemos
$$
u=
\begin{pmatrix}
u_1& 0\\
0&u_2
\end{pmatrix}
$$
Como $u$ conserva la métrica, sus restricciones a los subespacios también
$$
u_1 \in \mathrm{U}(p)\text{ y } u_2\in \mathrm{U}(q)
$$ 
 Como el determinante de $u$ es $1$ se tiene que 
 $$
 \det(u_1)\det(u_2)=1
 $$
   Introducimos la siguiente notación para este subgrupo
$$
\mathrm{S}(\mathrm{U}(p) \times \mathrm{U}(q))= \left\{\left(\begin{smallmatrix}
u_1 & 0 \\
0& u_2\end{smallmatrix}\right) \text{ tal que } u_1 \in\mathrm{U}(p),  u_2\in \mathrm{U}(q) \text{ y } \det( u_1)\det( u_2)=1\right\}
$$
La estructura topológica de este este subgrupo se obtiene en el 

\begin{lema}

$\mathrm{S}(\mathrm{U}(p) \times \mathrm{U}(q))$ es difeomorfo (como variedad, no como grupo) a $\mathrm{SU}(p)\times \mathrm{U}(1) \times \mathrm{SU}(q)$.

\end{lema}

\sdem

Sea $e_k(z)$ la matrix cuadrada de tamaño $k$
$$
e_k(z)=
\begin{pmatrix}
z & 0&0 &0\\
0 & 1 &0 &0\\
0& 0 &\ddots& 0\\
0&0&0&1
\end{pmatrix}
$$      
       Si $\abs{z}=1$, geométricamente $e_k(z)$ realiza un giro del primer vector de la base.
     
     \smallskip
     
 Sea la aplicación
 $$
 \begin{array}{ccc}
 \mathrm{SU}(p) \times \mathrm{U}(1)\times \mathrm{SU}(q)&\rightarrow& S(\mathrm{U}(p) \times \mathrm{U}(q))\\
 (u_1,z,u_2)& \rightarrow & (e_p(z) u_1, e_q(\overline{z})u_2)
 \end{array}
 $$    
 Esta aplicación es diferenciable.  Como $z= \det(e_p(z)u_1)$, entonces 
 $$
 u_1= e_p(\det(g_1)^{-1})g_1\qquad u_2= e_q(\det(g_1))g_2
 $$ que son las coordenadas de la función inversa. \fin
 
 
 \begin{cor}
 
 $\mathrm{SU}(p,q)$ es conexo y su grupo fundamental es $\Z$.
 
 \end{cor}
 
 El mismo razonamiento es válido para $\mathrm{SO}(p,q)$, obteniendose que este grupo es homotópicamente equivalente a $\mathrm{SO}(p)\times \mathrm{O}(1)\times \mathrm{SO}(q)$. Como $\mathrm{O}(1)$ consta de dos puntos disjuntos, $\mathrm{SO}(p,q)$ tiene dos componentes conexas.  El grupo fundamental de la componente conexa de este grupo coincide el grupo fundamental de $\mathrm{SO}(p)\times \mathrm{SO}(q)$.  Teniendo en cuenta que $
 \pi_1(\mathrm{SO}(1))=1$, $ \pi_1(\mathrm{SO}(2))= \Z$ y $\pi_1(\mathrm{SO}(n))= \Z_2$ si $n>2$, se calcula fácilmente el grupo fundamental.
 
 \smallskip
 
 Consideremos la descomposición polar
 $$
 \theta:\mathrm{SO}(2n,\R)\times \mathrm{Sim}_0(2n) \rightarrow \mathrm{Sl}(2n,\R)
 $$
 y sea $f(g)= W^{-1}(g^t)^{-1}W$, siendo
 $$
 W=
 \begin{pmatrix}
 0& -\mathrm{Id}\\
 \mathrm{Id}& 0
 \end{pmatrix}
 $$
 la matriz de la forma simpléctica canónica (tener en cuenta que $W^{-1}=W^t$).  Se tiene que $G^f= \mathrm{Sp}(2n,\R))$.  Le aplicamos $f$ a la descomposición polar de un elemento del grupo simplectico y obtenemos que $u=f(u)$ siendo $u$ ortogonal.  Resulta que $u\in \mathrm{SO}(2n,\R)\cap \mathrm{Sp}(2n,\R)$.  Pero este grupo coincide con $\mathrm{U}(n)$, pues si un endomorfismo conserva la parte simétrica y la parte simpléctica, entonces conserva la forma hermítica construida con ambas.
 
 \begin{cor}
 
 $\mathrm{Sp}(2n,\R)$ es conexo y su grupo fundamental es $\Z$.
 
 \end{cor}
 
 
 \newpage
 
 \section{Problemas}
 
 
 \begin{pro}
 
 Sea $E$ un espacio euclídeo y $f:E \rightarrow E$ un endomorfismo no necesariamente invertible.
 
 \begin{itemize}
 
 \item Tanto $f^*f$ como $ff^*$ son endomorfismos autoadjuntos.
 
 \item $f^*f$ y $ff^*$ son semidefinidos positivos (sus autovalores son positivos o nulos).
 
 \item $f^*f$ y $ff^*$ tienen los mismos autovalores.  Si $\mu_i^2$ es un autovalor no nulo, decimos que $\mu_i$ es un \index{valor singular} {\sf valor singular} de $f$.
 
 
 \end{itemize}
 
 
 \end{pro}
 
 \begin{pro}
 
 Probar que
 
 \begin{itemize}
 
 \item $\mathrm{Ker}(f) = \mathrm{Ker}(f^*f)$
 
 \item $\mathrm{Ker}(f)= (\mathrm{Im}(f))^\perp$
 
 \item $\mathrm{Rango}(f)=\mathrm{Rango}(f^*)$
 
 
 
 
 \end{itemize}
 
 
 \end{pro}
 
   
 \begin{pro}
 
 Si $f$ es autoadjunto y semidefinido positivo, existe $h$, también autoadjunto y semidefinido positivo, tal que $h^2$=f.  Los valores propios de $h$ son los valores singulares de~$f$.
 
 \end{pro}
 
 
  \begin{pro}
 
 Si $f$ es un endomorfismo de un espacio euclídeo, existe un endomorfismo ortogonal y dos endomorfismos $h_1$ y $h_2$ autoadjuntos  y semidefinidos positivos tal que
 $$
 f= gh_1= h_2g
 $$
 Si $f$ es normal ($f$ conmuta con su adjunto) entonces $h_1=h_2$.
 
 
 \end{pro}
 
 
 \begin{pro}
 
 Sea $A$ hermítica y $T$ un endomorfismo arbitrario.  Si $[e^A,T]=0$ entonces $[A,C]=0$.
 
 \end{pro}
 
 \begin{pro}
 
 Sea $\theta: G \rightarrow G$ una involución no necesariamente polar.  Con las notaciones del texto probar:
 
 \begin{itemize}
 
 \item Se tiene la descomposición en suma directa  $\g= \g^\theta \oplus \mathfrak{p}$ como espacios vectoriales.
 
 \item $[\g^\theta,\g^\theta]\subset \g^\theta$ ($\g^\theta$ es subálgebra), $[\g^\theta,\mathfrak{p}]\subset \mathfrak{p}$ y $[\mathfrak{p},\mathfrak{p}]\subset \g^\theta$.
 
 \item Si $u\in G^\theta$ entonces $Ad_u(\mathfrak{p})\subset \mathfrak{p}$
 
 \item  Se puede obtener una descomposición polar similar utilizando la fórmula
 $$
 u\cdot \exp(A)= \exp(Ad_u(A))\cdot u
 $$
 
 \end{itemize}
 
 \end{pro}
 
 
 \newpage
 
 
 \begin{pro}
Demostrar que si
$$ 
 A=\begin{pmatrix}
 0 & -\theta \\
 \theta&0
 \end{pmatrix}
\quad \Rightarrow \quad
 \exp(A) =
 \begin{pmatrix}
 \cos(\theta) & -\sen(\theta)\\
 \sen(\theta)&\cos(\theta)
 \end{pmatrix}
 $$
 La exponencial $\exp:\mathfrak{so}(2,\R) \rightarrow \mathrm{SO}(2,\R)$ es epiyectiva.
 
 \end{pro}
 
 
 \begin{pro}
 
Demostrar que si
 $$
 A=
 \begin{pmatrix}
 a & b\\
 c & -a
 \end{pmatrix}
 \quad \Rightarrow \quad 
  A^2= (a^2+cb) \cdot \mathrm{Id}= \det(A)\cdot \mathrm{Id}
 $$
 \begin{itemize}
 
 \item Si $\det(A)=0$ entonces 
 $$
 \exp(A)=\mathrm{Id}+A
 $$
  
 \item  Si $\det(A)<0$ entonces 
 $$
 \exp(A)= \cos(\omega)\mathrm{id}+ \frac{\sen(\omega)}{\omega} A
 $$
 donde $\omega= \sqrt{\abs{\det(A)}}$.
 
 \item Si $\det(A)>0$ entonces
 $$
 \exp(A)= \cosh(\omega)\mathrm{id}+ \frac{\senh(\omega)}{\omega} A
 $$ 
      
      \end{itemize}
      En el primer caso la traza es $2$, en el segundo la traza es $2\cosh(\omega)$ y en el tercero es $2\cos(\omega)$.  En todos los casos $\mathrm{Tr}(\exp(A))\geq -2$.  La exponencial en el grupo  especial lineal no es epiyectiva.
      
   
      
      \end{pro}
      
      
      \begin{pro}
      
      
      Sea $V$ un espacio vectorial real de dimensión finita.  Una estructura compleja en $V$ es una aplicación lineal $J:V \rightarrow V$ tal que $J^2=-\mathrm{Id}$.
      
      \begin{itemize}
      
      
      
      
      \item En $V$ se puede introducir una estrutura de espacio vectorial sobre complejo definiendo
      
      $$
      (x+iy)u= xu+yJ(u)
      $$
      Denotamos por $V_J$ al espacio vectorial complejo así obtenido.
      
      \item  Sea $V$ un espacio vectorial complejo.  Por restricción podemos considerar en $V$ una estructrura de espacio vectorial real, que denotamos $V_\R$.  Demostrar que la aplicación
      $$
      \begin{array}{cccc}
      J:& V_\R& \rightarrow& V_\R\\
      & x & \rightarrow &ix
      \end{array}
      $$
      es una estructura compleja. El espacio complejo asociado a $J$ es nuevamente $V$.
      
      \item Si $e$ es no nulo, los vectores $e$ y $J(e)$ son independientes.
      
      \item Sea $\{e_1, \dots,e_n\}$ una base del espacio complejo $V_J$.  Demostrar que
      $$
      \{e_1,\dots,e_n,J(e_1),\dots,J(e_n)\}
      $$
      es una base del espacio vectorial (real) $V$.  Concluir que $V$ es necesariamente de dimensión par.
      
      \item  Todo endomorfismo de $V_J$ es un  endomorfismo de $V$.  Sin embargo el recíproco no es cierto.  Demostrar que $\varphi\in \mathrm{End}(V)$ es $\C$-lineal, si y solo si $\varphi$ conmuta con $J$. En fórmulas
      $$
      \mathrm{End}(V_J)=\{g\in \mathrm{End}(V)\text{ tales que } [g,J]=0\}
      $$
      
      \item La matriz de $J$ en una base $   \{e_1,\dots,e_n,J(e_1),\dots,J(e_n)\}$ es 
      $$
      J=
      \begin{pmatrix}
      0& -\mathrm{Id}\\
      \mathrm{Id}&0
      \end{pmatrix}
      $$
      Demostrar que matricialmente
      $$
      \mathrm{Gl}(n,\C)=\{g \in \mathrm{Gl}(2n,\R)\text{ tales que } [g,J]=0\}
      $$
      Expresar por cajas esta condición.
      
      
      
      \end{itemize}
      
      
      \end{pro}
      
      \begin{pro}
      
      Sea $V$ un espacio vectorial complejo.  Cuando consideremos en $V$ la estrutura de espacio vectorial real lo denotaremos $V_\R$.
      
      \begin{itemize}
      
      \item Si $\{e_1,\dots,e_n\}$ es una base de $V$ entonces
      $$
      \{\{e_1,\dots,e_n,J(e_1),\dots,J(e_n)\}
      $$
      es una base de $V_\R$.
      
      \item  Sea $h:V\times V \rightarrow \C$ una forma hermítica de cualquier signatura.  Demostrar que
      $$
      \begin{array}{cccc}
      h_1:& V_\V \times V_\R &\rightarrow &\R\\
      & (x,y)& \rightarrow & \mathrm{Re}(h(x,y))
      \end{array}
      $$
      es una forma simétrica no degenerada y que
       $$
      \begin{array}{cccc}
      h_2:& V_\V \times V_\R &\rightarrow &\R\\
      & (x,y)& \rightarrow & \mathrm{Im}(h(x,y))
      \end{array}
      $$
      es una forma simpléctica no degenerada.
      
      \item Si $h$ tiene signatura $(p,q)$ entonces $h_1$ tiene signatura $(2p,2q)$.
      
      \item Si un endomorfismo conserva $h$, entonces conserva $h_1$ y $h_2$.
      
      \item Si un enfomorfismo conserva $h_1$ y $h_2$ entonces también conserva $h$ y $J$.
      
      \item Demostrar que 
      $$
      \mathrm{U}(p,q)= \mathrm{SO}(2p,2q)\cap \mathrm{Sp}(2n,\R)
      $$
      
      \end{itemize}
      
      \end{pro}
      
      
      
      
      
      
      
      
\end{document}
 







En el estudio de las representaciones lineales, tienen gran importancia los endomorfismos que conmutan con todo endomorfismo de la representación. Por la propiedad universal de  $\mathrm{U}(\g)$, los elementos del centro de  $\mathrm{U}(\g)$ van a parar por una representación a operadores  \guillemotleft permutables\guillemotright\  con la representación. Por ello es lógico estudiar el centro del  álgebra $\mathrm{U}(\g)$. Nosotros no podemos meternos tan a fondo, pero diremos que en condiciones bastante generales el álgebra  $\mathrm{U}(\g)$ posee centro no trivial y un elemento, cuadrático en los elementos del álgebra,que recibe el nombre de operador de Casimir\index{Operador!de Casimir}. Vamos con un ejemplo que es vital en física cuántica.

Sea  $\mathrm{SO} (3)$ el álgebra del grupo de rotaciones. Podemos tomar una base en  $\mathrm{SO} (3)$ de la forma $(J_1 , J_2 , J_3 )$  que satisfaga las reglas de conmutación
$$
[J_i  , J_j  ] = \epsilon_{ij}^k J_k
$$
En estas condiciones es fácil probar que  $J^2 = J_1^2 + J_2^2 + J_3^2$	conmuta con $J_1 , J_2$   y  $J_3$. Por lo tanto conmuta con todo elemento del álgebra envolvente de   $\mathrm{SO} (3)$.

Razonamientos parecidos pueden emplearse también para el grupo de Lo\-rentz.

El problema de construir Casimires y el estudio general del centro de $\mathrm{U}(\g)$ es complicado.
















\begin{propo}

Sea  $\varphi : G \times \mathcal{V} \rightarrow\mathcal{V}$ una acción . El álgebra de Lie del grupo de isotropia  $G_x$ coincide con el núcleo de la diferencial de $A_x$ (en el neutro).

\end{propo}

\dem



$A_x$ restringido a  $G_x$  es constante. Entonces  $\g_x \subset  \mathrm{Ker} ( {A_x}' )_e$.
Tomemos un elemento $X \in \mathrm{Ker} ( {A_x}' )_e$. Debemos ver que   $\exp (X)$  está en  $G_x$.

Sea  $\beta (t) = \exp (tX) \cdot x=A_x(\exp(tX))$. Derivamos esta función aplicando la regla de la cadena y observamos que su derivada es nula. La función es constante y por lo tanto   $x = \exp (tX) \cdot x$. \fin


